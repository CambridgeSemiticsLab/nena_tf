{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NENA 2 TF 2.0\n",
    "\n",
    "This version of the NENA conversion is the first to use the new \n",
    "text markup and parsing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import json\n",
    "import tabulate\n",
    "import unicodedata as ud\n",
    "from tf.fabric import Fabric\n",
    "from tf.convert.walker import CV\n",
    "from pathlib import Path\n",
    "\n",
    "# configure paths to data input and output\n",
    "VERSION = 'alpha'\n",
    "CSL_DIR = Path.home().joinpath('github/CambridgeSemiticsLab/')\n",
    "PROJECT_DIR = CSL_DIR.joinpath('nena_tf')\n",
    "CORPUS_DIR = CSL_DIR.joinpath('nena_corpus')\n",
    "INPUT_DIR = CORPUS_DIR.joinpath(f'parsed_texts/{VERSION}')\n",
    "CORPUS_METADATA = CORPUS_DIR.joinpath('standards/metadata.json')\n",
    "OUTPUT_DIR = PROJECT_DIR.joinpath(f'tf/{VERSION}')\n",
    "#TF_methods = {'Fabric': Fabric, 'CV': CV}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Feature and Object Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_meta = json.loads(CORPUS_METADATA.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'North Eastern Neo-Aramaic Text Corpus',\n",
       " 'origin': 'The NENA text corpus is derived from decades of fieldwork by Professor Geoffrey Khan and his colleagues at the University of Cambridge, Faculty of Asian and Middle Eastern Studies',\n",
       " 'license': 'Creative Commons Attribution 4.0 International Public License',\n",
       " 'contributors': 'Geoffrey Khan, Eleanor Coghill, Roberta Borghero, Lidia Napiorkowska, Hezy Mutzafi, Alinda Damsma, Paul Noorlander, Dorota Molin, Johan Lundberg',\n",
       " 'scientific_programmers': 'Cody Kingham, James Strachan, Dirk Roorda, Hannes Vlaardingerbroek',\n",
       " 'DOI': '10.5281/zenodo.3541999',\n",
       " 'corpus_repo': 'https://github.com/CambridgeSemiticsLab/nena_corpus'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_meta['NENA_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some Text-Fabric specific metadata\n",
    "# to configure this corpus\n",
    "corpus_meta['intFeatures'] = {\n",
    "    f for f,fd in corpus_meta['object_features'].items() \n",
    "        if fd['value'] == 'integer'\n",
    "}\n",
    "\n",
    "corpus_meta['otext'] = {\n",
    "    'sectionTypes': 'dialect,text,line',\n",
    "    'sectionFeatures': 'dialect,title,line_number',\n",
    "    'fmt:text-orig-full': 'word#{text}{text_end}',\n",
    "    'fmt:text-orig-lite': 'word#{lite}{lite_end}',\n",
    "    'fmt:text-trans-full': 'word#{full}{full_end}',\n",
    "    'fmt:text-trans-lite': 'word#{lite}{lite_end}',\n",
    "    'fmt:text-trans-fuzzy': 'word#{fuzzy}{fuzzy_end}',\n",
    "}\n",
    "\n",
    "corpus_meta['slot_type'] = 'letter'\n",
    "\n",
    "# these features are currently unattested\n",
    "# we ignore them to avoid error messages from TF\n",
    "ignore_features = {\n",
    "    \"full_begin\", \"fuzzy_begin\", \"lemma\", \"lite_begin\",\n",
    "    \"stem\", \"syn\", \"text_begin\", \"timestamp\", \"trns\"\n",
    "}\n",
    "corpus_meta['object_features'] = {k:v for k,v in corpus_meta['object_features'].items() if k not in ignore_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NenaTfBuilder:\n",
    "    \"\"\"Construct Text-Fabric graph resource from parsed JSON files\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir, output_dir, metadata, Fabric, CV, **TF_kwargs):\n",
    "        \"\"\"Load json data from input dir to prepare for TF conversion.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: pathlib Path which is a directory that contains\n",
    "                subdirectories named after the respective dialects in the\n",
    "                text corpus. Each subdirectory should contain parsed JSON\n",
    "                texts ready for analysis.\n",
    "            output_dir: directory to save the .tf files\n",
    "            metadata: dictionary containing metadata on the corpus needed\n",
    "                to construct the TF graph\n",
    "            Fabric: Text-Fabric module for saving/outputing data\n",
    "            CV: Text-Fabric CV module for constructing text graphs\n",
    "            **TF_kwargs: optional kwargs to pass to Text-Fabric loader\n",
    "        \"\"\"\n",
    "        \n",
    "        # load JSON data and initialize TF objects\n",
    "        self.metadata = metadata\n",
    "        self.dialect2parsings = self.load_parsed_jsons(input_dir)\n",
    "        self.Fabric = Fabric(locations=str(output_dir), **TF_kwargs)\n",
    "        self.cv = CV(self.Fabric)\n",
    "        self.message = self.Fabric.tmObj # timestamped messages\n",
    "        \n",
    "    def load_parsed_jsons(self, dialect_dir):\n",
    "        \"\"\"Map directory of dialect subdirectories to parsed json data.\n",
    "        \n",
    "        Args:\n",
    "            dialect_dir: a pathlib Path that contains subdirectories\n",
    "                named after respective dialects; each subdirectory \n",
    "                contains parsed JSON files which are each a text\n",
    "        \n",
    "        Returns:\n",
    "            dict with structure of dict[dialect] = list(text_parsings)\n",
    "        \"\"\"\n",
    "        dialect2parsings = collections.defaultdict(list)\n",
    "        for dialect_dir in sorted(INPUT_DIR.glob('*')):\n",
    "            for text_file in sorted(dialect_dir.glob('*.json')):\n",
    "                dialect = dialect_dir.name\n",
    "                text_data = json.loads(text_file.read_text())\n",
    "                dialect2parsings[dialect].append(text_data)\n",
    "        return dialect2parsings\n",
    "    \n",
    "    def build(self, **walk_kwargs):\n",
    "        \"\"\"Executes the TF conversion on the loaded source\n",
    "        \n",
    "        Args:\n",
    "            walk_kwargs: optional keyword arguments to feed \n",
    "                to TF's cv.walk function.\n",
    "        \"\"\"\n",
    "        self.good = self.cv.walk(\n",
    "            self.director,\n",
    "            self.metadata['slot_type'],\n",
    "            otext=self.metadata['otext'],\n",
    "            generic=self.metadata['NENA_corpus'],\n",
    "            intFeatures=self.metadata['intFeatures'],\n",
    "            featureMeta=self.metadata['object_features'],\n",
    "            **walk_kwargs,\n",
    "        )\n",
    "    \n",
    "    def dict_intersect(self, dict1, dict2):\n",
    "        \"\"\"Set intersection from one dict to another\"\"\"\n",
    "        return {k:v for k,v in dict1.items() if k in dict2}\n",
    "    \n",
    "    def director(self, cv):\n",
    "        \"\"\"Call Text-Fabric CV methods to index the text graph.\n",
    "        \n",
    "        This function does the bulk of the work of building the TF resource.\n",
    "        It operates in one large loop that walks over all parsed data. \n",
    "        The supplied cv Text-Fabric class possesses methods that create node \n",
    "        IDs and associate features with those IDs. These methods are called \n",
    "        throughout the loop. The CV class also handles embedding relationships\n",
    "        between the nodes based on the overlap of slots (i.e. atomic units\n",
    "        for the text graph).\n",
    "        \n",
    "        cv methods used here:\n",
    "            cv.slot: make a new slot, the atomic element of the graph. All \n",
    "                nodes active during an active slot will contain that slot.\n",
    "            cv.node: make a new node in the graph with supplied object name\n",
    "            cv.feature: add a string/integer feature to a supplied cv.node\n",
    "            cv.terminate: deactivate a given node; this ends any further\n",
    "                slot embeddings, which are calculated automatically from \n",
    "                whichever slots are activated while the node is also active.\n",
    "                \n",
    "        Further info about cv functionality can be referenced in the \n",
    "        Text-Fabric documentation.\n",
    "        \n",
    "        Args:\n",
    "            cv: Text-Fabric CV class loaded with Fabric\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self.metadata['object_features']\n",
    "        text_features = {f for f,fd in features.items() if fd['value'] == 'text'}\n",
    "        general_features = {f for f in features if f not in text_features}\n",
    "        nodes = {} # gets updated throughout\n",
    "    \n",
    "        def swap_node(node_type):\n",
    "            \"\"\"Replace any active nodes with new node or just add new node.\"\"\"\n",
    "            try:\n",
    "                cv.terminate(nodes[node_type])\n",
    "                nodes[node_type] = cv.node(node_type)\n",
    "            except KeyError:\n",
    "                nodes[node_type] = cv.node(node_type)\n",
    "        \n",
    "        # parse all data for every dialect\n",
    "        for dialect, texts in self.dialect2parsings.items():\n",
    "            \n",
    "            # make dialect node / features\n",
    "            nodes['dialect'] = cv.node('dialect')\n",
    "            cv.feature(nodes['dialect'], dialect=dialect)\n",
    "            \n",
    "            # make text node / features\n",
    "            for text in texts:\n",
    "        \n",
    "                nodes['text'] = cv.node('text')\n",
    "                text_attributes, paragraphs = text\n",
    "                \n",
    "                for feature, value in text_attributes.items():\n",
    "                    if feature == 'speakers':\n",
    "                        speakers = ', '.join(value.values())\n",
    "                        cv.feature(nodes['text'], speakers=speakers)\n",
    "                    elif feature in features:\n",
    "                        cv.feature(nodes['text'], **{feature:value})\n",
    "                        \n",
    "                for ith_paragraph, paragraph in enumerate(paragraphs):\n",
    "                    \n",
    "                    nodes['stress'] = cv.node('stress')\n",
    "                    nodes['inton'] = cv.node('inton')\n",
    "                    nodes['subsentence'] = cv.node('subsentence')\n",
    "                    nodes['sentence'] = cv.node('sentence')\n",
    "                    nodes['paragraph'] = cv.node('paragraph')\n",
    "                \n",
    "                    # -- Process Paragraph elements --\n",
    "                    # which are comprised of word and span tags\n",
    "                    \n",
    "                    # track span features and add them to words as words are made\n",
    "                    # span feature values can be altered when triggered by new span tags\n",
    "                    span_feats = {\n",
    "                        'speaker': list(text_attributes['speakers'].values())[0],\n",
    "                        'lang': 'NENA',\n",
    "                        'timestamp': None,\n",
    "                    }\n",
    "                    \n",
    "                    for ith_element, element in enumerate(paragraph):\n",
    "                        \n",
    "                        # -- process span elements --\n",
    "                        if element['class'] == 'span':\n",
    "                            \n",
    "                            # build line nodes\n",
    "                            if 'line_number' in element:\n",
    "                                swap_node('line')\n",
    "                                line_num = element['line_number']\n",
    "                                cv.feature(\n",
    "                                    nodes['line'], \n",
    "                                    line_number=line_num\n",
    "                                )\n",
    "                            \n",
    "                            # update other span fields\n",
    "                            span_feats.update(\n",
    "                                self.dict_intersect(element, span_feats)\n",
    "                            )\n",
    "                            \n",
    "                        # -- Process words, their letters, and their beginnings/ends --\n",
    "                        elif element['class'] == 'word':\n",
    "                            \n",
    "                            nodes['word'] = cv.node('word')\n",
    "                            word_features = {}\n",
    "                            word_features.update(span_feats) # add span features\n",
    "                            word_features.update(\n",
    "                                self.dict_intersect(element, general_features) # + other features\n",
    "                            )\n",
    "                            \n",
    "                            # 1. ** process word's letters and their features **\n",
    "                            # also get text from the letters\n",
    "                            for letter in element['letters']:\n",
    "                                letter_node = cv.slot()\n",
    "                                letter_features = self.dict_intersect(letter, features)\n",
    "                                \n",
    "                                # process features for letter\n",
    "                                # pass on text features to word \n",
    "                                for feature, value in letter_features.items():\n",
    "                                    cv.feature(letter_node, **{feature:value})\n",
    "                                    if feature in text_features:\n",
    "                                        word_features[feature] = word_features.get(feature,'') + value\n",
    "                                \n",
    "                                # we're done with letter node; terminate to deactivate it\n",
    "                                cv.terminate(letter_node)\n",
    "    \n",
    "                            # 2. ** process word's parsing features **\n",
    "                            # current process allows multiple parsings to co-exist\n",
    "                            # thus we construct a composite parse-string for each feature\n",
    "                            parse_values = collections.defaultdict(list)\n",
    "                            word_features['n_parses'] = len(element['parsings'])\n",
    "                            for parse in element['parsings']:\n",
    "                                # gather feature / val strings to be joined on '|' (see next codeblock)\n",
    "                                keep_parse = self.dict_intersect(parse, features)\n",
    "                                for feat, val in keep_parse.items():\n",
    "                                    parse_values[feat].append(val)\n",
    "                            \n",
    "                            # join multiple parse strings on '|'\n",
    "                            # add the new strings to word features\n",
    "                            for feat, vals in parse_values.items():\n",
    "                                word_features[feat] = '|'.join(vals)\n",
    "                            \n",
    "                            # 3. ** Process beginnings on a word **\n",
    "                            for begin in element['beginnings']:\n",
    "                                \n",
    "                                # build begin strings\n",
    "                                begin_features = self.dict_intersect(begin, text_features)\n",
    "                                text_feat_prfx = {k+'_begin':v for k,v in begin_features.items()}\n",
    "                                for feat_val in text_feat_prfx.items():\n",
    "                                    word_features[feat] = word_features.get(feat,'') + val\n",
    "                            \n",
    "                            # 4. ** Process endings on a word ** \n",
    "                            # mark sentence/subsentence/inton/stress bounds on endings of word\n",
    "                            # also add endings as their own text features of a word, with _end suffix\n",
    "                            detected_boundaries = set()\n",
    "                            for end in element['endings']:\n",
    "                                \n",
    "                                # build end strings\n",
    "                                end_features = self.dict_intersect(end, text_features)\n",
    "                                text_feat_sffx = {k+'_end':v for k,v in end_features.items()}\n",
    "                                \n",
    "                                for feat, val in text_feat_sffx.items():\n",
    "                                    word_features[feat] = word_features.get(feat,'') + val\n",
    "                                \n",
    "                                # skip non-separating punctuation\n",
    "                                if end['class'] != 'separator':\n",
    "                                    continue\n",
    "                                \n",
    "                                # detect stress bounds; end at any of the following:\n",
    "                                if end['modifies'] in {'word', 'intonation group',  'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('stress')\n",
    "                                \n",
    "                                # detect inton bounds\n",
    "                                if end['modifies'] in {'intonation group', 'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('inton')\n",
    "                                \n",
    "                                # detect subsentence bounds\n",
    "                                if end['modifies'] in {'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('subsentence')\n",
    "                                \n",
    "                                # detect sentence bounds\n",
    "                                if end['modifies'] in {'sentence'}:\n",
    "                                    detected_boundaries.add('sentence')\n",
    "                                \n",
    "                            # end the word and execute boundary divisions\n",
    "                            cv.feature(nodes['word'], **word_features)\n",
    "                            cv.terminate(nodes['word'])\n",
    "                            for bound in detected_boundaries:\n",
    "                                if ith_element+1 != len(paragraph):\n",
    "                                    swap_node(bound)\n",
    "                                else:\n",
    "                                    cv.terminate(nodes[bound])\n",
    "\n",
    "                    # we've come to the end of the paragraph\n",
    "                    # we do some house-cleaning before finishing with the paragraph\n",
    "                    \n",
    "                    # do a sanity check for un-closed intons, subsentences, sentences\n",
    "                    # possibly due to lack of proper punctuation in the source text (to be fixed later)\n",
    "                    title = text_attributes['title']\n",
    "                    for obj in {'stress', 'inton', 'sentence', 'subsentence'} & cv.activeTypes():\n",
    "                        sys.stderr.write(f'force-closing {obj} in {title}, §{ith_paragraph}.{ith_element}\\n')\n",
    "                        cv.terminate(nodes[obj])\n",
    "                    \n",
    "                    # check for active line on last paragraph of a text\n",
    "                    # though a line can straddle paragraphs it should not straddle texts!\n",
    "                    if (ith_paragraph+1 == len(paragraphs)) and nodes.get('line', None):\n",
    "                        cv.terminate(nodes['line'])\n",
    "                    \n",
    "                    # close shop on the §\n",
    "                    cv.terminate(nodes['paragraph'])\n",
    "                        \n",
    "                # -- trigger section node endings --\n",
    "                cv.terminate(nodes['text'])\n",
    "            cv.terminate(nodes['dialect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 8.4.4\n",
      "Api reference : https://annotation.github.io/text-fabric/cheatsheet.html\n",
      "\n",
      "43 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "nena_builder = NenaTfBuilder(INPUT_DIR, OUTPUT_DIR, corpus_meta, Fabric, CV, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |     0.00s No structure nodes will be set up\n",
      "   |   SECTION   TYPES:    dialect, text, line\n",
      "   |   SECTION   FEATURES: dialect, title, line_number\n",
      "   |   STRUCTURE TYPES:    \n",
      "   |   STRUCTURE FEATURES: \n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       text, text_end\n",
      "   |      |   text-orig-lite       lite, lite_end\n",
      "   |      |   text-trans-full      full, full_end\n",
      "   |      |   text-trans-fuzzy     fuzzy, fuzzy_end\n",
      "   |      |   text-trans-lite      lite, lite_end\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §0.80\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §0.80\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §1.57\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §1.57\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §2.17\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §2.17\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §3.68\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §4.38\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §4.38\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §5.20\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §5.20\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §6.17\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §6.17\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §7.13\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §7.13\n",
      "force-closing sentence in Qaṭina Rescues His Nephew From Leliθa, §8.19\n",
      "force-closing subsentence in Qaṭina Rescues His Nephew From Leliθa, §8.19\n",
      "force-closing sentence in The Battle With Yuwanəs the Armenian, §1.17\n",
      "force-closing subsentence in The Battle With Yuwanəs the Armenian, §1.17\n",
      "force-closing sentence in The Battle With Yuwanəs the Armenian, §2.513\n",
      "force-closing subsentence in The Battle With Yuwanəs the Armenian, §2.513\n",
      "force-closing sentence in The Battle With Yuwanəs the Armenian, §3.37\n",
      "force-closing subsentence in The Battle With Yuwanəs the Armenian, §3.37\n",
      "force-closing sentence in The Battle With Yuwanəs the Armenian, §4.127\n",
      "force-closing sentence in The Battle With Yuwanəs the Armenian, §5.10\n",
      "force-closing subsentence in The Battle With Yuwanəs the Armenian, §5.10\n",
      "force-closing sentence in The Crow and the Cheese, §0.76\n",
      "force-closing subsentence in The Crow and the Cheese, §0.76\n",
      "force-closing sentence in The Girl and the Seven Brothers, §0.61\n",
      "force-closing sentence in The Girl and the Seven Brothers, §1.8\n",
      "force-closing subsentence in The Girl and the Seven Brothers, §1.8\n",
      "force-closing sentence in The Sisisambər Plant, §2.109\n",
      "force-closing subsentence in The Sisisambər Plant, §2.109\n",
      "force-closing sentence in The Sisisambər Plant, §3.17\n",
      "force-closing subsentence in The Sisisambər Plant, §3.17\n",
      "force-closing sentence in The Sisisambər Plant, §4.162\n",
      "force-closing subsentence in The Sisisambər Plant, §4.162\n",
      "force-closing sentence in The Sisisambər Plant, §5.13\n",
      "force-closing subsentence in The Sisisambər Plant, §5.13\n",
      "force-closing sentence in The Tale of Nasimo, §0.87\n",
      "force-closing sentence in The Tale of Nasimo, §1.38\n",
      "force-closing subsentence in The Tale of Nasimo, §1.38\n",
      "force-closing sentence in The Tale of Nasimo, §2.21\n",
      "force-closing sentence in The Wise Snake, §0.15\n",
      "force-closing sentence in Šošət Xere, §0.204\n",
      "force-closing subsentence in Šošət Xere, §0.204\n",
      "force-closing sentence in Šošət Xere, §2.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       16s \"edge\" actions: 0\n",
      "   |       16s \"feature\" actions: 4288281\n",
      "   |       16s \"node\" actions: 294206\n",
      "   |       16s \"resume\" actions: 0\n",
      "   |       16s \"slot\" actions: 539378\n",
      "   |       16s \"terminate\" actions: 833709\n",
      "   |          2 x \"dialect\" node \n",
      "   |      36444 x \"inton\" node \n",
      "   |     539378 x \"letter\" node  = slot type\n",
      "   |       2544 x \"line\" node \n",
      "   |        350 x \"paragraph\" node \n",
      "   |      16326 x \"sentence\" node \n",
      "   |      93766 x \"stress\" node \n",
      "   |      24497 x \"subsentence\" node \n",
      "   |        126 x \"text\" node \n",
      "   |     120151 x \"word\" node \n",
      "   |     833584 nodes of all types\n",
      "   |       17s OK\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.05s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.23s Sorting 2 nodes of type \"dialect\"\n",
      "   |     0.28s Sorting 36444 nodes of type \"inton\"\n",
      "   |     0.45s Sorting 2544 nodes of type \"line\"\n",
      "   |     0.51s Sorting 350 nodes of type \"paragraph\"\n",
      "   |     0.61s Sorting 16326 nodes of type \"sentence\"\n",
      "   |     0.85s Sorting 93766 nodes of type \"stress\"\n",
      "   |     1.13s Sorting 24497 nodes of type \"subsentence\"\n",
      "   |     1.57s Sorting 126 nodes of type \"text\"\n",
      "   |     1.71s Sorting 120151 nodes of type \"word\"\n",
      "   |     2.03s Max node = 833584\n",
      "   |     2.03s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |    1m 31s node feature \"dialect\" with 128 nodes\n",
      "   |      |    1m 31s node feature \"full\" with 659529 nodes\n",
      "   |      |    1m 32s node feature \"full_end\" with 120139 nodes\n",
      "   |      |    1m 32s node feature \"fuzzy\" with 659529 nodes\n",
      "   |      |    1m 32s node feature \"fuzzy_end\" with 120139 nodes\n",
      "   |      |    1m 32s node feature \"gloss\" with 21713 nodes\n",
      "   |      |    1m 32s node feature \"gn\" with 14366 nodes\n",
      "   |      |    1m 32s node feature \"lang\" with 120151 nodes\n",
      "   |      |    1m 32s node feature \"line_number\" with 2544 nodes\n",
      "   |      |    1m 32s node feature \"lite\" with 659529 nodes\n",
      "   |      |    1m 33s node feature \"lite_end\" with 120139 nodes\n",
      "   |      |    1m 33s node feature \"n_parses\" with 120151 nodes\n",
      "   |      |    1m 33s node feature \"nu\" with 8206 nodes\n",
      "   |      |    1m 33s node feature \"nu_class\" with 132 nodes\n",
      "   |      |    1m 33s node feature \"phonation\" with 306781 nodes\n",
      "   |      |    1m 33s node feature \"phonetic_class\" with 539360 nodes\n",
      "   |      |    1m 34s node feature \"phonetic_manner\" with 310962 nodes\n",
      "   |      |    1m 34s node feature \"phonetic_place\" with 310962 nodes\n",
      "   |      |    1m 34s node feature \"place\" with 126 nodes\n",
      "   |      |    1m 34s node feature \"pos\" with 21716 nodes\n",
      "   |      |    1m 34s node feature \"speaker\" with 120151 nodes\n",
      "   |      |    1m 34s node feature \"speakers\" with 126 nodes\n",
      "   |      |    1m 34s node feature \"st\" with 483 nodes\n",
      "   |      |    1m 34s node feature \"tense\" with 1 node\n",
      "   |      |    1m 34s node feature \"text\" with 659529 nodes\n",
      "   |      |    1m 35s node feature \"text_end\" with 120139 nodes\n",
      "   |      |    1m 35s node feature \"text_id\" with 125 nodes\n",
      "   |      |    1m 35s node feature \"text_nostress\" with 659529 nodes\n",
      "   |      |    1m 35s node feature \"text_nostress_end\" with 120139 nodes\n",
      "   |      |    1m 35s node feature \"title\" with 126 nodes\n",
      "   |      |    1m 35s node feature \"variable\" with 125 nodes\n",
      "   |     4.35s OK\n",
      "  0.00s Exporting 32 node and 1 edge and 1 config features to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.15s VALIDATING oslots feature\n",
      "  0.15s maxSlot=     539378\n",
      "  0.15s maxNode=     833584\n",
      "  0.25s OK: oslots is valid\n",
      "   |     0.00s T dialect              to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.85s T full                 to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.15s T full_end             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     1.13s T fuzzy                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.29s T fuzzy_end            to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.07s T gloss                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.08s T gn                   to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.19s T lang                 to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.01s T line_number          to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     1.25s T lite                 to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.18s T lite_end             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.16s T n_parses             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.01s T nu                   to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T nu_class             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.25s T otype                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.43s T phonation            to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.68s T phonetic_class       to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.66s T phonetic_manner      to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.44s T phonetic_place       to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T place                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.08s T pos                  to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.32s T speaker              to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T speakers             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T st                   to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T tense                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     1.05s T text                 to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.32s T text_end             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T text_id              to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     1.21s T text_nostress        to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.19s T text_nostress_end    to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T title                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s T variable             to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     1.91s T oslots               to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "   |     0.00s M otext                to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n",
      "    12s Exported 32 node features and 1 edge features and 1 config features to ~/github/CambridgeSemiticsLab/nena_tf/tf/alpha\n"
     ]
    }
   ],
   "source": [
    "nena_builder.build(generateTf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nena_builder.dialect2parsings['Barwar'][0][1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nena_builder.metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
