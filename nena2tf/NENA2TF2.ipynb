{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NENA 2 TF 2.0\n",
    "\n",
    "This version of the NENA conversion is the first to use the new \n",
    "text markup and parsing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import tabulate\n",
    "import unicodedata as ud\n",
    "from tf.fabric import Fabric\n",
    "from tf.convert.walker import CV\n",
    "from pathlib import Path\n",
    "\n",
    "# configure paths to data input and output\n",
    "VERSION = 'alpha'\n",
    "CSL_DIR = Path.home().joinpath('github/CambridgeSemiticsLab/')\n",
    "PROJECT_DIR = CSL_DIR.joinpath('nena_tf')\n",
    "CORPUS_DIR = CSL_DIR.joinpath('nena_corpus')\n",
    "INPUT_DIR = CORPUS_DIR.joinpath(f'parsed_texts/{VERSION}')\n",
    "CORPUS_METADATA = CORPUS_DIR.joinpath('standards/metadata.json')\n",
    "OUTPUT_DIR = PROJECT_DIR.joinpath(f'tf/{VERSION}')\n",
    "TF_methods = {'Fabric': Fabric, 'CV': CV}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Feature and Object Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_meta = json.loads(CORPUS_METADATA.read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'North Eastern Neo-Aramaic Text Corpus',\n",
       " 'origin': 'The NENA text corpus is derived from decades of fieldwork by Professor Geoffrey Khan and his colleagues at the University of Cambridge, Faculty of Asian and Middle Eastern Studies',\n",
       " 'license': 'Creative Commons Attribution 4.0 International Public License',\n",
       " 'contributors': 'Geoffrey Khan, Eleanor Coghill, Roberta Borghero, Lidia Napiorkowska, Hezy Mutzafi, Alinda Damsma, Paul Noorlander, Dorota Molin, Johan Lundberg',\n",
       " 'scientific_programmers': 'Cody Kingham, James Strachan, Dirk Roorda, Hannes Vlaardingerbroek',\n",
       " 'DOI': '10.5281/zenodo.3541999',\n",
       " 'corpus_repo': 'https://github.com/CambridgeSemiticsLab/nena_corpus'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_meta['NENA_corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intFeatures = {'number'}\n",
    "otext = {\n",
    "    'sectionTypes': 'dialect,text,line',\n",
    "    'sectionFeatures': 'dialect,title,number',\n",
    "    'fmt:text-orig-full': 'word#{text}{end}',\n",
    "    'fmt:text-orig-lite': 'word#{text_lite}{end}',\n",
    "    'fmt:text-trans-full': 'word#{full}{full_end}',\n",
    "    'fmt:text-trans-lite': 'word#{lite}{lite_end}',\n",
    "    'fmt:text-trans-fuzzy': 'word#{fuzzy}{fuzzy_end}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NenaTfBuilder:\n",
    "    \"\"\"Construct Text-Fabric graph resource from parsed JSON files\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir, output_dir, metadata, TF_methods, **TF_kwargs):\n",
    "        \"\"\"Load json data from input dir to prepare for TF conversion.\n",
    "        \n",
    "        Args:\n",
    "            input_dir: pathlib Path which is a directory that contains\n",
    "                subdirectories named after the respective dialects in the\n",
    "                text corpus. Each subdirectory should contain parsed JSON\n",
    "                texts ready for analysis.\n",
    "            output_dir: directory to save the .tf files\n",
    "            metadata: dictionary containing metadata on the corpus needed\n",
    "                to construct the TF graph\n",
    "            TF: Text-Fabric module loaded\n",
    "            **TF_kwargs: optional kwargs to pass to Text-Fabric loader\n",
    "        \"\"\"\n",
    "        \n",
    "        # load JSON data and initialize TF objects with paths\n",
    "        self.metadata = metadata\n",
    "        self.dialect2parsings = self.load_parsed_jsons(input_dir)\n",
    "        self.Fabric = TF_methods['Fabric'](locations=str(output_dir), **TF_kwargs)\n",
    "        self.cv = TF_methods['CV'](self.Fabric)\n",
    "        self.message = self.Fabric.tmObj # timestamped messages\n",
    "        \n",
    "    def load_parsed_jsons(self, dialect_dir):\n",
    "        \"\"\"Map directory of dialect subdirectories to parsed json data.\n",
    "        \n",
    "        Args:\n",
    "            dialect_dir: a pathlib Path that contains subdirectories\n",
    "                named after respective dialects; each subdirectory \n",
    "                contains parsed JSON files which are each a text\n",
    "        \n",
    "        Returns:\n",
    "            dict with structure of dict[dialect] = list(text_parsings)\n",
    "        \"\"\"\n",
    "        dialect2parsings = collections.defaultdict(list)\n",
    "        for dialect_dir in sorted(INPUT_DIR.glob('*')):\n",
    "            for text_file in sorted(dialect_dir.glob('*.json')):\n",
    "                dialect = dialect_dir.name\n",
    "                text_data = json.loads(text_file.read_text())\n",
    "                dialect2parsings[dialect].append(text_data)\n",
    "        return dialect2parsings\n",
    "    \n",
    "    def build(self, **walk_kwargs):\n",
    "        \"\"\"Executes the TF conversion on the loaded source\n",
    "        \n",
    "        Args:\n",
    "            walk_kwargs: optional. Keyword arguments to feed \n",
    "                to TF's cv.walk function.\n",
    "        \"\"\"\n",
    "        slot_type = 'letter'\n",
    "        self.good = self.cv.walk(\n",
    "            self.director,\n",
    "            slot_type,\n",
    "            **walk_kwargs,\n",
    "        )\n",
    "    \n",
    "    def dict_intersect(self, dict1, dict2):\n",
    "        \"\"\"Set intersection from one dict to another\"\"\"\n",
    "        return {k:v for k,v in dict1.items() if k in dict2}\n",
    "    \n",
    "    def director(self, cv):\n",
    "        \"\"\"Call cv methods to index the graph.\n",
    "        \n",
    "        This function does the bulk of the work of building the TF resource.\n",
    "        It operates in one large loop that walks over all parsed data. \n",
    "        The supplied cv Text-Fabric class possesses methods that create node \n",
    "        IDs and associate features with those IDs. These methods are called \n",
    "        throughout the loop. \n",
    "        \n",
    "        cv methods used here:\n",
    "            cv.slot: make a new slot, the atomic element of the graph. All \n",
    "                nodes active during an active slot will contain that slot.\n",
    "            cv.node: make a new node in the graph with supplied object name\n",
    "            cv.feature: add a string/integer feature to a supplied cv.node\n",
    "            cv.terminate: deactivate a given node; this ends any further\n",
    "                slot embeddings, which are calculated automatically from \n",
    "                whichever slots are activated while the node is also active.\n",
    "                \n",
    "        Further info about cv functionality can be referenced in the \n",
    "        Text-Fabric documentation.\n",
    "        \n",
    "        Args:\n",
    "            cv: Text-Fabric CV class loaded with Fabric\n",
    "        \"\"\"\n",
    "        \n",
    "        features = self.metadata['object_features']\n",
    "        text_features = {f for f in features if f['value'] == 'text'}\n",
    "        general_features = {f for f in features if f not in text_features}\n",
    "        nodes = {} # gets updated throughout\n",
    "    \n",
    "        def swap_node(node_type):\n",
    "            \"\"\"Replace any active nodes with new node.\"\"\"\n",
    "            try:\n",
    "                cv.terminate(nodes[node_type])\n",
    "                nodes[node_type] = cv.node(node_type)\n",
    "            except KeyError:\n",
    "                nodes[node_type] = cv.node(node_type)\n",
    "        \n",
    "        # parse all data for every dialect\n",
    "        for dialect, texts in self.dialect2parsings.items():\n",
    "            \n",
    "            # make dialect node / features\n",
    "            nodes['dialect'] = cv.node('dialect')\n",
    "            cv.feature(nodes['dialect'], dialect=dialect)\n",
    "            \n",
    "            # make text node / features\n",
    "            for text in texts:\n",
    "        \n",
    "                nodes['text'] = cv.node('text')\n",
    "                text_feats, paragraphs = text\n",
    "                cv.feature(nodes['text'], **self.dict_intersect(text_feats, features))\n",
    "                \n",
    "                for ith_paragraph, paragraph in enumerate(paragraphs):\n",
    "                    \n",
    "                    nodes['stress'] = cv.node('stress')\n",
    "                    nodes['inton'] = cv.node('inton')\n",
    "                    nodes['subsentence'] = cv.node('subsentence')\n",
    "                    nodes['sentence'] = cv.node('sentence')\n",
    "                    nodes['paragraph'] = cv.node('paragraph')\n",
    "                \n",
    "                    # -- Process Paragraph elements --\n",
    "                    # which are comprised of word and span tags\n",
    "                    \n",
    "                    # track span features for addition to words\n",
    "                    # features get updated by span triggers during element iteration\n",
    "                    span_feats = {\n",
    "                        'speaker': list(text_feats['speakers'].values())[0],\n",
    "                        'lang': 'NENA',\n",
    "                        'timestamp': None,\n",
    "                    }\n",
    "                    \n",
    "                    for ith_element, element in enumerate(paragraph):\n",
    "                        \n",
    "                        # -- process span elements --\n",
    "                        if element['class'] == 'span':\n",
    "                            \n",
    "                            # build line nodes\n",
    "                            if 'line_number' in element:\n",
    "                                swap_node('line')\n",
    "                                line_num = element['line_number']\n",
    "                                cv.feature(\n",
    "                                    nodes['line'], \n",
    "                                    line_number=line_num\n",
    "                                )\n",
    "                            \n",
    "                            # update other span fields\n",
    "                            span_feats.update(\n",
    "                                self.dict_intersect(element, span_feats)\n",
    "                            )\n",
    "                            \n",
    "                        # -- Process words, their letters, and their beginnings/ends --\n",
    "                        elif element['class'] == 'word':\n",
    "                            \n",
    "                            nodes['word'] = cv.node('word')\n",
    "                            word_features = {}\n",
    "                            word_features.update(span_feats)\n",
    "                            word_features.update(\n",
    "                                self.dict_intersect(element, general_features)\n",
    "                            )\n",
    "                        \n",
    "                            # 1. ** process word's letters and their features **\n",
    "                            # also get text from the letters\n",
    "                            for letter in element['letters']:\n",
    "                                letter_node = cv.slot()\n",
    "                                letter_features = self.dict_intersect(letter, features)\n",
    "                                \n",
    "                                # process features for letter\n",
    "                                # pass on text features to word \n",
    "                                for feature, value in letter_features.items():\n",
    "                                    cv.feature(letter_node, feature=value)\n",
    "                                    if feature in text_features:\n",
    "                                        word_features[feature] = word_features.get(feature,'') + value\n",
    "                                        \n",
    "                                cv.terminate(letter_node)\n",
    "    \n",
    "                            # 2. ** process word's parsing features **\n",
    "                            # current process allows multiple parsings to co-exist\n",
    "                            # thus we construct a composite parsestring for each \n",
    "                            # feature\n",
    "                            parse_values = collections.defaultdict(list)\n",
    "                            word_features['n_parses'] = len(element['parsings'])\n",
    "                            for parse in element['parsings']:\n",
    "                                # gather feature / val strings to be joined next\n",
    "                                for feat, val in parse.items():\n",
    "                                    parse_values[feat].append(val)\n",
    "                            \n",
    "                            # now add parse features to word features\n",
    "                            # join multiple parse features on |\n",
    "                            for feat, vals in parse_values.items():\n",
    "                                word_features[feat] = '|'.join(vals)\n",
    "                            \n",
    "                            # 3. ** Process beginnings on a word **\n",
    "                            for begin in element['beginnings']:\n",
    "                                \n",
    "                                # build begin strings\n",
    "                                text_features = self.dict_intersect(end, text_features)\n",
    "                                text_feat_prfx = {k+'_begin':v for k,v in text_features.items()}\n",
    "                                for feat_val in text_feat_prfx.items():\n",
    "                                    word_features[feat] = word_features.get(feat,'') + val\n",
    "                            \n",
    "                            # 4. ** Process endings on a word ** \n",
    "                            # mark sentence/subsentence/inton/stress bounds on endings of word\n",
    "                            # also add endings as their own text features of a word, with _end suffix\n",
    "                            detected_boundaries = set()\n",
    "                            for end in element['endings']:\n",
    "                                \n",
    "                                # build end strings\n",
    "                                text_features = self.dict_intersect(end, text_features)\n",
    "                                text_feat_sffx = {k+'_end':v for k,v in text_features.items()}\n",
    "                                for feat, val in text_feat_suffx.items():\n",
    "                                    word_features[feat] = word_features.get(feat,'') + val\n",
    "                                \n",
    "                                # skip non-separating puncts\n",
    "                                if end['class'] != 'separator':\n",
    "                                    continue\n",
    "                                \n",
    "                                # detect stress bounds; end at any of the following:\n",
    "                                if end['modifies'] in {'word', 'intonation group',  'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('stress')\n",
    "                                \n",
    "                                # detect inton bounds\n",
    "                                if end['modifies'] in {'intonation group', 'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('inton')\n",
    "                                \n",
    "                                # detect subsentence bounds\n",
    "                                if end['modifies'] in {'subsentence', 'sentence'}:\n",
    "                                    detected_boundaries.add('subsentence')\n",
    "                                \n",
    "                                # detect sentence bounds\n",
    "                                if end['modifies'] in {'sentence'}:\n",
    "                                    detected_boundaries.add('sentence')\n",
    "                                \n",
    "                            # end the word and execute boundary divisions\n",
    "                            cv.feature(nodes['word'], **word_features)\n",
    "                            cv.terminate(nodes['word'])\n",
    "                            for bound in detected_boundaries:\n",
    "                                if ith_element+1 != len(paragraph):\n",
    "                                    swap_node(bound)\n",
    "                                else:\n",
    "                                    cv.terminate(nodes[bound])\n",
    "\n",
    "                    # we've come to the end of the paragraph\n",
    "                    # we do some house-cleaning before finishing with the paragraph\n",
    "                    \n",
    "                    # do a sanity check for un-closed intons, subsentences, sentences\n",
    "                    # possibly due to lack of proper punctuation in the source text (to be fixed later)\n",
    "                    title = text_feats['title']\n",
    "                    for obj in {'stress', 'inton', 'sentence', 'subsentence'} & cv.activeTypes():\n",
    "                        sys.stderr.write(f'force-closing {obj} in {title}, ยง{ith_paragraph}.{ith_element}\\n')\n",
    "                        cv.terminate(nodes[obj])\n",
    "                    \n",
    "                    # check for active line on last paragraph\n",
    "                    # thus line can straddle paragraphs\n",
    "                    # but obviously it should not straddle texts!\n",
    "                    if (ith_paragraph+1 == len(paragraphs)) & nodes.get('line', None):\n",
    "                        cv.terminate(nodes['line'])\n",
    "                    \n",
    "                    # close shop on the ยง\n",
    "                    cv.terminate(nodes['paragraph'])\n",
    "                        \n",
    "                # -- trigger section node endings --\n",
    "                cv.terminate(nodes['text'])\n",
    "            cv.terminate(nodes['dialect'])\n",
    "    \n",
    "#     def exclude_keys(self, dicti, *exclusions):\n",
    "#         \"\"\"Filter dictionary to exclude keys\"\"\"\n",
    "#         return {k:v for k,v in dicti.items() if k not in exclusions}\n",
    "    \n",
    "#     def build_text(self, letters, text_features):\n",
    "#         \"\"\"Construct text representations of word based on available features.\n",
    "        \n",
    "#         Letters need to be re-joined to form a word's text representation.\n",
    "#         This method constructs all available text representations from a \n",
    "#         list of letter dicts.\n",
    "#         \"\"\"\n",
    "#         text_forms = {}\n",
    "#         for feat in text_features:\n",
    "#             try:\n",
    "#                 text_forms[feat] = self.join_letters(letters, feat)\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#         return text_forms\n",
    "    \n",
    "#     def join_letters(self, letters, feature, on=''):\n",
    "#         \"\"\"Joins letters based on given text feature\"\"\"\n",
    "#         letter_text = [l[feature] for l in letters]\n",
    "#         return f'{on}'.join(letter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nena_builder = NenaTfBuilder(INPUT_DIR, OUTPUT_DIR, corpus_meta, TF_methods, silent='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |   SECTION   TYPES:    \n",
      "   |   SECTION   FEATURES: \n",
      "   |   STRUCTURE TYPES:    \n",
      "   |   STRUCTURE FEATURES: \n",
      "   |   TEXT      FEATURES:\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'textFormats' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-cd63a5864a9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnena_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerateTf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-d5af6ed759f8>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, **walk_kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m     52\u001b[0m         \u001b[0mslot_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'letter'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         self.good = self.cv.walk(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mslot_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tf/convert/walker.py\u001b[0m in \u001b[0;36mwalk\u001b[0;34m(self, director, slotType, otext, generic, intFeatures, featureMeta, warn, generateTf, force)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mindent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepareMeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0motext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mindent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tf/convert/walker.py\u001b[0m in \u001b[0;36m_prepareMeta\u001b[0;34m(self, otext, generic)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TEXT      FEATURES:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mindent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextFormats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{fmt:<20} {\", \".join(sorted(feats))}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0mindent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'textFormats' referenced before assignment"
     ]
    }
   ],
   "source": [
    "nena_builder.build(generateTf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nena_builder.dialect2parsings['Barwar'][0][1][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nena_builder.metadata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
