{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NENA to TF\n",
    "\n",
    "This notebook will be used to develop code for converting texts from .nena format to Text-Fabric. The parser has principally been written by Hannes Vlaardingerbroek. Many thanks to him for his hard work on it. Updates and refinements have been added by Cody Kingham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated\n",
      "Mon 16 Mar 2020 20:08:54 GMT\n"
     ]
    }
   ],
   "source": [
    "! echo \"last updated\"; date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 17 shift/reduce conflicts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "import tabulate\n",
    "from pathlib import Path\n",
    "from tf.convert.walker import CV\n",
    "from tf.fabric import Fabric\n",
    "from Levenshtein import distance\n",
    "\n",
    "# path to parser\n",
    "parserpath = f'../../nena_corpus/parse_nena/'\n",
    "sys.path.append(parserpath)\n",
    "from nena_parser import NenaLexer, NenaParser\n",
    "\n",
    "# paths\n",
    "corpus = Path('/Users/cody/github/CambridgeSemiticsLab/nena_corpus')\n",
    "VERSION = '0.01'\n",
    "OUT_DIR = Path(f'../tf/{VERSION}')\n",
    "data_dir = corpus.joinpath(f'nena/{VERSION}')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))\n",
    "glossary = corpus.joinpath(f'glossaries/bar glossary general.txt')\n",
    "\n",
    "# open char tables\n",
    "# trans_lite_table = Path('../char_tables/trans_lite.tsv')\n",
    "# with open(trans_lite_table, 'r') as infile:\n",
    "#     trans_data = list(csv.reader(infile, delimiter='\\t'))[1:]\n",
    "#     trans_lite = {unicodedata.normalize('NFC', td[0]):td[1] for td in trans_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = Fabric(locations=[str(OUT_DIR)], silent=True)\n",
    "cv = CV(TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NENA Parser\n",
    "\n",
    "The NENA Parser delivers the text as structured morphemes, which can then be processed into a TF graph. We do that below by opening each source text, retrieving its parsed form, and begin each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = NenaLexer()\n",
    "parser = NenaParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialect2file2parsed = collections.defaultdict(lambda: collections.defaultdict())\n",
    "\n",
    "# nparsed = 0\n",
    "\n",
    "# for dialect in sorted(dialect_dirs):    \n",
    "#     print()\n",
    "#     print(dialect.name)\n",
    "#     for file in sorted(dialect.glob('*.nena')):\n",
    "#         with open(file, 'r') as infile:\n",
    "#             text = infile.read()\n",
    "#             print(f'parsing: {file.name}')\n",
    "#             parse = parser.parse(lexer.tokenize(text))\n",
    "#             nparsed += 1\n",
    "#             dialect2file2parsed[dialect.name][file.name] = parse\n",
    "            \n",
    "\n",
    "# print('\\n', nparsed, 'texts ready for conversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linenum, elements = dialect2file2parsed['Urmi_C']['Village Life.nena'][1][0]\n",
    "# eg_morph = elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialect2file2parsed['Barwar']['A Hundred Gold Coins.nena'][1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entry(text):\n",
    "    \"\"\"Normalize text by removing style elements/quirks\"\"\"\n",
    "    # remove trailing spaces\n",
    "    text = text.strip()\n",
    "    # remove style asterixes\n",
    "    text = text.replace('*', '')\n",
    "    # remove begin/end single quotes\n",
    "    text = re.sub('^\\u0027|\\u0027$', '', text)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    return text\n",
    "    \n",
    "with open(glossary,'r') as infile:\n",
    "    glosses = infile.read()\n",
    "    glosses = re.split('\\n\\n', glosses)\n",
    "    gloss_data = []\n",
    "    for lemma in glosses:\n",
    "        lemma_data = {}\n",
    "        for dataline in lemma.split('\\n'):\n",
    "            key, value = dataline.split(':', 1)\n",
    "            key,value = key.strip(), value.strip()\n",
    "            lemma_data[key] = value\n",
    "            \n",
    "        gloss_data.append(lemma_data)\n",
    "        \n",
    "# clean up and prep nena gloss data for \n",
    "# matching with instances in the text\n",
    "nena_glosses = {}\n",
    "for gl_id, gloss in enumerate(gloss_data):\n",
    "    data = {}\n",
    "    data['lemmas'] = tuple(\n",
    "        normalize_entry(ent) for ent in re.split('[,;]', gloss['lemma'])\n",
    "    )\n",
    "    data['lemma'] = normalize_entry(gloss['lemma'])\n",
    "    data['grm_desc'] = normalize_entry(gloss.get('grm_desc', ''))\n",
    "    forms = eval(gloss.get('forms', '[]'))\n",
    "    data['forms'] = [\n",
    "        (fdata, normalize_entry(form)) \n",
    "            for fdata, forms in forms\n",
    "                for form in re.split('[;,]', forms)\n",
    "    ]\n",
    "    data['examples'] = unicodedata.normalize('NFC',\n",
    "        re.sub('^\\u0027|\\u0027$', '', gloss.get('examples',''))\n",
    "    )\n",
    "    data['trans'] = normalize_entry(\n",
    "        re.sub('\\(§[\\d.,;\\s§]+\\)', '', gloss.get('trans',''))\n",
    "    )\n",
    "    \n",
    "    data['lang'] = normalize_entry(gloss.get('lang',''))\n",
    "    data['ref'] = normalize_entry(gloss.get('ref', ''))\n",
    "    nena_glosses[gl_id] = data\n",
    "    \n",
    "# prepare matchset for matching surface forms\n",
    "gloss_matchset = []\n",
    "\n",
    "for lem_id, lemdat in nena_glosses.items():\n",
    "    for lemma in lemdat['lemmas']:\n",
    "        lemma = lemma.strip('-')\n",
    "        gloss_matchset.append(\n",
    "            (re.compile(f'^{lemma}$'), (lem_id, ''))\n",
    "        )\n",
    "    for form in lemdat['forms']:\n",
    "        form_type = form[0]\n",
    "        for formstring in form[1:]:\n",
    "            if not formstring:\n",
    "                continue\n",
    "            formstring = formstring.strip('-')\n",
    "            gloss_matchset.append(\n",
    "                (re.compile(f'^{formstring}$'), (lem_id, form_type))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6674 surface forms ready for match attempts...\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(gloss_matchset)} surface forms ready for match attempts...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(re.compile(r'^ʾabaya$', re.UNICODE), (0, '')),\n",
       " (re.compile(r'^ʾabbaya$', re.UNICODE), (0, ''))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloss_matchset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1, 10):\n",
    "#     print(nena_glosses[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_gloss(string):\n",
    "    \"\"\"A function to match strings to glosses\n",
    "    \n",
    "    Matches can be made at the word or the morpheme level.\n",
    "    \"\"\"\n",
    "    for pattern, pattern_data in gloss_matchset:\n",
    "        if pattern.match(string):\n",
    "            return pattern_data\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_full = {\n",
    "#     # non-latin vowels\n",
    "#     '\\u0131': '1',  # 0x0131 ı dotless i\n",
    "#     '\\u0251': '@',  # 0x0251 ɑ alpha\n",
    "#     '\\u0259': '3',  # 0x0259 ə schwa\n",
    "#     '\\u025B': '$',  # 0x025B ɛ open e\n",
    "#     # vowel accents\n",
    "#     '\\u0300': '`',  # 0x0300 à grave\n",
    "#     '\\u0301': \"'\",  # 0x0301 á acute\n",
    "#     '\\u0304': '_',  # 0x0304 ā macron\n",
    "#     '\\u0306': '%',  # 0x0306 ă breve\n",
    "#     '\\u0308': '\"',  # 0x0308 ä diaeresis\n",
    "#     '\\u0303': '~',  # 0x0303 ã tilde\n",
    "#     '\\u02C8': '', # 0x2c8 ˈ small vertical line\n",
    "#     # non-latin consonants\n",
    "#     '\\u00F0': '6',  # 0x00F0 ð eth\n",
    "#     '\\u025F': '&',  # 0x025F ɟ small dotless j with stroke\n",
    "#     '\\u0248': '!',  # 0x0248 Ɉ capital J with stroke\n",
    "#     '\\u03B8': '8',  # 0x03B8 θ greek theta\n",
    "#     '\\u02B8': '7',  # 0x02B8 ʸ small superscript y\n",
    "#     '\\u02BE': '}',  # 0x02BE ʾ right half ring (alaph)\n",
    "#     '\\u02BF': '{',  # 0x02BF ʿ left half ring (ayin)\n",
    "#     # consonant diacritics\n",
    "#     '\\u207A': '+',  # 0x207A ⁺ superscript plus\n",
    "#     '\\u030C': '<',  # 0x030C x̌ caron\n",
    "#     '\\u0302': '^',  # 0x0302 x̂ circumflex\n",
    "#     '\\u0307': ';',  # 0x0307 ẋ dot above\n",
    "#     '\\u0323': '.',  # 0x0323 x̣ dot below\n",
    "#     '\\u032D': '>',  # 0x032D x̭ circumflex below\n",
    "    \n",
    "#     # punctuation\n",
    "#     '\\u02C8': '|', # 0x2c8 ˈ small vertical line\n",
    "# }\n",
    "# def trans(s, table, mark_punct=True):\n",
    "#     '''\n",
    "#     Transcribes a text.\n",
    "#     '''\n",
    "#     s = unicodedata.normalize('NFD', s)\n",
    "#     # mark punctuation \n",
    "#     if mark_punct:\n",
    "#         s = re.sub('([\\n,.!?:;/])', r'/\\g<1>', s, 1)\n",
    "#     return ''.join([table.get(c, c) for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriber:\n",
    "    \"\"\"Transcribe a string according to transcription rules.\n",
    "    \n",
    "    This transcription class is essentially a filter\n",
    "    which determines which characters make it into a new,\n",
    "    transcribed string. The filter is applied on a letter-by-letter \n",
    "    basis. A \"letter\" (token) is defined by the `tokens` argument \n",
    "    and can include diacritics/accents. The filter is applied \n",
    "    in one of three methods:\n",
    "        1. replacements on a unicode composed letter (NFC)\n",
    "        2. or replacements on punctuation if letter is punctuation\n",
    "        3. or replacements on a unicode decomposed letter (NFD)\n",
    "    The changes are added to a new string which is then returned.\n",
    "    \n",
    "    __init__(tokens, replacements, punctuation, keep):\n",
    "        string: a string to transcribe\n",
    "        tokens: regex for splitting letters (tokens)\n",
    "            to be used with findall\n",
    "        replacements: dict with find:replace mappings\n",
    "        keep: regex for characters to keep\n",
    "            \n",
    "    Returns:\n",
    "        str in transcribed form\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens='', replace={}, \n",
    "                 punctuation='', keep='', keep_case=False):    \n",
    "        self.tokenize = re.compile(f'{tokens}|{punctuation}').findall\n",
    "        self.punct = re.compile(punctuation)\n",
    "        self.keep = re.compile(keep)\n",
    "        self.keep_case = keep_case\n",
    "        \n",
    "        # ensure normalized characters for pattern searches\n",
    "        self.repl = {\n",
    "            unicodedata.normalize('NFC',f):r \n",
    "                for f,r in replace.items()\n",
    "        }\n",
    "        \n",
    "    def convert(self, string):\n",
    "        \"\"\"Convert string to transcription.\n",
    "        \n",
    "        Returns:\n",
    "            str in transcribed form\n",
    "        \"\"\"\n",
    "        \n",
    "        string = unicodedata.normalize('NFC',string)\n",
    "        if not self.keep_case:\n",
    "            string = string.lower()\n",
    "        transcription = ''\n",
    "\n",
    "        for token in self.tokenize(string):\n",
    "\n",
    "            # filter a composed string\n",
    "            if token in self.repl:\n",
    "                transcription += self.repl[token]\n",
    "\n",
    "            # keep punctuation\n",
    "            elif self.punct.match(token):\n",
    "                transcription += token\n",
    "\n",
    "            # filter at \n",
    "            else:\n",
    "                for char in unicodedata.normalize('NFD', token):\n",
    "\n",
    "                    # attempt second match on char by char basis\n",
    "                    if char in self.repl:\n",
    "                        transcription += self.repl[char]\n",
    "\n",
    "                    # attempt to keep with keep-set\n",
    "                    elif self.keep.match(char):\n",
    "                        transcription += char   \n",
    "                        \n",
    "        return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_full = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "   \n",
    "    # char combinations\n",
    "    'p̌':'p<',\n",
    "    'ṭ': 't',\n",
    "    'ð̣': '6',\n",
    "\n",
    "    # non-latin vowels\n",
    "    '\\u0131': 'i',  # 0x0131 ı dotless i\n",
    "    '\\u0251': 'a',  # 0x0251 ɑ alpha\n",
    "    '\\u0259': '9',  # 0x0259 ə schwa\n",
    "    '\\u025B': '3',  # 0x025B ɛ open e\n",
    "    \n",
    "    # vowel accents\n",
    "    '\\u0300': '`',  # 0x0300 à grave\n",
    "    '\\u0301': \"'\",  # 0x0301 á acute\n",
    "    '\\u0304': '_',  # 0x0304 ā macron\n",
    "    '\\u0306': '%',  # 0x0306 ă breve\n",
    "    '\\u0308': '\"',  # 0x0308 ä diaeresis\n",
    "    '\\u0303': '~',  # 0x0303 ã tilde\n",
    "    '\\u02C8': '', # 0x2c8 ˈ small vertical line\n",
    "        \n",
    "    # non-latin consonants\n",
    "    '\\u00F0': '6',  # 0x00F0 ð eth\n",
    "    '\\u025F': '4',  # 0x025F ɟ small dotless j with stroke\n",
    "    '\\u0248': '4',  # 0x0248 Ɉ capital J with stroke\n",
    "    '\\u03B8': '8',  # 0x03B8 θ greek theta\n",
    "    '\\u02B8': '7',  # 0x02B8 ʸ small superscript y\n",
    "    '\\u02BE': ')',  # 0x02BE ʾ right half ring (alaph)\n",
    "    '\\u02BF': '(',  # 0x02BF ʿ left half ring (ayin)\n",
    "        \n",
    "    # consonant diacritics\n",
    "    '\\u207A': '+',  # 0x207A ⁺ superscript plus\n",
    "    '\\u030C': '>',  # 0x030C x̌ caron\n",
    "    '\\u0302': '^',  # 0x0302 x̂ circumflex\n",
    "    '\\u0307': ';',  # 0x0307 ẋ dot above\n",
    "    '\\u0323': '.',  # 0x0323 x̣ dot below\n",
    "    '\\u032D': '<',  # 0x032D x̭ circumflex below\n",
    "    \n",
    "    # punctuation\n",
    "    '\\u02C8': '|', # 0x2c8 ˈ small vertical line\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "trans_lite = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*', \n",
    "    'replace': {\n",
    "        'ʾ': ')',\n",
    "        'ʿ': '(',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '%',\n",
    "        'ḍ': 'D',\n",
    "        'ð': '6',\n",
    "        'ð̣': '^',\n",
    "        'ġ': 'G',\n",
    "        'ḥ': 'H',\n",
    "        'ɟ': '4',\n",
    "        'Ɉ': '4',\n",
    "        'k̭': '&',\n",
    "        'ḷ': 'L',\n",
    "        'ṃ': 'M',    \n",
    "        'p̣': 'P',\n",
    "        'ṛ': 'R',\n",
    "        'ṣ': 'S',\n",
    "        'š': '$',\n",
    "        'ṱ': '+',\n",
    "        'ṭ': 'T',\n",
    "        'θ': '8',\n",
    "        'ž': '7',\n",
    "        'ẓ': 'Z',\n",
    "        'ā̀': 'A',\n",
    "        'ā́': 'A',\n",
    "        'ă': '@',\n",
    "        'ắ': '@',\n",
    "        'ằ': '@',\n",
    "        'ē': 'E',\n",
    "        'ɛ': '3',\n",
    "        'ī': 'I',\n",
    "        'ĭ': '9',\n",
    "        'ə': '9',\n",
    "        'o': 'o',\n",
    "        'ō': 'O',\n",
    "        'ū': 'U',\n",
    "        'ŭ': '2',\n",
    "        'ı': 'i',\n",
    "        'ɑ': 'a',\n",
    "        'ˈ': '|'\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_urmi = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'c': 'k',\n",
    "        'c̭': 'k',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'q',\n",
    "        'ɟ': 'g',\n",
    "        'Ɉ': 'g',\n",
    "        'ə': 'i',\n",
    "        'v': 'w',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_barwar = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'k',\n",
    "        'θ': 't',\n",
    "        'ð': 'd',\n",
    "        'ɛ': 'e',\n",
    "        'ə': 'i',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "trans_text = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "    },\n",
    "    'punctuation': '[-–]',\n",
    "    'keep': '[^\\s.,?!:;\\u02C8]+',\n",
    "}\n",
    "\n",
    "def normalize_nena(word):\n",
    "    \"\"\"Strip accents and spaces from NENA text on a node.\n",
    "    \n",
    "    Args:\n",
    "        word: a node number to get normalized text\n",
    "    \"\"\"\n",
    "    accents = '\\u0300|\\u0301|\\u0304|\\u0306|\\u0308|\\u0303'\n",
    "    norm = unicodedata.normalize('NFD', word) # decompose for accent stripping\n",
    "    norm = re.sub(accents, '', norm) # strip accents\n",
    "    return unicodedata.normalize('NFC', norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for char in unicodedata.normalize('NFD','č̣'):\n",
    "#     print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_test = Transcriber(**fuzzy_barwar)\n",
    "# trans_test_full = Transcriber(**trans_full)\n",
    "\n",
    "test = 'ʾə́č̣č̣a dáwe zìle mə́nne'\n",
    "\n",
    "trans_test = Transcriber(**trans_lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "')9%%a dawe zile m9nne'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_test.convert(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bəṱ-lábən'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'bəṱ-lábən.ˈ'\n",
    "trans_test = Transcriber(**trans_text)\n",
    "trans_test.convert(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "slotType = 'letter'\n",
    "\n",
    "otext = {\n",
    "    'sectionTypes': 'dialect,text,line',\n",
    "    'sectionFeatures': 'dialect,title,number',\n",
    "    'fmt:text-orig-full': '{text}{end}',\n",
    "    'fmt:text-trans-full': '{full}{full_end}',\n",
    "    'fmt:text-trans-lite': '{lite}{lite_end}',\n",
    "    'fmt:text-trans-fuzzy': '{fuzzy}{fuzzy_end}',\n",
    "}\n",
    "\n",
    "description = ''.join(\"\"\"\n",
    "The NENA linguistic corpus is derived from decades of \n",
    "field work by Prof. Geoffrey Khan and his students.\n",
    "\"\"\".split('\\n'))\n",
    "\n",
    "generic = {\n",
    "    'origin': 'Cambridge University, Faculty of Asian and Middle Eastern Studies',\n",
    "    'author': 'Geoffrey Khan et al.',\n",
    "    'editors': 'Cody Kingham, Paul Noorlander, James Strachan, Hannes Vlaardingerbroek',\n",
    "    'researchers': 'Dorota Molin, Johan Lundberg',\n",
    "    'source': description,\n",
    "    'url': 'https://github.com/CambridgeSemiticsLab/nena_tf',\n",
    "}\n",
    "\n",
    "intFeatures = {'number'}\n",
    "\n",
    "d = 'about'\n",
    "feature_type = 'feature_type'\n",
    "\n",
    "featureMeta = {\n",
    "    'dialect': {d: 'name of a dialect in Northeastern Neo-Aramaic', feature_type:'categorical'},\n",
    "    'title': {d: 'title of a text (story)', feature_type:'string'},\n",
    "    'version': {d: 'version of the story if there are multiple instances of the same story', feature_type:'categorical'},\n",
    "    'number': {d: 'sequential number of a paragraph or line within a text or paragraph, respectively', feature_type:'integer'},\n",
    "    'text': {d: 'plain text representation of a letter, morpheme, or word', feature_type:'text'},\n",
    "    'text_norm': {d: 'plain text without accents', feature_type:'text'},\n",
    "    'full': {d: 'full transcription, one-to-one transcription of a letter, morpheme, or word', feature_type:'text'},\n",
    "    'lite': {d: 'lite transcription of a letter, morpheme, or word, without vowel accents', feature_type:'text'},\n",
    "    'fuzzy': {d: 'fuzzy transcription that leaves out most diacritics and maps certain characters in certain dialects to common characters', feature_type:'text'},\n",
    "    'end': {d: 'space, punctuation, or other stylistic text at the end of a morpheme or word', feature_type:'text'},\n",
    "    'full_end': {d: 'full transcription of punctuation or other stylistic text at the end of a morpheme or word; see also trans_f', feature_type:'text'},\n",
    "    'lite_end': {d: 'lite transcription of punctuation or other stylistic text at the end of a morpheme or word, excluding intonation boundary markers; see also trans_l', feature_type:'text'},\n",
    "    'fuzzy_end': {d: 'fuzzy transcription of punctuation or other stylistic text at the end of a morpheme or word, excluding intonation boundary markers; see also trans_l', feature_type:'text'},\n",
    "    'speaker': {d: 'name or initials of person speaking a morpheme or word; see also informant', feature_type:'string'},\n",
    "    'footnotes': {d: 'explanatory footnote on a morpheme or text', feature_type:'string'},\n",
    "    'lang': {d: 'language of a morpheme foreign to a text', feature_type:'categorical'},\n",
    "    'foreign': {d: 'indicates whether a morpheme is foreign to a text; see also lang', feature_type:'string'},\n",
    "    'comment': {d: 'explanatory comment inserted in the text, stored on a morpheme', feature_type:'string'},\n",
    "    'continued_from': {d: 'text is a follow-up to the named text', feature_type:'string'},\n",
    "    'informant': {d: 'name of person who spoke these words', feature_type:'categorical'},\n",
    "    'place': {d: 'place a text was recorded', feature_type:'categorical'},\n",
    "    'source': {d: 'name of the file from which a text was converted', feature_type:'string'},\n",
    "    'text_id': {d: 'id of a text within its original publication; can overlap between publications', feature_type:'string'},\n",
    "    'class': {d: 'class of a letter (consonant or vowel)', feature_type:'categorical'},\n",
    "    'lemma': {d: 'lemma of a word', feature_type:'string'},\n",
    "    'lemma_form': {d: 'grammatical form of a word lemma', feature_type:'string'},\n",
    "    'grm_desc': {d: 'grammatical description of a word lemma', feature_type:'categorical'},\n",
    "    'gloss': {d: 'English gloss of a word lemma', feature_type:'string'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converter\n",
    "\n",
    "Build a TF Walker class that can walk over the NENA parsed data and fit the text graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def director(CV):\n",
    "    \"\"\"Walk the source data and produce a TF graph\"\"\"\n",
    "    \n",
    "    info = TF.tm.info\n",
    "    \n",
    "    # transcriptions particular to dialects\n",
    "    dialect2fuzzy = {\n",
    "        'Barwar': Transcriber(**fuzzy_barwar),\n",
    "        'Urmi_C': Transcriber(**fuzzy_urmi),\n",
    "    }\n",
    "    \n",
    "    matched_glosses = set() # track matched glosses from gloss-set\n",
    "    \n",
    "    def make_footnotes(fn_dict):\n",
    "        \"\"\"Format footnote dict into string\"\"\"\n",
    "        if fn_dict:\n",
    "            return '; '.join(\n",
    "                f'[^{num}]: {txt}' for num, txt in fn_dict.items()\n",
    "            )\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    t_text = Transcriber(**trans_text) # full text transcription without punctuation\n",
    "    t_full = Transcriber(**trans_full) # transcription full\n",
    "    t_lite = Transcriber(**trans_lite) # transcription lite feature\n",
    "\n",
    "    def make_wordfeats(mfeat_list, ignore={}, dialect=''):\n",
    "        \"\"\"Convert a list of morpheme feature dicts into one for a word.\n",
    "\n",
    "        Features stored on a word must be inherited in special ways\n",
    "        for words. For example, a word's \"end\" feature should be \n",
    "        the last morpheme, not all of the ends. Those features \n",
    "        are specially processed here.\n",
    "        \"\"\"\n",
    "\n",
    "        # gather word features here\n",
    "        word_fs = collections.defaultdict(set)\n",
    "\n",
    "        # add features\n",
    "        for mfeats in mfeat_list:\n",
    "            for feat,val in mfeats.items():\n",
    "                if feat in ignore:\n",
    "                    continue\n",
    "                else:\n",
    "                    word_fs[feat].add(val)\n",
    "\n",
    "        # handle special cases\n",
    "        word_fs['end'] = mfeat_list[-1]['end']\n",
    "        word_fs['full_end'] = mfeat_list[-1]['full_end']\n",
    "        word_fs['lite_end'] = mfeat_list[-1]['lite_end']\n",
    "        word_fs['fuzzy_end'] = mfeat_list[-1]['fuzzy_end']\n",
    "        word_fs['text'] = t_text.convert(''.join(\n",
    "            mf['text']+mf['end'] for mf in mfeat_list\n",
    "        ))\n",
    "        # add transcription with end, leaving off the end from the \n",
    "        # last morpheme\n",
    "        trans_parts = [('full', 'full_end'), ('lite', 'lite_end'), ('fuzzy', 'fuzzy_end')]\n",
    "        for trans, end in trans_parts:\n",
    "            word_fs[trans] = ''\n",
    "            for i,mf in enumerate(mfeat_list):\n",
    "                word_fs[trans] += mf[trans]\n",
    "                if i+1 != len(mfeat_list):\n",
    "                    word_fs[trans] += mf[end]\n",
    "\n",
    "        # convert to strings and handle duplicates\n",
    "        for feat,val in word_fs.items():\n",
    "            if type(val) == set:\n",
    "                val = {v for v in val if v}\n",
    "                if val:\n",
    "                    word_fs[feat] = ' '.join(val)\n",
    "                else:\n",
    "                    word_fs[feat] = None\n",
    "\n",
    "        return word_fs\n",
    "    \n",
    "    for dialect_dir in sorted(dialect_dirs):  \n",
    "        \n",
    "        # make dialect node\n",
    "        dialect = cv.node('dialect')\n",
    "        dia = dialect_dir.name\n",
    "        cv.feature(dialect, dialect=dia)\n",
    "        \n",
    "        # retrieve fuzzy transcription particular to dialect\n",
    "        t_fuzzy = dialect2fuzzy[dia]\n",
    "        \n",
    "        # process file into TF graph\n",
    "        for file in sorted(dialect_dir.glob('*.nena')):\n",
    "            \n",
    "            info(f'processing: [{file}]')\n",
    "            \n",
    "            with open(file, 'r') as infile:\n",
    "                nena_text = infile.read()\n",
    "            \n",
    "            # parse the .nena format\n",
    "            header, paragraphs = parser.parse(lexer.tokenize(nena_text))\n",
    "            \n",
    "            # -- begin TF node creation --\n",
    "            \n",
    "            # cv.node initializes a node object\n",
    "            # all slots added in between its creation and \n",
    "            # termination will be considered embedded within\n",
    "            # this node; same is true of following cv.node calls\n",
    "            text = cv.node('text')\n",
    "            cv.feature(text, **header) # adds features to supplied node\n",
    "            title = header['title']\n",
    "            \n",
    "            for i, para in enumerate(paragraphs):\n",
    "                \n",
    "                # TODO: Process footnotes here\n",
    "                if len(para[0]) != 2:\n",
    "                    continue\n",
    "                \n",
    "                # make paragraph node\n",
    "                paragraph = cv.node('paragraph')\n",
    "                cv.feature(paragraph, number=i+1)\n",
    "                \n",
    "                for line_num, line_elements in para:\n",
    "                    \n",
    "                    # make line nodes\n",
    "                    line = cv.node('line')\n",
    "                    cv.feature(line, number=line_num)\n",
    "                    \n",
    "                    # Make linguistic nodes by parsing morphemes.\n",
    "                    # This must be done iteratively and composed\n",
    "                    # based on characters at the end of each morpheme. \n",
    "                    # Punctuation signals intonation/subsentence/sentence \n",
    "                    # boundaries; spaces and hyphens signal word bounds. \n",
    "                    # This is handled in the loop below.\n",
    "                    word = cv.node('word')\n",
    "                    inton = cv.node('inton')\n",
    "                    subsentence = cv.node('subsentence')\n",
    "                    sentence = cv.node('sentence')\n",
    "                    word_features = [] # store morphs feats here for processing\n",
    "\n",
    "                    for i, elem in enumerate(line_elements):\n",
    "                        \n",
    "                        is_end = i+1 == len(line_elements)\n",
    "\n",
    "                        # add morphemes as slots\n",
    "                        # 'slot' being the most basic element\n",
    "                        if elem.__class__.__name__ == 'Morpheme':\n",
    "                            \n",
    "                            # make morpheme node\n",
    "                            morph = cv.node('morpheme')\n",
    "                            \n",
    "                            # access/prepare morph features\n",
    "                            fs = elem.__dict__\n",
    "                            trailer = elem.trailer.replace('/', '\\n')\n",
    "                            \n",
    "                            # package & edit morph features for cv\n",
    "                            # NB: None values are ignored by default\n",
    "                            m_string = ''.join(elem.value)\n",
    "                            feats = {\n",
    "                                #  make string representations\n",
    "                                'text': t_text.convert(m_string),\n",
    "                                'text_norm': normalize_nena(t_text.convert(m_string)),\n",
    "                                'full': t_full.convert(m_string),\n",
    "                                'lite': t_lite.convert(m_string),\n",
    "                                'fuzzy': t_fuzzy.convert(m_string),\n",
    "                                'end': trailer,\n",
    "                                \n",
    "                                # make punctuation strings at end of morpheme\n",
    "                                'full_end': t_full.convert(trailer),\n",
    "                                'lite_end': t_lite.convert(trailer),\n",
    "                                'fuzzy_end': t_fuzzy.convert(trailer),\n",
    "                                \n",
    "                                # make metadata features\n",
    "                                'speaker': fs.get('speaker') or header.get('informant'),\n",
    "                                'footnotes': make_footnotes(fs.get('footnotes', {})),\n",
    "                                'lang': fs.get('lang'),\n",
    "                                'foreign': str(fs.get('foreign')) if fs.get('foreign') else None,\n",
    "                            }\n",
    "                            \n",
    "                            # attempt gloss matches\n",
    "                            if dia == 'Barwar':\n",
    "                                gloss_match = match_gloss(feats['text_norm'])\n",
    "                                if gloss_match:\n",
    "                                    lem_id, lem_form = gloss_match\n",
    "                                    feats['lemma'] = nena_glosses[lem_id]['lemma']\n",
    "                                    feats['lemma_form'] = lem_form or None\n",
    "                                    feats['grm_desc'] = nena_glosses[lem_id].get('grm_desc')\n",
    "                                    feats['gloss'] = nena_glosses[lem_id].get('trans')\n",
    "                                \n",
    "                                    # track which forms are found\n",
    "                                    matched_glosses.add(gloss_match)\n",
    "                            \n",
    "                            # make letter slots\n",
    "                            # creation of a slot simultaneously \n",
    "                            # embeds it within all active nodes\n",
    "                            \n",
    "                            for i, let in enumerate(elem.value):\n",
    "                                # letter features\n",
    "                                letfs = {\n",
    "                                    \n",
    "                                    #  make string representations\n",
    "                                    'text': let,\n",
    "                                    'full': t_full.convert(let),\n",
    "                                    'lite': t_lite.convert(let),\n",
    "                                    'fuzzy': t_fuzzy.convert(let),\n",
    "                                    \n",
    "                                    # make punctuation strings at end of letter\n",
    "                                    # set it to null by default\n",
    "                                    'end': '',\n",
    "                                    'full_end': '',\n",
    "                                    'lite_end': '', \n",
    "                                    'fuzzy_end': '',                        \n",
    "                                }\n",
    "                                \n",
    "                                # make letter class data\n",
    "                                vowels = {'a','e','i','o','u'}\n",
    "                                if letfs['fuzzy'] in vowels:\n",
    "                                    letfs['class'] = 'vowel'\n",
    "                                else:\n",
    "                                    letfs['class'] = 'consonant'\n",
    "                                \n",
    "                                # keep punctuation after letter at end of a morpheme \n",
    "                                if i+1 == len(elem.value):\n",
    "                                    letfs['end'] = trailer\n",
    "                                    letfs['full_end'] = feats['full_end']\n",
    "                                    letfs['lite_end'] = feats['lite_end']\n",
    "                                    letfs['fuzzy_end'] = feats['fuzzy_end']\n",
    "                                letter = cv.slot()\n",
    "                                cv.feature(letter, **letfs)\n",
    "                                cv.terminate(letter)\n",
    "                            \n",
    "                            word_features.append(feats)\n",
    "                            cv.feature(morph, **feats)\n",
    "                            cv.terminate(morph)\n",
    "                                \n",
    "                            # -- trigger linguistic node endings --\n",
    "                            \n",
    "                            # word ending\n",
    "                            if (not re.match('^$|^[-=]$', trailer)) or is_end:\n",
    "                                cv.feature(word, **make_wordfeats(word_features))\n",
    "                                word_features = []\n",
    "                                cv.terminate(word)\n",
    "                                if not is_end:\n",
    "                                    word = cv.node('word')\n",
    "                                    \n",
    "                            # intonation group ending\n",
    "                            if re.search('\\u02c8', trailer):\n",
    "                                cv.terminate(inton)\n",
    "                                if not is_end:\n",
    "                                    inton = cv.node('inton')\n",
    "                            \n",
    "                            # subsentence ending\n",
    "                            if re.search('[,;:\\u2014\\u2013]', trailer):\n",
    "                                cv.terminate(subsentence)\n",
    "                                if not is_end:\n",
    "                                    subsentence = cv.node('subsentence')\n",
    "                            \n",
    "                            # sentence ending\n",
    "                            elif re.search('[.!?]', trailer):\n",
    "                                cv.terminate(subsentence)\n",
    "                                cv.terminate(sentence)\n",
    "                                if not is_end:\n",
    "                                    subsentence = cv.node('subsentence')\n",
    "                                    sentence = cv.node('sentence')\n",
    "                                \n",
    "                        # add other elements\n",
    "                        else:\n",
    "                            kind, data = elem\n",
    "                            if kind == 'footnote':\n",
    "                                cv.feature(text, footnote=make_footnotes(data))\n",
    "                            else:\n",
    "                                cv.feature(morph, **{kind:str(data)})\n",
    "                    \n",
    "                    # sanity check for un-closed words, itons, subsentences, sentences\n",
    "                    # due either to lack of proper punctuation in the source text (to be fixed later)\n",
    "                    # or due to non-morpheme elements intervening in the iteration\n",
    "                    unclosed = {'inton','sentence', 'subsentence', 'word'} & cv.activeTypes()\n",
    "                    if unclosed:\n",
    "                        sys.stderr.write(f'force-closing types {unclosed} in {title} ln {line_num}\\n')\n",
    "                        cv.terminate(word)\n",
    "                        cv.terminate(inton)\n",
    "                        cv.terminate(subsentence)\n",
    "                        cv.terminate(sentence)\n",
    "                        \n",
    "                    # -- trigger section node endings --\n",
    "                    cv.terminate(line)\n",
    "                cv.terminate(paragraph)\n",
    "            cv.terminate(text)\n",
    "        cv.terminate(dialect)\n",
    "        \n",
    "    info(f'{len(matched_glosses)} glosses matched...')\n",
    "    info(f'{len(set(gloss_matchset) - matched_glosses)} glosses not matched...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |     0.00s No structure nodes will be set up\n",
      "   |   SECTION   TYPES:    dialect, text, line\n",
      "   |   SECTION   FEATURES: dialect, title, number\n",
      "   |   STRUCTURE TYPES:    \n",
      "   |   STRUCTURE FEATURES: \n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       end, text\n",
      "   |      |   text-trans-full      full, full_end\n",
      "   |      |   text-trans-fuzzy     fuzzy, fuzzy_end\n",
      "   |      |   text-trans-lite      lite, lite_end\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n",
      "   |     0.00s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/A Hundred Gold Coins.nena]\n",
      "   |     0.66s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/A Man Called Čuxo.nena]\n",
      "   |     2.15s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/A Tale of Two Kings.nena]\n",
      "   |     3.15s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/A Tale of a Prince and a Princess.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in A Tale of a Prince and a Princess ln 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     7.18s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Baby Leliθa.nena]\n",
      "   |     9.23s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Dəmdəma.nena]\n",
      "   |       11s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Gozali and Nozali.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Gozali and Nozali ln 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       18s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/I Am Worth the Same as a Blind Wolf.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in I Am Worth the Same as a Blind Wolf ln 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       19s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Man Is Treacherous.nena]\n",
      "   |       20s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Measure for Measure.nena]\n",
      "   |       20s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Nanno and Jəndo.nena]\n",
      "   |       21s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Qaṭina Rescues His Nephew From Leliθa.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 1\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 7\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 8\n",
      "force-closing types {'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 10\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 11\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 12\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 13\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 14\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 15\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       22s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Sour Grapes.nena]\n",
      "   |       22s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/Tales From the 1001 Nights.nena]\n",
      "   |       28s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Battle With Yuwanəs the Armenian.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 20\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 21\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       30s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Bear and the Fox.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton', 'sentence'} in The Battle With Yuwanəs the Armenian ln 25\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       30s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Brother of Giants.nena]\n",
      "   |       31s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Cat and the Mice.nena]\n",
      "   |       32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Cooking Pot.nena]\n",
      "   |       32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Crafty Hireling.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Crafty Hireling ln 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       35s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Crow and the Cheese.nena]\n",
      "   |       35s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Daughter of the King.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Crafty Hireling ln 54\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 1\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       37s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Fox and the Lion.nena]\n",
      "   |       38s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Fox and the Miller.nena]\n",
      "   |       39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Fox and the Stork.nena]\n",
      "   |       39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Giant’s Cave.nena]\n",
      "   |       40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Girl and the Seven Brothers.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Girl and the Seven Brothers ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Girl and the Seven Brothers ln 3\n",
      "force-closing types {'sentence'} in The Girl and the Seven Brothers ln 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       41s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The King With Forty Sons.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The King With Forty Sons ln 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       46s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Leliθa From č̭āl.nena]\n",
      "   |       46s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Lion King.nena]\n",
      "   |       47s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Lion With a Swollen Leg.nena]\n",
      "   |       48s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Man Who Cried Wolf.nena]\n",
      "   |       48s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Man Who Wanted to Work.nena]\n",
      "   |       51s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Monk Who Wanted to Know When He Would Die.nena]\n",
      "   |       51s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Monk and the Angel.nena]\n",
      "   |       53s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Priest and the Mullah.nena]\n",
      "   |       54s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Sale of an Ox.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Sale of an Ox ln 41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       56s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Scorpion and the Snake.nena]\n",
      "   |       56s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Selfish Neighbour.nena]\n",
      "   |       57s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Sisisambər Plant.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 8\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       57s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Story With No End.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 14\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       58s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Farxo and Səttiya.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Tale of Farxo and Səttiya ln 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 03s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Mămo and Zine.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Mămo and Zine ln 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 08s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Mərza Pămət.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Mərza Pămət ln 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 11s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Nasimo.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Tale of Nasimo ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 5\n",
      "force-closing types {'sentence'} in The Tale of Nasimo ln 6\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 11s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Parizada, Warda and Nargis.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Tale of Parizada, Warda and Nargis ln 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 15s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Rustam (1).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Parizada, Warda and Nargis ln 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 17s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Tale of Rustam (2).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Rustam (2) ln 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 21s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Wise Daughter of the King.nena]\n",
      "   |    1m 22s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Wise Snake.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Wise Snake ln 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 23s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/The Wise Young Man.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Wise Young Man ln 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 26s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Barwar/šošət Xere.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in šošət Xere ln 6\n",
      "force-closing types {'sentence'} in šošət Xere ln 7\n",
      "force-closing types {'subsentence', 'inton', 'sentence'} in šošət Xere ln 8\n",
      "force-closing types {'sentence'} in šošət Xere ln 10\n",
      "force-closing types {'subsentence', 'sentence'} in šošət Xere ln 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 27s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Close Shave.nena]\n",
      "   |    1m 27s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Cure for a Husband’s Madness.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 1\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 4\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 5\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 27s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Donkey Knows Best.nena]\n",
      "   |    1m 27s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Dragon in the Well.nena]\n",
      "   |    1m 27s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Dutiful Son.nena]\n",
      "   |    1m 28s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Frog Wants a Husband.nena]\n",
      "   |    1m 28s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Lost Donkey.nena]\n",
      "   |    1m 28s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Lost Ring.nena]\n",
      "   |    1m 28s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Painting of the King of Iran.nena]\n",
      "   |    1m 29s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Pound of Flesh.nena]\n",
      "   |    1m 29s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Sweater to Pay Off a Debt.nena]\n",
      "   |    1m 29s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Thousand Dinars.nena]\n",
      "   |    1m 29s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/A Visit From Harun Ar-Rashid.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in A Thousand Dinars ln 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 29s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Agriculture and Village Life.nena]\n",
      "   |    1m 30s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Am I Dead?.nena]\n",
      "   |    1m 30s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/An Orphan Duckling.nena]\n",
      "   |    1m 31s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Axiqar.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'word', 'subsentence', 'inton', 'sentence'} in Axiqar ln 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 31s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Events in 1946 on the Urmi Plain.nena]\n",
      "   |    1m 31s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Games.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Axiqar ln 89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Hunting.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/I Have Died.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Ice for Dinner.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Is There a Man With No Worries?.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Kindness to a Donkey.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Lost Money.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Mistaken Identity.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Much Ado About Nothing.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Nipuxta.nena]\n",
      "   |    1m 32s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/No Bread Today.nena]\n",
      "   |    1m 33s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Problems Lighting a Fire.nena]\n",
      "   |    1m 33s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/St. Zayya’s Cake Dough.nena]\n",
      "   |    1m 33s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Star-Crossed Lovers.nena]\n",
      "   |    1m 33s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Stomach Trouble.nena]\n",
      "   |    1m 33s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Adventures of Ashur.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Adventures of Ashur ln 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 34s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Adventures of Two Brothers.nena]\n",
      "   |    1m 35s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Adventures of a Princess.nena]\n",
      "   |    1m 36s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Angel of Death.nena]\n",
      "   |    1m 36s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Assyrians of Armenia.nena]\n",
      "   |    1m 36s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Assyrians of Urmi.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Assyrians of Armenia ln 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 38s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Bald Child and the Monsters.nena]\n",
      "   |    1m 38s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Bald Man and the King.nena]\n",
      "   |    1m 39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Bird and the Fox.nena]\n",
      "   |    1m 39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Cat’s Dinner.nena]\n",
      "   |    1m 39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Cow and the Poor Girl.nena]\n",
      "   |    1m 39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Dead Rise and Return.nena]\n",
      "   |    1m 39s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Fisherman and the Princess.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Fisherman and the Princess ln 2\n",
      "force-closing types {'inton'} in The Fisherman and the Princess ln 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Giant One-Eyed Demon.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Little Prince and the Snake.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Loan of a Cooking Pot.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Man Who Wanted to Complain to God.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Old Man and the Fish.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Purchase of a Donkey.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Snake’s Dilemma.nena]\n",
      "   |    1m 40s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Stupid Carpenter.nena]\n",
      "   |    1m 41s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Wife Who Learns How to Work (2).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Snake’s Dilemma ln 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 41s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Wife Who Learns How to Work.nena]\n",
      "   |    1m 41s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Wife’s Condition.nena]\n",
      "   |    1m 41s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Wise Brother.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Wife Who Learns How to Work ln 1\n",
      "force-closing types {'inton'} in The Wife Who Learns How to Work ln 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 42s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/The Wise Young Daughter.nena]\n",
      "   |    1m 43s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Trickster.nena]\n",
      "   |    1m 43s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Two Birds Fall in Love.nena]\n",
      "   |    1m 43s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Two Wicked Daughters-In-Law.nena]\n",
      "   |    1m 43s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life (2).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'inton', 'sentence'} in Two Wicked Daughters-In-Law ln 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 43s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life (3).nena]\n",
      "   |    1m 44s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life (4).nena]\n",
      "   |    1m 44s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life (5).nena]\n",
      "   |    1m 44s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life (6).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in Village Life (5) ln 1\n",
      "force-closing types {'inton'} in Village Life (6) ln 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 46s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Village Life.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in Village Life ln 1\n",
      "force-closing types {'inton'} in Village Life ln 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 46s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Vineyards.nena]\n",
      "   |    1m 46s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Weddings and Festivals.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in Village Life ln 18\n",
      "force-closing types {'inton'} in Village Life ln 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |    1m 47s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Weddings.nena]\n",
      "   |    1m 47s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/When Shall I Die?.nena]\n",
      "   |    1m 47s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Women Are Stronger Than Men.nena]\n",
      "   |    1m 48s processing: [/Users/cody/github/CambridgeSemiticsLab/nena_corpus/nena/0.01/Urmi_C/Women Do Things Best.nena]\n",
      "   |    1m 48s 1429 glosses matched...\n",
      "   |    1m 48s 6674 glosses not matched...\n",
      "   |    1m 48s \"edge\" actions: 0\n",
      "   |    1m 48s \"feature\" actions: 756315\n",
      "   |    1m 48s \"node\" actions: 294155\n",
      "   |    1m 48s \"resume\" actions: 0\n",
      "   |    1m 48s \"slot\" actions: 539381\n",
      "   |    1m 48s \"terminate\" actions: 833720\n",
      "   |          2 x \"dialect\" node \n",
      "   |      35985 x \"inton\" node \n",
      "   |     539381 x \"letter\" node  = slot type\n",
      "   |       2544 x \"line\" node \n",
      "   |     120148 x \"morpheme\" node \n",
      "   |        351 x \"paragraph\" node \n",
      "   |      16708 x \"sentence\" node \n",
      "   |      24528 x \"subsentence\" node \n",
      "   |        126 x \"text\" node \n",
      "   |      93763 x \"word\" node \n",
      "   |     833536 nodes of all types\n",
      "   |    1m 48s OK\n",
      "   |     0.13s Removing unlinked nodes ... \n",
      "   |      |    -0.00s      1 unlinked \"word\" node: [51059]\n",
      "   |      |     0.00s      1 unlinked \"inton\" node: [19594]\n",
      "   |      |     0.00s      1 unlinked \"subsentence\" node: [14318]\n",
      "   |      |     0.00s      1 unlinked \"sentence\" node: [9796]\n",
      "   |      |     0.00s      4 unlinked nodes\n",
      "   |      |     0.00s Leaving 833532 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.17s Sorting 2 nodes of type \"dialect\"\n",
      "   |     0.21s Sorting 35984 nodes of type \"inton\"\n",
      "   |     0.37s Sorting 2544 nodes of type \"line\"\n",
      "   |     0.45s Sorting 120148 nodes of type \"morpheme\"\n",
      "   |     0.80s Sorting 351 nodes of type \"paragraph\"\n",
      "   |     0.84s Sorting 16707 nodes of type \"sentence\"\n",
      "   |     0.96s Sorting 24527 nodes of type \"subsentence\"\n",
      "   |     1.08s Sorting 126 nodes of type \"text\"\n",
      "   |     1.16s Sorting 93762 nodes of type \"word\"\n",
      "   |     1.41s Max node = 833532\n",
      "   |     1.41s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     1.76s node feature \"class\" with 539381 nodes\n",
      "   |      |     1.90s node feature \"comment\" with 1 node\n",
      "   |      |     1.90s node feature \"continued_from\" with 1 node\n",
      "   |      |     1.90s node feature \"dialect\" with 2 nodes\n",
      "   |      |     1.90s node feature \"end\" with 753291 nodes\n",
      "   |      |     2.15s node feature \"footnotes\" with 6 nodes\n",
      "   |      |     2.15s node feature \"foreign\" with 1041 nodes\n",
      "   |      |     2.15s node feature \"full\" with 753291 nodes\n",
      "   |      |     2.42s node feature \"full_end\" with 753291 nodes\n",
      "   |      |     2.82s node feature \"fuzzy\" with 753291 nodes\n",
      "   |      |     3.15s node feature \"fuzzy_end\" with 753291 nodes\n",
      "   |      |     3.38s node feature \"gloss\" with 50079 nodes\n",
      "   |      |     3.40s node feature \"grm_desc\" with 49799 nodes\n",
      "   |      |     3.43s node feature \"informant\" with 126 nodes\n",
      "   |      |     3.43s node feature \"lang\" with 898 nodes\n",
      "   |      |     3.43s node feature \"lemma\" with 51778 nodes\n",
      "   |      |     3.47s node feature \"lemma_form\" with 4539 nodes\n",
      "   |      |     3.47s node feature \"lite\" with 753291 nodes\n",
      "   |      |     3.73s node feature \"lite_end\" with 753291 nodes\n",
      "   |      |     3.97s node feature \"number\" with 2895 nodes\n",
      "   |      |     3.97s node feature \"place\" with 126 nodes\n",
      "   |      |     3.97s node feature \"source\" with 126 nodes\n",
      "   |      |     3.97s node feature \"speaker\" with 213910 nodes\n",
      "   |      |     4.06s node feature \"text\" with 753291 nodes\n",
      "   |      |     4.38s node feature \"text_id\" with 125 nodes\n",
      "   |      |     4.38s node feature \"text_norm\" with 213910 nodes\n",
      "   |      |     4.52s node feature \"title\" with 126 nodes\n",
      "   |      |     4.52s node feature \"version\" with 2 nodes\n",
      "   |     3.12s OK\n",
      "  0.00s Exporting 29 node and 1 edge and 1 config features to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01:\n",
      "  0.00s VALIDATING oslots feature\n"
     ]
    }
   ],
   "source": [
    "good = cv.walk(\n",
    "    director,\n",
    "    slotType,\n",
    "    otext=otext,\n",
    "    generic=generic,\n",
    "    intFeatures=intFeatures,\n",
    "    featureMeta=featureMeta,\n",
    "    warn=True,\n",
    "    force=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    "Automatically construct documentation from the features and arrange by objects in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = Path('../docs')\n",
    "feat_documentation = docs.joinpath('features.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = Fabric(locations=str(OUT_DIR))\n",
    "api = TF.loadAll()\n",
    "tf_vars = api.makeAvailableIn(globals())\n",
    "F, L, Fall = api.F, api.L, api.Fall\n",
    "C = api.C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctext = \"\"\"\n",
    "# NENA Text-Fabric Corpus\n",
    "\n",
    "The NENA Text-Fabric (TF) corpus contains textual transcriptions and linguistic annotations from the research group under Geoffrey Khan at the University of Cambridge.\n",
    "\n",
    "## Contents\n",
    "\n",
    "* [data model](#data-model)\n",
    "* [features](#features)\n",
    "\n",
    "## Data Model\n",
    "\n",
    "For a full description of the Text-Fabric data model, see the [datamodel documentation](https://annotation.github.io/text-fabric/Model/Data-Model/).\n",
    "\n",
    "One can think about the NENA Text-Fabric resource in two ways. The first is as a **conceptual** model, and the second is as a literal **implementation**. The conceptual model is simply a way of thinking about the text and all its various parts (words, sentences, letters, etc.). The literal implementation is the way that conceptual model is actually stored on a computer. \n",
    "\n",
    "The **conceptual** model of the TF NENA corpus is a graph. In mathematics, a [graph](https://en.wikipedia.org/wiki/Graph_theory) is a method of indicating relationships between entities. The entities in a graph are called \"nodes\", often illustrated visually as circles. Their relationships to one another are called \"edges\", illustrated with lines drawn between two or more circles. A visual representation can be seen below.\n",
    "\n",
    "<img src=\"images/graph_illustration.png\" height=30% width=30%>\n",
    "\n",
    "In the case of a [text graph](https://www.balisage.net/Proceedings/vol19/html/Dekker01/BalisageVol19-Dekker01.html), entities like letters, words, sentences are stored as nodes. These entities also have relationships. A key relationship in Text-Fabric is \"containment\": a sentence contains a word, a word contains a letter. Other, optional relationships might be syntactic relations or discourse relations between sentences. With the exception of \"containment\", the graph model of Text-Fabric does not \"care\" which other relationships are modeled (syntax, discourse, etc.). The user(s) are free to choose whatever relationships they are interested in.\n",
    "\n",
    "For instance, in the example below we can see a containment relationship being modelled between a given word and its letter:\n",
    "\n",
    "<img src=\"images/containment_illustration.png\" height=30% width=30%>\n",
    "\n",
    "\n",
    "*To be continued...*\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Features\n",
    "\"\"\".strip()\n",
    "\n",
    "def maketable(tabledata, headers=[]):\n",
    "    return tabulate.tabulate(tabledata, headers=headers, tablefmt='pipe')\n",
    "\n",
    "for otype_data in C.levels.data:\n",
    "    otype = otype_data[0]\n",
    "    count = len(list(F.otype.s(otype)))\n",
    "    feature_counts = collections.Counter()\n",
    "    \n",
    "    # add object to the doc\n",
    "    doctext += '\\n\\n'\n",
    "    doctext += f'## {otype} ({count}x)'\n",
    "    \n",
    "    for feat, fdata in featureMeta.items(): \n",
    "        \n",
    "        # see if otype ever used with given feature\n",
    "        uses = list(Fs(feat).freqList(nodeTypes=otype))\n",
    "        values = [fl[0] for fl in uses]\n",
    "        total = sum(fl[-1] for fl in uses)\n",
    "        if not uses:\n",
    "            continue\n",
    "        uses.append(('TOTAL', total))\n",
    "        \n",
    "        # add feature section to the document\n",
    "        doctext += '\\n\\n'\n",
    "        doctext += f'### {feat}'\n",
    "        doctext += '\\n\\n'\n",
    "        doctext += fdata['about']\n",
    "        doctext += '\\n'\n",
    "        \n",
    "        # add data about the feature\n",
    "        if fdata['feature_type'] == 'categorical':\n",
    "            doctext += '\\n'\n",
    "            doctext += maketable(uses, (feat, 'frequency'))\n",
    "        elif fdata['feature_type'] == 'text':\n",
    "            doctext += '\\n'\n",
    "            doctext += 'See the [transcription tables](transcription.md).\\n\\n'\n",
    "            doctext += maketable([uses[-1]])\n",
    "        else:\n",
    "            doctext += '\\n'\n",
    "            doctext += 'Arbitrary string.\\n\\n'\n",
    "            doctext += 'examples:\\n'\n",
    "            doctext += '```' + '\\n' + '\\n'.join(str(v) for v in values[:5]) + '\\n' + '```'\n",
    "            doctext += '\\n\\n'\n",
    "            doctext += maketable([uses[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feat_documentation, 'w') as outfile:\n",
    "    outfile.write(doctext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
