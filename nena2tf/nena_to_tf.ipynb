{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NENA to TF\n",
    "\n",
    "This notebook will be used to develop code for converting texts from .nena format to Text-Fabric. The parser has principally been written by Hannes Vlaardingerbroek. Many thanks to him for his hard work on it. Updates and refinements have been added by Cody Kingham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last updated\n",
      "Mon  2 Mar 2020 14:48:24 GMT\n"
     ]
    }
   ],
   "source": [
    "! echo \"last updated\"; date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 17 shift/reduce conflicts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import re\n",
    "import csv\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from tf.convert.walker import CV\n",
    "from tf.fabric import Fabric\n",
    "\n",
    "# path to parser\n",
    "parserpath = f'../../nena_corpus/parse_nena/'\n",
    "sys.path.append(parserpath)\n",
    "from nena_parser import NenaLexer, NenaParser\n",
    "\n",
    "# paths\n",
    "VERSION = '0.01'\n",
    "OUT_DIR = Path(f'../tf/{VERSION}')\n",
    "data_dir = Path(f'../../nena_corpus/nena/{VERSION}')\n",
    "dialect_dirs = list(Path(data_dir).glob('*'))\n",
    "\n",
    "# open char tables\n",
    "# trans_lite_table = Path('../char_tables/trans_lite.tsv')\n",
    "# with open(trans_lite_table, 'r') as infile:\n",
    "#     trans_data = list(csv.reader(infile, delimiter='\\t'))[1:]\n",
    "#     trans_lite = {unicodedata.normalize('NFC', td[0]):td[1] for td in trans_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = Fabric(locations=[str(OUT_DIR)], silent=True)\n",
    "cv = CV(TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test NENA Parser\n",
    "\n",
    "The NENA Parser delivers the text as structured morphemes, which can then be processed into a TF graph. We do that below by opening each source text, retrieving its parsed form, and begin each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = NenaLexer()\n",
    "parser = NenaParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialect2file2parsed = collections.defaultdict(lambda: collections.defaultdict())\n",
    "\n",
    "# nparsed = 0\n",
    "\n",
    "# for dialect in sorted(dialect_dirs):    \n",
    "#     print()\n",
    "#     print(dialect.name)\n",
    "#     for file in sorted(dialect.glob('*.nena')):\n",
    "#         with open(file, 'r') as infile:\n",
    "#             text = infile.read()\n",
    "#             print(f'parsing: {file.name}')\n",
    "#             parse = parser.parse(lexer.tokenize(text))\n",
    "#             nparsed += 1\n",
    "#             dialect2file2parsed[dialect.name][file.name] = parse\n",
    "            \n",
    "\n",
    "# print('\\n', nparsed, 'texts ready for conversion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linenum, elements = dialect2file2parsed['Urmi_C']['Village Life.nena'][1][0][\n",
    "# eg_morph = elements[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dialect2file2parsed['Barwar']['A Hundred Gold Coins.nena'][1][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_full = {\n",
    "    # non-latin vowels\n",
    "    '\\u0131': '1',  # 0x0131 ı dotless i\n",
    "    '\\u0251': '@',  # 0x0251 ɑ alpha\n",
    "    '\\u0259': '3',  # 0x0259 ə schwa\n",
    "    '\\u025B': '$',  # 0x025B ɛ open e\n",
    "    # vowel accents\n",
    "    '\\u0300': '`',  # 0x0300 à grave\n",
    "    '\\u0301': \"'\",  # 0x0301 á acute\n",
    "    '\\u0304': '_',  # 0x0304 ā macron\n",
    "    '\\u0306': '%',  # 0x0306 ă breve\n",
    "    '\\u0308': '\"',  # 0x0308 ä diaeresis\n",
    "    '\\u0303': '~',  # 0x0303 ã tilde\n",
    "    '\\u02C8': '', # 0x2c8 ˈ small vertical line\n",
    "    # non-latin consonants\n",
    "    '\\u00F0': '6',  # 0x00F0 ð eth\n",
    "    '\\u025F': '&',  # 0x025F ɟ small dotless j with stroke\n",
    "    '\\u0248': '!',  # 0x0248 Ɉ capital J with stroke\n",
    "    '\\u03B8': '8',  # 0x03B8 θ greek theta\n",
    "    '\\u02B8': '7',  # 0x02B8 ʸ small superscript y\n",
    "    '\\u02BE': '}',  # 0x02BE ʾ right half ring (alaph)\n",
    "    '\\u02BF': '{',  # 0x02BF ʿ left half ring (ayin)\n",
    "    # consonant diacritics\n",
    "    '\\u207A': '+',  # 0x207A ⁺ superscript plus\n",
    "    '\\u030C': '<',  # 0x030C x̌ caron\n",
    "    '\\u0302': '^',  # 0x0302 x̂ circumflex\n",
    "    '\\u0307': ';',  # 0x0307 ẋ dot above\n",
    "    '\\u0323': '.',  # 0x0323 x̣ dot below\n",
    "    '\\u032D': '>',  # 0x032D x̭ circumflex below\n",
    "    \n",
    "    # punctuation\n",
    "    '\\u02C8': '|', # 0x2c8 ˈ small vertical line\n",
    "}\n",
    "def trans(s, table, mark_punct=True):\n",
    "    '''\n",
    "    Transcribes a text.\n",
    "    '''\n",
    "    s = unicodedata.normalize('NFD', s)\n",
    "    # mark punctuation \n",
    "    if mark_punct:\n",
    "        s = re.sub('([\\n,.!?:;/])', r'/\\g<1>', s, 1)\n",
    "    return ''.join([table.get(c, c) for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transcriber:\n",
    "    \"\"\"Transcribe a string according to transcription rules.\n",
    "    \n",
    "    This transcription class is essentially a filter\n",
    "    which determines which characters make it into a new,\n",
    "    transcribed string. The filter is applied on a letter-by-letter \n",
    "    basis. A \"letter\" (token) is defined by the `tokens` argument \n",
    "    and can include diacritics/accents. The filter is applied \n",
    "    in one of three methods:\n",
    "        1. replacements on a unicode composed letter (NFC)\n",
    "        2. or replacements on punctuation if letter is punctuation\n",
    "        3. or replacements on a unicode decomposed letter (NFD)\n",
    "    The changes are added to a new string which is then returned.\n",
    "    \n",
    "    __init__(tokens, replacements, punctuation, keep):\n",
    "        string: a string to transcribe\n",
    "        tokens: regex for splitting letters (tokens)\n",
    "            to be used with findall\n",
    "        replacements: dict with find:replace mappings\n",
    "        keep: regex for characters to keep\n",
    "            \n",
    "    Returns:\n",
    "        str in transcribed form\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens='', replace={}, \n",
    "                 punctuation='', keep='', keep_case=False):    \n",
    "        self.tokenize = re.compile(f'{tokens}|{punctuation}').findall\n",
    "        self.punct = re.compile(punctuation)\n",
    "        self.keep = re.compile(keep)\n",
    "        self.keep_case = keep_case\n",
    "        \n",
    "        # ensure normalized characters for pattern searches\n",
    "        self.repl = {\n",
    "            unicodedata.normalize('NFC',f):r \n",
    "                for f,r in replace.items()\n",
    "        }\n",
    "        \n",
    "    def convert(self, string):\n",
    "        \"\"\"Convert string to transcription.\n",
    "        \n",
    "        Returns:\n",
    "            str in transcribed form\n",
    "        \"\"\"\n",
    "        \n",
    "        string = unicodedata.normalize('NFC',string)\n",
    "        if not self.keep_case:\n",
    "            string = string.lower()\n",
    "        transcription = ''\n",
    "\n",
    "        for token in self.tokenize(string):\n",
    "\n",
    "            # filter a composed string\n",
    "            if token in self.repl:\n",
    "                transcription += self.repl[token]\n",
    "\n",
    "            # keep punctuation\n",
    "            elif self.punct.match(token):\n",
    "                transcription += token\n",
    "\n",
    "            # filter at \n",
    "            else:\n",
    "                for char in unicodedata.normalize('NFD', token):\n",
    "\n",
    "                    # attempt second match on char by char basis\n",
    "                    if char in self.repl:\n",
    "                        transcription += self.repl[char]\n",
    "\n",
    "                    # attempt to keep with keep-set\n",
    "                    elif self.keep.match(char):\n",
    "                        transcription += char   \n",
    "                        \n",
    "        return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_full = {\n",
    "#     'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "#     'replace': {\n",
    "#         ''\n",
    "#     }\n",
    "#     'punctuation': '[\\s-]',\n",
    "#     'keep': '[A-Za-z]',\n",
    "# }\n",
    "\n",
    "trans_lite = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*', \n",
    "    'replace': {\n",
    "        'ʾ': ')',\n",
    "        'ʿ': '(',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '%',\n",
    "        'ḍ': 'D',\n",
    "        'ð': '6',\n",
    "        'ð̣': '^',\n",
    "        'ġ': 'G',\n",
    "        'ḥ': 'H',\n",
    "        'ɟ': '4',\n",
    "        'Ɉ': '4',\n",
    "        'k̭': '&',\n",
    "        'ḷ': 'L',\n",
    "        'ṃ': 'M',    \n",
    "        'p̣': 'P',\n",
    "        'ṛ': 'R',\n",
    "        'ṣ': 'S',\n",
    "        'š': '$',\n",
    "        'ṱ': '<+>',\n",
    "        'ṭ': 'T',\n",
    "        'θ': '8',\n",
    "        'ž': '7',\n",
    "        'ẓ': 'Z',\n",
    "        'ā̀': 'A',\n",
    "        'ā́': 'A',\n",
    "        'ă': '@',\n",
    "        'ắ': '@',\n",
    "        'ằ': '@',\n",
    "        'ē': 'E',\n",
    "        'ɛ': '3',\n",
    "        'ī': 'I',\n",
    "        'ĭ': '9',\n",
    "        'ə': '9',\n",
    "        'o': 'o',\n",
    "        'ō': 'O',\n",
    "        'ū': 'U',\n",
    "        'ŭ': '2',\n",
    "        'ı': 'i',\n",
    "        'ɑ': 'a',\n",
    "        'ˈ': '|'\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_urmi = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'c': 'k',\n",
    "        'c̭': 'k',\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'q',\n",
    "        'ɟ': 'g',\n",
    "        'Ɉ': 'g',\n",
    "        'ə': 'i',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}\n",
    "\n",
    "fuzzy_barwar = {\n",
    "    'tokens': f'[\\u207A]?[^\\W\\d_][\\u0300-\\u036F]*',\n",
    "    'replace': {\n",
    "        'č': '5',\n",
    "        'č̭': '5',\n",
    "        'č̣': '5',\n",
    "        'k̭': 'k',\n",
    "        'θ': 't',\n",
    "        'ð': 'd',\n",
    "        'ɛ': 'e',\n",
    "        'ə': 'i',\n",
    "    },\n",
    "    'punctuation': '[\\s.,?!:;–\\-\\u2014]',\n",
    "    'keep': '[A-Za-z]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'xòš-məndila'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_test = Transcriber(**fuzzy_barwar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xos-mindila'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans_test.convert(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "slotType = 'letter'\n",
    "\n",
    "otext = {\n",
    "    'sectionTypes': 'dialect,text,line',\n",
    "    'sectionFeatures': 'dialect,title,number',\n",
    "    'fmt:text-orig-full': '{text}{end}',\n",
    "    'fmt:text-trans-full': '{trans_f}{etrans_f}',\n",
    "    'fmt:text-trans-lite': '{trans_l}{etrans_l}',\n",
    "    'fmt:text-trans-fuzzy': '{t_fuzzy}{efuzzy}',\n",
    "}\n",
    "\n",
    "description = ''.join(\"\"\"\n",
    "The NENA linguistic corpus is derived from decades of \n",
    "field work by Prof. Geoffrey Khan and his students.\n",
    "\"\"\".split('\\n'))\n",
    "\n",
    "generic = {\n",
    "    'origin': 'Cambridge University, Faculty of Asian and Middle Eastern Studies',\n",
    "    'author': 'Geoffrey Khan et al.',\n",
    "    'editors': 'Cody Kingham, Paul Noorlander, James Strachan, Hannes Vlaardingerbroek',\n",
    "    'researchers': 'Dorota Molin, Johan Lundberg',\n",
    "    'source': description,\n",
    "    'url': 'https://github.com/CambridgeSemiticsLab/nena_tf',\n",
    "}\n",
    "\n",
    "intFeatures = {'number'}\n",
    "\n",
    "d = 'about'\n",
    "\n",
    "featureMeta = {\n",
    "    'dialect': {d: 'name of a dialect in Northeastern Neo-Aramaic'},\n",
    "    'title': {d: 'title of a text (story)'},\n",
    "    'version': {d: 'version of the story if there are multiple instances of the same story'},\n",
    "    'number': {d: 'sequential number of a paragraph or line within a text or paragraph, respectively'},\n",
    "    'text': {d: 'plain text representation of a letter, morpheme, or word'},\n",
    "    'end': {d: 'space, punctuation, or other stylistic text at the end of a morpheme or word'},\n",
    "    'trans_f': {d: 'full, one-to-one transcription of a letter, morpheme, or word'},\n",
    "    'trans_l': {d: 'lite transcription of a letter, morpheme, or word, without vowel accents'},\n",
    "    'etrans_f': {d: 'full transcription of punctuation or other stylistic text at the end of a morpheme or word; see also trans_f'},\n",
    "    'etrans_l': {d: 'lite transcription of punctuation or other stylistic text at the end of a morpheme or word, excluding intonation boundary markers; see also trans_l'},\n",
    "    'speaker': {d: 'name or initials of person speaking a morpheme or word; see also informant'},\n",
    "    'footnotes': {d: 'explanatory footnote on a morpheme or text'},\n",
    "    'lang': {d: 'language of a morpheme foreign to a text'},\n",
    "    'foreign': {d: 'indicates whether a morpheme is foreign to a text; see also lang'},\n",
    "    'comment': {d: 'explanatory comment inserted in the text, stored on a morpheme'},\n",
    "    'continued_from': {d: 'text is a follow-up to the named text'},\n",
    "    'informant': {d: 'name of person who spoke these words'},\n",
    "    'place': {d: 'place a text was recorded'},\n",
    "    'source': {d: 'name of the file from which a text was converted'},\n",
    "    'text_id': {d: 'id of a text within its original publication; can overlap between publications'},\n",
    "    't_fuzzy': {d: 'fuzzy transcription that leaves out most diacritics and maps certain characters in certain dialects to common characters'},\n",
    "    'efuzzy': {d: 'fuzzy transcription of punctuation or other stylistic text at the end of a morpheme or word, excluding intonation boundary markers; see also trans_l'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converter\n",
    "\n",
    "Build a TF Walker class that can walk over the NENA parsed data and fit the text graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_footnotes(fn_dict):\n",
    "    \"\"\"Format footnote dict into string\"\"\"\n",
    "    if fn_dict:\n",
    "        return '; '.join(\n",
    "            f'[^{num}]: {txt}' for num, txt in fn_dict.items()\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def make_wordfeats(mfeat_list, ignore={}):\n",
    "    \"\"\"Convert a list of morpheme feature dicts into one for a word.\n",
    "    \n",
    "    Features stored on a word must be inherited in special ways\n",
    "    for words. For example, a word's \"end\" feature should be \n",
    "    the last morpheme, not all of the ends. Those features \n",
    "    are specially processed here.\n",
    "    \"\"\"\n",
    "    \n",
    "    # gather word features here\n",
    "    word_fs = collections.defaultdict(set)\n",
    "\n",
    "    # add features\n",
    "    for mfeats in mfeat_list:\n",
    "        for feat,val in mfeats.items():\n",
    "            if feat in ignore:\n",
    "                continue\n",
    "            else:\n",
    "                word_fs[feat].add(val)\n",
    "\n",
    "    # handle special cases\n",
    "    word_fs['end'] = mfeat_list[-1]['end']\n",
    "    word_fs['etrans_f'] = mfeat_list[-1]['etrans_f']\n",
    "    word_fs['etrans_l'] = mfeat_list[-1]['etrans_l']\n",
    "    word_fs['efuzzy'] = mfeat_list[-1]['efuzzy']\n",
    "    word_fs['text'] = ''.join(\n",
    "        mf['text']+mf['end'] for mf in mfeat_list\n",
    "    )\n",
    "    # add transcription with end, leaving off the end from the \n",
    "    # last morpheme\n",
    "    trans_parts = [('trans_f', 'etrans_f'), ('trans_l', 'etrans_l'), ('t_fuzzy', 'efuzzy')]\n",
    "    for trans, end in trans_parts:\n",
    "        word_fs[trans] = ''\n",
    "        for i,mf in enumerate(mfeat_list):\n",
    "            word_fs[trans] += mf[trans]\n",
    "            if i+1 != len(mfeat_list):\n",
    "                word_fs[trans] += mf[end]\n",
    "    \n",
    "    # convert to strings and handle duplicates\n",
    "    for feat,val in word_fs.items():\n",
    "        if type(val) == set:\n",
    "            val = {v for v in val if v}\n",
    "            if val:\n",
    "                word_fs[feat] = ' '.join(val)\n",
    "            else:\n",
    "                word_fs[feat] = None\n",
    "        \n",
    "    return word_fs\n",
    "    \n",
    "t_lite = Transcriber(**trans_lite) # transcription lite feature\n",
    "\n",
    "    \n",
    "def director(CV):\n",
    "    \"\"\"Walk the source data and produce a TF graph\"\"\"\n",
    "    \n",
    "    info = TF.tm.info\n",
    "    \n",
    "    # transcriptions particular to dialects\n",
    "    dialect2fuzzy = {\n",
    "        'Barwar': Transcriber(**fuzzy_barwar),\n",
    "        'Urmi_C': Transcriber(**fuzzy_urmi),\n",
    "    }\n",
    "    \n",
    "    for dialect_dir in sorted(dialect_dirs):  \n",
    "        \n",
    "        # make dialect node\n",
    "        dialect = cv.node('dialect')\n",
    "        dia = dialect_dir.name\n",
    "        cv.feature(dialect, dialect=dia)\n",
    "        \n",
    "        # retrieve fuzzy transcription particular to dialect\n",
    "        t_fuzzy = dialect2fuzzy[dia]\n",
    "        \n",
    "        # process file into TF graph\n",
    "        for file in sorted(dialect_dir.glob('*.nena')):\n",
    "            \n",
    "            info(f'processing: [{file}]')\n",
    "            \n",
    "            with open(file, 'r') as infile:\n",
    "                nena_text = infile.read()\n",
    "            \n",
    "            # parse the .nena format\n",
    "            header, paragraphs = parser.parse(lexer.tokenize(nena_text))\n",
    "            \n",
    "            # -- begin TF node creation --\n",
    "            \n",
    "            # cv.node initializes a node object\n",
    "            # all slots added in between its creation and \n",
    "            # termination will be considered embedded within\n",
    "            # this node; same is true of following cv.node calls\n",
    "            text = cv.node('text')\n",
    "            cv.feature(text, **header) # adds features to supplied node\n",
    "            title = header['title']\n",
    "            \n",
    "            for i, para in enumerate(paragraphs):\n",
    "                \n",
    "                # TODO: Process footnotes here\n",
    "                if len(para[0]) != 2:\n",
    "                    continue\n",
    "                \n",
    "                # make paragraph node\n",
    "                paragraph = cv.node('paragraph')\n",
    "                cv.feature(paragraph, number=i+1)\n",
    "                \n",
    "                for line_num, line_elements in para:\n",
    "                    \n",
    "                    # make line nodes\n",
    "                    line = cv.node('line')\n",
    "                    cv.feature(line, number=line_num)\n",
    "                    \n",
    "                    # Make linguistic nodes by parsing morphemes.\n",
    "                    # This must be done iteratively and composed\n",
    "                    # based on characters at the end of each morpheme. \n",
    "                    # Punctuation signals intonation/subsentence/sentence \n",
    "                    # boundaries; spaces and hyphens signal word bounds. \n",
    "                    # This is handled in the loop below.\n",
    "                    word = cv.node('word')\n",
    "                    inton = cv.node('inton')\n",
    "                    subsentence = cv.node('subsentence')\n",
    "                    sentence = cv.node('sentence')\n",
    "                    word_features = [] # store morphs feats here for processing\n",
    "\n",
    "                    for i, elem in enumerate(line_elements):\n",
    "                        \n",
    "                        is_end = i+1 == len(line_elements)\n",
    "\n",
    "                        # add morphemes as slots\n",
    "                        # 'slot' being the most basic element\n",
    "                        if elem.__class__.__name__ == 'Morpheme':\n",
    "                            \n",
    "                            # make morpheme node\n",
    "                            morph = cv.node('morpheme')\n",
    "                            \n",
    "                            # access/prepare morph features\n",
    "                            fs = elem.__dict__\n",
    "                            trailer = elem.trailer.replace('/', '\\n')\n",
    "                            \n",
    "                            # package & edit morph features for cv\n",
    "                            # NB: None values are ignored by default\n",
    "                            m_string = ''.join(elem.value)\n",
    "                            feats = {\n",
    "                                'text': ''.join(elem.value),\n",
    "                                'trans_f': trans(m_string, trans_full),\n",
    "                                'trans_l': t_lite.convert(m_string),\n",
    "                                't_fuzzy': t_fuzzy.convert(m_string),\n",
    "                                'end': trailer,\n",
    "                                'etrans_f': trans(trailer, trans_full),\n",
    "                                'etrans_l': t_lite.convert(trailer),\n",
    "                                'efuzzy': t_fuzzy.convert(trailer),\n",
    "                                'speaker': fs.get('speaker') or header.get('informant'),\n",
    "                                'footnotes': make_footnotes(fs.get('footnotes', {})),\n",
    "                                'lang': fs.get('lang'),\n",
    "                                'foreign': str(fs.get('foreign')) if fs.get('foreign') else None,\n",
    "                            }\n",
    "                            \n",
    "                            # make letter slots\n",
    "                            # creation of a slot simultaneously \n",
    "                            # embeds it within all active nodes\n",
    "                            for i, let in enumerate(elem.value):\n",
    "                                # letter features\n",
    "                                letfs = {\n",
    "                                    'text': let,\n",
    "                                    'end': '',\n",
    "                                    'trans_f': trans(let, trans_full),\n",
    "                                    'etrans_f': '',\n",
    "                                    'trans_l': t_lite.convert(let),\n",
    "                                    'etrans_l': '', \n",
    "                                    't_fuzzy': t_fuzzy.convert(let),\n",
    "                                    'efuzzy': '',                        \n",
    "                                }\n",
    "                                if i+1 == len(elem.value):\n",
    "                                    letfs['end'] = trailer\n",
    "                                    letfs['etrans_f'] = feats['etrans_f']\n",
    "                                    letfs['etrans_l'] = feats['etrans_l']\n",
    "                                    letfs['efuzzy'] = feats['efuzzy']\n",
    "                                letter = cv.slot()\n",
    "                                cv.feature(letter, **letfs)\n",
    "                                cv.terminate(letter)\n",
    "                            \n",
    "                            word_features.append(feats)\n",
    "                            cv.feature(morph, **feats)\n",
    "                            cv.terminate(morph)\n",
    "                                \n",
    "                            # -- trigger linguistic node endings --\n",
    "                            \n",
    "                            # word ending\n",
    "                            if (not re.match('^$|^[-=]$', trailer)) or is_end:\n",
    "                                cv.feature(word, **make_wordfeats(word_features))\n",
    "                                word_features = []\n",
    "                                cv.terminate(word)\n",
    "                                if not is_end:\n",
    "                                    word = cv.node('word')\n",
    "\n",
    "                            # intonation group ending\n",
    "                            if re.search('\\u02c8', trailer):\n",
    "                                cv.terminate(inton)\n",
    "                                if not is_end:\n",
    "                                    inton = cv.node('inton')\n",
    "                            \n",
    "                            # subsentence ending\n",
    "                            if re.search('[,;:\\u2014\\u2013]', trailer):\n",
    "                                cv.terminate(subsentence)\n",
    "                                if not is_end:\n",
    "                                    subsentence = cv.node('subsentence')\n",
    "                            \n",
    "                            # sentence ending\n",
    "                            elif re.search('[.!?]', trailer):\n",
    "                                cv.terminate(subsentence)\n",
    "                                cv.terminate(sentence)\n",
    "                                if not is_end:\n",
    "                                    subsentence = cv.node('subsentence')\n",
    "                                    sentence = cv.node('sentence')\n",
    "                                \n",
    "                        # add other elements\n",
    "                        else:\n",
    "                            kind, data = elem\n",
    "                            if kind == 'footnote':\n",
    "                                cv.feature(text, footnote=make_footnotes(data))\n",
    "                            else:\n",
    "                                cv.feature(morph, **{kind:str(data)})\n",
    "                    \n",
    "                    # sanity check for un-closed words, itons, subsentences, sentences\n",
    "                    # due either to lack of proper punctuation in the source text (to be fixed later)\n",
    "                    # or due to non-morpheme elements intervening in the iteration\n",
    "                    unclosed = {'inton','sentence', 'subsentence', 'word'} & cv.activeTypes()\n",
    "                    if unclosed:\n",
    "                        sys.stderr.write(f'force-closing types {unclosed} in {title} ln {line_num}\\n')\n",
    "                        cv.terminate(word)\n",
    "                        cv.terminate(inton)\n",
    "                        cv.terminate(subsentence)\n",
    "                        cv.terminate(sentence)\n",
    "                        \n",
    "                    # -- trigger section node endings --\n",
    "                    cv.terminate(line)\n",
    "                cv.terminate(paragraph)\n",
    "            cv.terminate(text)\n",
    "        cv.terminate(dialect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Importing data from walking through the source ...\n",
      "   |     0.00s Preparing metadata... \n",
      "   |     0.00s No structure nodes will be set up\n",
      "   |   SECTION   TYPES:    dialect, text, line\n",
      "   |   SECTION   FEATURES: dialect, title, number\n",
      "   |   STRUCTURE TYPES:    \n",
      "   |   STRUCTURE FEATURES: \n",
      "   |   TEXT      FEATURES:\n",
      "   |      |   text-orig-full       end, text\n",
      "   |      |   text-trans-full      etrans_f, trans_f\n",
      "   |      |   text-trans-fuzzy     efuzzy, t_fuzzy\n",
      "   |      |   text-trans-lite      etrans_l, trans_l\n",
      "   |     0.01s OK\n",
      "   |     0.00s Following director... \n",
      "   |     0.00s processing: [../../nena_corpus/nena/0.01/Barwar/A Hundred Gold Coins.nena]\n",
      "   |     0.11s processing: [../../nena_corpus/nena/0.01/Barwar/A Man Called Čuxo.nena]\n",
      "   |     0.32s processing: [../../nena_corpus/nena/0.01/Barwar/A Tale of Two Kings.nena]\n",
      "   |     0.51s processing: [../../nena_corpus/nena/0.01/Barwar/A Tale of a Prince and a Princess.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in A Tale of a Prince and a Princess ln 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     1.24s processing: [../../nena_corpus/nena/0.01/Barwar/Baby Leliθa.nena]\n",
      "   |     1.57s processing: [../../nena_corpus/nena/0.01/Barwar/Dəmdəma.nena]\n",
      "   |     1.82s processing: [../../nena_corpus/nena/0.01/Barwar/Gozali and Nozali.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Gozali and Nozali ln 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     2.97s processing: [../../nena_corpus/nena/0.01/Barwar/I Am Worth the Same as a Blind Wolf.nena]\n",
      "   |     3.23s processing: [../../nena_corpus/nena/0.01/Barwar/Man Is Treacherous.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in I Am Worth the Same as a Blind Wolf ln 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     3.30s processing: [../../nena_corpus/nena/0.01/Barwar/Measure for Measure.nena]\n",
      "   |     3.35s processing: [../../nena_corpus/nena/0.01/Barwar/Nanno and Jəndo.nena]\n",
      "   |     3.54s processing: [../../nena_corpus/nena/0.01/Barwar/Qaṭina Rescues His Nephew From Leliθa.nena]\n",
      "   |     3.64s processing: [../../nena_corpus/nena/0.01/Barwar/Sour Grapes.nena]\n",
      "   |     3.66s processing: [../../nena_corpus/nena/0.01/Barwar/Tales From the 1001 Nights.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 1\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 7\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 8\n",
      "force-closing types {'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 10\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 11\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 12\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 13\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 14\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 15\n",
      "force-closing types {'subsentence', 'sentence'} in Qaṭina Rescues His Nephew From Leliθa ln 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     4.49s processing: [../../nena_corpus/nena/0.01/Barwar/The Battle With Yuwanəs the Armenian.nena]\n",
      "   |     4.65s processing: [../../nena_corpus/nena/0.01/Barwar/The Bear and the Fox.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 20\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 21\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 22\n",
      "force-closing types {'inton', 'sentence'} in The Battle With Yuwanəs the Armenian ln 25\n",
      "force-closing types {'subsentence', 'sentence'} in The Battle With Yuwanəs the Armenian ln 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     4.86s processing: [../../nena_corpus/nena/0.01/Barwar/The Brother of Giants.nena]\n",
      "   |     5.01s processing: [../../nena_corpus/nena/0.01/Barwar/The Cat and the Mice.nena]\n",
      "   |     5.04s processing: [../../nena_corpus/nena/0.01/Barwar/The Cooking Pot.nena]\n",
      "   |     5.12s processing: [../../nena_corpus/nena/0.01/Barwar/The Crafty Hireling.nena]\n",
      "   |     5.48s processing: [../../nena_corpus/nena/0.01/Barwar/The Crow and the Cheese.nena]\n",
      "   |     5.49s processing: [../../nena_corpus/nena/0.01/Barwar/The Daughter of the King.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Crafty Hireling ln 43\n",
      "force-closing types {'subsentence', 'sentence'} in The Crafty Hireling ln 54\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 1\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 5\n",
      "force-closing types {'subsentence', 'sentence'} in The Crow and the Cheese ln 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     5.85s processing: [../../nena_corpus/nena/0.01/Barwar/The Fox and the Lion.nena]\n",
      "   |     5.88s processing: [../../nena_corpus/nena/0.01/Barwar/The Fox and the Miller.nena]\n",
      "   |     6.10s processing: [../../nena_corpus/nena/0.01/Barwar/The Fox and the Stork.nena]\n",
      "   |     6.12s processing: [../../nena_corpus/nena/0.01/Barwar/The Giant’s Cave.nena]\n",
      "   |     6.18s processing: [../../nena_corpus/nena/0.01/Barwar/The Girl and the Seven Brothers.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Girl and the Seven Brothers ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Girl and the Seven Brothers ln 3\n",
      "force-closing types {'sentence'} in The Girl and the Seven Brothers ln 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     6.55s processing: [../../nena_corpus/nena/0.01/Barwar/The King With Forty Sons.nena]\n",
      "   |     7.28s processing: [../../nena_corpus/nena/0.01/Barwar/The Leliθa From č̭āl.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The King With Forty Sons ln 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     7.35s processing: [../../nena_corpus/nena/0.01/Barwar/The Lion King.nena]\n",
      "   |     7.38s processing: [../../nena_corpus/nena/0.01/Barwar/The Lion With a Swollen Leg.nena]\n",
      "   |     7.48s processing: [../../nena_corpus/nena/0.01/Barwar/The Man Who Cried Wolf.nena]\n",
      "   |     7.53s processing: [../../nena_corpus/nena/0.01/Barwar/The Man Who Wanted to Work.nena]\n",
      "   |     7.81s processing: [../../nena_corpus/nena/0.01/Barwar/The Monk Who Wanted to Know When He Would Die.nena]\n",
      "   |     7.92s processing: [../../nena_corpus/nena/0.01/Barwar/The Monk and the Angel.nena]\n",
      "   |     8.28s processing: [../../nena_corpus/nena/0.01/Barwar/The Priest and the Mullah.nena]\n",
      "   |     8.42s processing: [../../nena_corpus/nena/0.01/Barwar/The Sale of an Ox.nena]\n",
      "   |     8.77s processing: [../../nena_corpus/nena/0.01/Barwar/The Scorpion and the Snake.nena]\n",
      "   |     8.83s processing: [../../nena_corpus/nena/0.01/Barwar/The Selfish Neighbour.nena]\n",
      "   |     8.87s processing: [../../nena_corpus/nena/0.01/Barwar/The Sisisambər Plant.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Sale of an Ox ln 41\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 2\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 8\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 9\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 14\n",
      "force-closing types {'subsentence', 'sentence'} in The Sisisambər Plant ln 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     8.94s processing: [../../nena_corpus/nena/0.01/Barwar/The Story With No End.nena]\n",
      "   |     8.98s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Farxo and Səttiya.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Tale of Farxo and Səttiya ln 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |     9.60s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Mămo and Zine.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Mămo and Zine ln 22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       11s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Mərza Pămət.nena]\n",
      "   |       11s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Nasimo.nena]\n",
      "   |       11s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Parizada, Warda and Nargis.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Mərza Pămət ln 32\n",
      "force-closing types {'sentence'} in The Tale of Nasimo ln 3\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 4\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 5\n",
      "force-closing types {'sentence'} in The Tale of Nasimo ln 6\n",
      "force-closing types {'subsentence', 'sentence'} in The Tale of Nasimo ln 7\n",
      "force-closing types {'inton'} in The Tale of Parizada, Warda and Nargis ln 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       12s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Rustam (1).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Parizada, Warda and Nargis ln 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       12s processing: [../../nena_corpus/nena/0.01/Barwar/The Tale of Rustam (2).nena]\n",
      "   |       13s processing: [../../nena_corpus/nena/0.01/Barwar/The Wise Daughter of the King.nena]\n",
      "   |       13s processing: [../../nena_corpus/nena/0.01/Barwar/The Wise Snake.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Tale of Rustam (2) ln 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       13s processing: [../../nena_corpus/nena/0.01/Barwar/The Wise Young Man.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Wise Snake ln 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       13s processing: [../../nena_corpus/nena/0.01/Barwar/šošət Xere.nena]\n",
      "   |       13s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Close Shave.nena]\n",
      "   |       13s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Cure for a Husband’s Madness.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Wise Young Man ln 25\n",
      "force-closing types {'subsentence', 'sentence'} in šošət Xere ln 6\n",
      "force-closing types {'sentence'} in šošət Xere ln 7\n",
      "force-closing types {'subsentence', 'inton', 'sentence'} in šošət Xere ln 8\n",
      "force-closing types {'sentence'} in šošət Xere ln 10\n",
      "force-closing types {'subsentence', 'sentence'} in šošət Xere ln 11\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 1\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 4\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 5\n",
      "force-closing types {'inton'} in A Cure for a Husband’s Madness ln 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Donkey Knows Best.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Dragon in the Well.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Dutiful Son.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Frog Wants a Husband.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Lost Donkey.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Lost Ring.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Painting of the King of Iran.nena]\n",
      "   |       15s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Pound of Flesh.nena]\n",
      "   |       16s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Sweater to Pay Off a Debt.nena]\n",
      "   |       16s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Thousand Dinars.nena]\n",
      "   |       16s processing: [../../nena_corpus/nena/0.01/Urmi_C/A Visit From Harun Ar-Rashid.nena]\n",
      "   |       16s processing: [../../nena_corpus/nena/0.01/Urmi_C/Agriculture and Village Life.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in A Thousand Dinars ln 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       17s processing: [../../nena_corpus/nena/0.01/Urmi_C/Am I Dead?.nena]\n",
      "   |       17s processing: [../../nena_corpus/nena/0.01/Urmi_C/An Orphan Duckling.nena]\n",
      "   |       17s processing: [../../nena_corpus/nena/0.01/Urmi_C/Axiqar.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'word', 'inton', 'sentence'} in Axiqar ln 28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       18s processing: [../../nena_corpus/nena/0.01/Urmi_C/Events in 1946 on the Urmi Plain.nena]\n",
      "   |       18s processing: [../../nena_corpus/nena/0.01/Urmi_C/Games.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in Axiqar ln 89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       18s processing: [../../nena_corpus/nena/0.01/Urmi_C/Hunting.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/I Have Died.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Ice for Dinner.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Is There a Man With No Worries?.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Kindness to a Donkey.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Lost Money.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Mistaken Identity.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Much Ado About Nothing.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Nipuxta.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/No Bread Today.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Problems Lighting a Fire.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/St. Zayya’s Cake Dough.nena]\n",
      "   |       19s processing: [../../nena_corpus/nena/0.01/Urmi_C/Star-Crossed Lovers.nena]\n",
      "   |       20s processing: [../../nena_corpus/nena/0.01/Urmi_C/Stomach Trouble.nena]\n",
      "   |       20s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Adventures of Ashur.nena]\n",
      "   |       21s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Adventures of Two Brothers.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Adventures of Ashur ln 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       21s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Adventures of a Princess.nena]\n",
      "   |       22s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Angel of Death.nena]\n",
      "   |       22s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Assyrians of Armenia.nena]\n",
      "   |       22s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Assyrians of Urmi.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'sentence'} in The Assyrians of Armenia ln 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       22s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Bald Child and the Monsters.nena]\n",
      "   |       23s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Bald Man and the King.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Bird and the Fox.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Cat’s Dinner.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Cow and the Poor Girl.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Dead Rise and Return.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Fisherman and the Princess.nena]\n",
      "   |       24s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Giant One-Eyed Demon.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Little Prince and the Snake.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Fisherman and the Princess ln 2\n",
      "force-closing types {'inton'} in The Fisherman and the Princess ln 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Loan of a Cooking Pot.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Man Who Wanted to Complain to God.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Old Man and the Fish.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Purchase of a Donkey.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Snake’s Dilemma.nena]\n",
      "   |       25s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Stupid Carpenter.nena]\n",
      "   |       26s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Wife Who Learns How to Work (2).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'sentence'} in The Snake’s Dilemma ln 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       26s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Wife Who Learns How to Work.nena]\n",
      "   |       26s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Wife’s Condition.nena]\n",
      "   |       26s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Wise Brother.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in The Wife Who Learns How to Work ln 1\n",
      "force-closing types {'inton'} in The Wife Who Learns How to Work ln 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       27s processing: [../../nena_corpus/nena/0.01/Urmi_C/The Wise Young Daughter.nena]\n",
      "   |       28s processing: [../../nena_corpus/nena/0.01/Urmi_C/Trickster.nena]\n",
      "   |       28s processing: [../../nena_corpus/nena/0.01/Urmi_C/Two Birds Fall in Love.nena]\n",
      "   |       28s processing: [../../nena_corpus/nena/0.01/Urmi_C/Two Wicked Daughters-In-Law.nena]\n",
      "   |       28s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life (2).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'subsentence', 'inton', 'sentence'} in Two Wicked Daughters-In-Law ln 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       28s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life (3).nena]\n",
      "   |       29s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life (4).nena]\n",
      "   |       29s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life (5).nena]\n",
      "   |       29s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life (6).nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in Village Life (5) ln 1\n",
      "force-closing types {'inton'} in Village Life (6) ln 34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       30s processing: [../../nena_corpus/nena/0.01/Urmi_C/Village Life.nena]\n",
      "   |       30s processing: [../../nena_corpus/nena/0.01/Urmi_C/Vineyards.nena]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force-closing types {'inton'} in Village Life ln 1\n",
      "force-closing types {'inton'} in Village Life ln 5\n",
      "force-closing types {'inton'} in Village Life ln 18\n",
      "force-closing types {'inton'} in Village Life ln 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   |       30s processing: [../../nena_corpus/nena/0.01/Urmi_C/Weddings and Festivals.nena]\n",
      "   |       32s processing: [../../nena_corpus/nena/0.01/Urmi_C/Weddings.nena]\n",
      "   |       32s processing: [../../nena_corpus/nena/0.01/Urmi_C/When Shall I Die?.nena]\n",
      "   |       32s processing: [../../nena_corpus/nena/0.01/Urmi_C/Women Are Stronger Than Men.nena]\n",
      "   |       32s processing: [../../nena_corpus/nena/0.01/Urmi_C/Women Do Things Best.nena]\n",
      "   |       32s \"edge\" actions: 0\n",
      "   |       32s \"feature\" actions: 756315\n",
      "   |       32s \"node\" actions: 294155\n",
      "   |       32s \"resume\" actions: 0\n",
      "   |       32s \"slot\" actions: 539381\n",
      "   |       32s \"terminate\" actions: 833720\n",
      "   |          2 x \"dialect\" node \n",
      "   |      35985 x \"inton\" node \n",
      "   |     539381 x \"letter\" node  = slot type\n",
      "   |       2544 x \"line\" node \n",
      "   |     120148 x \"morpheme\" node \n",
      "   |        351 x \"paragraph\" node \n",
      "   |      16708 x \"sentence\" node \n",
      "   |      24528 x \"subsentence\" node \n",
      "   |        126 x \"text\" node \n",
      "   |      93763 x \"word\" node \n",
      "   |     833536 nodes of all types\n",
      "   |       32s OK\n",
      "   |     0.11s Removing unlinked nodes ... \n",
      "   |      |     0.00s      1 unlinked \"word\" node: [51059]\n",
      "   |      |     0.00s      1 unlinked \"inton\" node: [19594]\n",
      "   |      |     0.00s      1 unlinked \"subsentence\" node: [14318]\n",
      "   |      |     0.00s      1 unlinked \"sentence\" node: [9796]\n",
      "   |      |     0.00s      4 unlinked nodes\n",
      "   |      |     0.00s Leaving 833532 nodes\n",
      "   |     0.00s checking for nodes and edges ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s checking features ... \n",
      "   |     0.00s OK\n",
      "   |     0.00s reordering nodes ...\n",
      "   |     0.17s Sorting 2 nodes of type \"dialect\"\n",
      "   |     0.20s Sorting 35984 nodes of type \"inton\"\n",
      "   |     0.32s Sorting 2544 nodes of type \"line\"\n",
      "   |     0.37s Sorting 120148 nodes of type \"morpheme\"\n",
      "   |     0.62s Sorting 351 nodes of type \"paragraph\"\n",
      "   |     0.66s Sorting 16707 nodes of type \"sentence\"\n",
      "   |     0.77s Sorting 24527 nodes of type \"subsentence\"\n",
      "   |     0.87s Sorting 126 nodes of type \"text\"\n",
      "   |     0.91s Sorting 93762 nodes of type \"word\"\n",
      "   |     1.12s Max node = 833532\n",
      "   |     1.12s OK\n",
      "   |     0.00s reassigning feature values ...\n",
      "   |      |     1.37s node feature \"comment\" with 1 node\n",
      "   |      |     1.37s node feature \"continued_from\" with 1 node\n",
      "   |      |     1.37s node feature \"dialect\" with 2 nodes\n",
      "   |      |     1.37s node feature \"efuzzy\" with 753291 nodes\n",
      "   |      |     1.62s node feature \"end\" with 753291 nodes\n",
      "   |      |     1.86s node feature \"etrans_f\" with 753291 nodes\n",
      "   |      |     2.10s node feature \"etrans_l\" with 753291 nodes\n",
      "   |      |     2.33s node feature \"footnotes\" with 6 nodes\n",
      "   |      |     2.33s node feature \"foreign\" with 1041 nodes\n",
      "   |      |     2.33s node feature \"informant\" with 126 nodes\n",
      "   |      |     2.33s node feature \"lang\" with 898 nodes\n",
      "   |      |     2.34s node feature \"number\" with 2895 nodes\n",
      "   |      |     2.34s node feature \"place\" with 126 nodes\n",
      "   |      |     2.34s node feature \"source\" with 126 nodes\n",
      "   |      |     2.34s node feature \"speaker\" with 213910 nodes\n",
      "   |      |     2.45s node feature \"t_fuzzy\" with 753291 nodes\n",
      "   |      |     2.70s node feature \"text\" with 753291 nodes\n",
      "   |      |     2.96s node feature \"text_id\" with 125 nodes\n",
      "   |      |     2.96s node feature \"title\" with 126 nodes\n",
      "   |      |     2.96s node feature \"trans_f\" with 753291 nodes\n",
      "   |      |     3.20s node feature \"trans_l\" with 753291 nodes\n",
      "   |      |     3.44s node feature \"version\" with 2 nodes\n",
      "   |     2.32s OK\n",
      "  0.00s Exporting 23 node and 1 edge and 1 config features to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.10s VALIDATING oslots feature\n",
      "  0.10s maxSlot=     539381\n",
      "  0.10s maxNode=     833532\n",
      "  0.15s OK: oslots is valid\n",
      "   |     0.00s T comment              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T continued_from       to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T dialect              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.88s T efuzzy               to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.87s T end                  to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.87s T etrans_f             to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.88s T etrans_l             to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T footnotes            to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T foreign              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T informant            to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T lang                 to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.01s T number               to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.24s T otype                to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T place                to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T source               to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.27s T speaker              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.95s T t_fuzzy              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     1.06s T text                 to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T text_id              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T title                to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     1.19s T trans_f              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.97s T trans_l              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s T version              to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     1.28s T oslots               to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "   |     0.00s M otext                to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n",
      "  9.63s Exported 23 node features and 1 edge features and 1 config features to /Users/cody/github/CambridgeSemiticsLab/nena_tf/tf/0.01\n"
     ]
    }
   ],
   "source": [
    "good = cv.walk(\n",
    "    director,\n",
    "    slotType,\n",
    "    otext=otext,\n",
    "    generic=generic,\n",
    "    intFeatures=intFeatures,\n",
    "    featureMeta=featureMeta,\n",
    "    warn=True,\n",
    "    force=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
