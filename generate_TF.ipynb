{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text-Fabric Resource from Source Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing for Christian Urmi and Barwar texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with the language data in the NENA corpus, it is important to separate the language text from other data, such as titles, authors/informants, and verse numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of the text comes in MS-Word document files. That is probably also the version of the text that is the richest in language data. It contains not only the text itself, but also meaningful formatting, e.g. word markers set in superscript, and foreign (loan) words set in roman type (where the regular text is set in italic type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert that information from a Word document to something that we can use in Python, we first convert the word documents to HTML, using LibreOffice in headless mode. It is assumed that the Word files are in the subdirectory `texts`, where the converted `.html` files will also be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ soffice --headless --convert-to html texts/*.doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces HTML 4.0 documents in the same directory. Earlier attempts with XHTML using wvWare/AbiWord, or LibreOffice using the XHTML conversion filter, produced output that was more difficult to parse or lacked certain characters that were lost in conversion. Although the conversion with LibreOffice takes a very long time compared with AbiWord, the resulting text seems more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom `nena_corpus` package contains the `Text` class, and several functions that assist in the conversion from HTML, of which we only need the functions `html_to_text()` and `parse_metadata()`.\n",
    "\n",
    "The function `html_to_text()` is a generator function yielding `Text` objects, each containing one paragraph of text.\n",
    "\n",
    "The function `parse_metadata()` extracts metadata from heading paragraphs (e.g. `title`, `text_id`, `informant`, `place`).\n",
    "\n",
    "The `Text` class contains an attribute `type` describing the type of paragraph (e.g., `'sectionheading'`, `'p'`, or `'footnote'`), and a list of tuples, containing the text and text style. A text like `'<i>Normal, </i>foreign<i>, and normal</i>'` becomes `[('Normal, ', ''), ('cursive,', 'italic'), (' and normal', '')]` (note the inversion -- because normal text in the source is actually set in italics).\n",
    "\n",
    "`Text` objects are iterable. New items can be appended with the `append(text, text_style)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nena_corpus import Text, html_to_text, parse_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demonstration of the `Text` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dit is een test.\n",
      "<Text 'test' 'Dit is een test.'>\n",
      "[('Dit is ', 'normal'), ('een test', 'test'), ('.', 'normal')]\n",
      "[('Dit is ', 'normal'), ('een test', 'test'), ('.', 'normal')]\n"
     ]
    }
   ],
   "source": [
    "p = Text(p_type='test', default_style='normal')\n",
    "\n",
    "p.append('Dit is ')\n",
    "p.append('een test', 'test')\n",
    "p.append('.')\n",
    "\n",
    "# str(p) returns concatenated string\n",
    "print(p)\n",
    "# repr(p) returns class name, p_type and str(p)\n",
    "print(repr(p))\n",
    "# list(p) returns the list of tuples\n",
    "print(list(p))\n",
    "# a list comprehension also works\n",
    "print([e for e in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some more useful libraries, and set logging level to DEBUG to make sure we see all logging messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pathlib, os\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tf.fabric import Fabric\n",
    "import pandas as pd\n",
    "\n",
    "logging.getLogger().setLevel(logging.DEBUG) # for terminal messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the subdirectory `texts` contains the HTML files generated earlier, we can import all files in the pattern `texts/*.html`. At this point we just want to do language statistics and not look at the actual texts, so it is sufficient to import the paragraphs of all texts in no particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_barwar = pathlib.Path.cwd().glob('texts/bar text *.html') # get source texts\n",
    "files_urmi_c = pathlib.Path.cwd().glob('texts/cu *.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also prepare a dictionary with some characters that need to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characters to be replaced\n",
    "replace = {\n",
    "    '\\u2011': '\\u002d',  # U+2011 NON-BREAKING HYPHEN -> U+002D HYPHEN-MINUS\n",
    "    '\\u01dd': '\\u0259',  # U+01DD LATIN SMALL LETTER TURNED E -> U+0259 LATIN SMALL LETTER SCHWA\n",
    "    '\\uf1ea': '\\u003d',  # U+F1EA Deprecated SIL character -> U+003D '=' EQUALS SIGN\n",
    "    '\\u2026': '...',  # U+2026 '…' HORIZONTAL ELLIPSIS -> three dots\n",
    "    'J\\u0335': '\\u0248',  # 'J' + U+0335 COMBINING SHORT STROKE OVERLAY -> U+0248 'Ɉ' LATIN CAPITAL LETTER J WITH STROKE\n",
    "    'J\\u0336': '\\u0248',  # 'J' + U+0336 COMBINING LONG STROKE OVERLAY -> U+0248 'Ɉ' LATIN CAPITAL LETTER J WITH STROKE\n",
    "    '\\u002d\\u032d': '\\u032d\\u002d',  # Switch positions of Hyphen and Circumflex accent below\n",
    "    '\\u2011\\u032d': '\\u032d\\u002d',  # Switch positions of Non-breaking hyphen and Circumflex accent below\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go right ahead to loop over the html files and convert them to a TextFabric structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing file bar text a15-A17.html ...\n",
      "INFO:root:Processing file bar text A45.html ...\n",
      "INFO:root:Processing file bar text a28.html ...\n",
      "INFO:root:Processing file bar text A49.html ...\n",
      "INFO:root:Processing file bar text a24.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 7 '.\n",
      "INFO:root:Processing file bar text A42-A44.html ...\n",
      "INFO:root:Processing file bar text a25.html ...\n",
      "INFO:root:Processing file bar text a48.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 1 '.\n",
      "INFO:root:Processing file bar text a29.html ...\n",
      "INFO:root:Processing file bar text A9-A13.html ...\n",
      "INFO:root:Processing file bar text a46-A47.html ...\n",
      "INFO:root:Processing file bar text A37-A40.html ...\n",
      "INFO:root:Processing file bar text a18.html ...\n",
      "INFO:root:Processing file bar text a1-A7.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote1'.\n",
      "DEBUG:root:Text: ' 1 The name Čuxo means ‘one who wears the woolen čuxa garment’. '.\n",
      "INFO:root:Processing file bar text a34.html ...\n",
      "INFO:root:Processing file bar text A14.html ...\n",
      "INFO:root:Processing file bar text a35.html ...\n",
      "INFO:root:Processing file bar text a36.html ...\n",
      "INFO:root:Processing file bar text a41.html ...\n",
      "INFO:root:Processing file bar text a8.html ...\n",
      "INFO:root:Processing file bar text a19-A23.html ...\n",
      "INFO:root:Processing file bar text a26.html ...\n",
      "INFO:root:Processing file bar text a30.html ...\n",
      "INFO:root:Processing file bar text a31-A33.html ...\n",
      "INFO:root:Processing file bar text a50-A52.html ...\n",
      "INFO:root:Processing file bar text a27.html ...\n",
      "INFO:root:Processing file cu vol 4 texts.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "WARNING:root:Unfinished marker: 'R', closed forcibly..\n",
      "DEBUG:root:Urmi_C, A43:17\n",
      "DEBUG:root:Text: ' várdə=da mattúvvəla k̭am-bràto.| ʾáxči cálu labùlola.| +p̂urmìlux k̭a-díyyi?| lublàlun,| lublàlun,| lublàlun,| lublàlun.|'\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote1'.\n",
      "DEBUG:root:Text: ' 1 In the original recording of the story the speaker used the word camra ‘animal droppings’ here, but subsequently corrected this to calla. '.\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote2'.\n",
      "DEBUG:root:Text: ' 2 Mistake for šənnə +xarayə. '.\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 189 '.\n"
     ]
    }
   ],
   "source": [
    "def combine_chars(text):\n",
    "    \"\"\"Yield letters combined with combining diacritics\"\"\"\n",
    "    \n",
    "    char = [] # compose string here: letter + diacritic\n",
    "    \n",
    "    for c in text:\n",
    "        \n",
    "        # add diacritic\n",
    "        # indicated as 'Mn': non-spacing combining mark\n",
    "        if unicodedata.category(c) == 'Mn':\n",
    "            char.append(c)\n",
    "            continue\n",
    "        \n",
    "        # yield the string; at this point will be: letter + (diacritic)\n",
    "        if char:\n",
    "            yield ''.join(char)\n",
    "            \n",
    "        char = [c] # save in list and get diacritic next if there is one\n",
    "        \n",
    "    yield ''.join(char) # yield empty chars\n",
    "\n",
    "# keep object counts\n",
    "raw_features = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "raw_oslots = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "\n",
    "# initialize counters (will be increased to start from 1)\n",
    "this_text = 0\n",
    "this_paragraph = 0\n",
    "this_line = 0\n",
    "this_sentence = 0\n",
    "this_subsentence = 0\n",
    "this_word = 0\n",
    "this_morpheme = 0\n",
    "this_foreign = 0\n",
    "this_prosa = 0\n",
    "\n",
    "slot = 0 # i.e. chars\n",
    "\n",
    "process_dialects = {'Barwar': files_barwar,\n",
    "                    'Urmi_C': files_urmi_c}\n",
    "\n",
    "text_ids = []\n",
    "\n",
    "for dialect, files in process_dialects.items():\n",
    "    \n",
    "    # TODO At this point record book/publication/dialect?\n",
    "    # E.g. SSLL_2016_Urmi_C, HOS_2008_Barwar?\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        logging.info(f'Processing file {file.name} ...')\n",
    "        \n",
    "        for p in html_to_text(file, replace=replace):\n",
    "            # metadata:\n",
    "            # - dialect\n",
    "            # - file.name\n",
    "            \n",
    "            if p.type.startswith('gp-') and str(p).strip():\n",
    "                # store metadata from headings:\n",
    "                # - text_id\n",
    "                # - title\n",
    "                # - informant\n",
    "                # - place\n",
    "                # - version (if applicable -- only Urmi_C A35)\n",
    "                if p.type.startswith('gp-sectionheading'):\n",
    "                    metadata = {}\n",
    "                for k, v in parse_metadata(p):\n",
    "                    metadata[k] = v\n",
    "            \n",
    "            elif p.type == 'p':\n",
    "                # regular paragraphs\n",
    "                \n",
    "                # format a text_id with version added (if there is one)\n",
    "                version = metadata.get('version', '')\n",
    "                version = f'.{version[-1]}' if version else ''\n",
    "                text_id = metadata.get('text_id', '') + version\n",
    "                \n",
    "                # first check if we need to update metadata\n",
    "                # informant and place are also added as features of text\n",
    "                if (metadata\n",
    "                    and (not raw_features['text_id']\n",
    "                         or raw_features['text_id'][this_text] != text_id)):\n",
    "                        \n",
    "                        \n",
    "                    text_ids.append(text_id)\n",
    "                        \n",
    "                    this_text += 1\n",
    "                    raw_features['text_id'][this_text] = text_id\n",
    "                    raw_features['title'][this_text] = metadata['title']\n",
    "                    raw_features['informant'][this_text] = metadata['informant']\n",
    "                    raw_features['place'][this_text] = metadata['place']\n",
    "                    raw_features['dialect'][this_text] = dialect\n",
    "                    raw_features['filename'][this_text] = file.name\n",
    "                \n",
    "                # increment paragraph\n",
    "                this_paragraph += 1\n",
    "                \n",
    "                # start paragraph with an empty marker stack\n",
    "                marker_stack = []\n",
    "                \n",
    "                # set end-of-unit markers to True at the beginning of paragraph,\n",
    "                # so the units can be increased on encounter of first word character\n",
    "                sentence_end = True\n",
    "                subsentence_end = True\n",
    "                word_end = True\n",
    "                morpheme_end = True\n",
    "                foreign_end = True\n",
    "                prosa_end = True\n",
    "                \n",
    "                for text, text_style in p:\n",
    "                    \n",
    "                    if text_style == 'verse_no':\n",
    "                        this_line += 1\n",
    "                        raw_features['line'][this_line] = text.strip(' ()') # TODO int()?\n",
    "                        metadata['verse_no'] = text.strip(' ()')  # TODO Remove from metadata dict?\n",
    "                        continue\n",
    "                        \n",
    "                    elif text_style == 'fn_anchor':\n",
    "                        # TODO handle footnotes in some way, discard for now\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style == 'comment':\n",
    "                        continue  # TODO handle comments\n",
    "                    \n",
    "                    elif text_style == 'marker':\n",
    "                        if marker_stack and marker_stack[-1] == text:\n",
    "                            marker_stack.pop()\n",
    "                        else:\n",
    "                            marker_stack.append(text)\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style not in ('', 'foreign'):\n",
    "                        logging.debug(f'Unhandled text_style: {repr(text_style)}, {repr(text)}')\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style == 'foreign' and foreign_end:\n",
    "                        foreign_end = False\n",
    "                        this_foreign += 1\n",
    "                        if marker_stack:\n",
    "                            language = marker_stack[-1]\n",
    "                        else:\n",
    "                            language = ''\n",
    "                        raw_features['language'][this_foreign] = language\n",
    "                    \n",
    "                    else: # text_style == '':\n",
    "                        if not foreign_end:\n",
    "                            foreign_end = True\n",
    "                        pass\n",
    "                    \n",
    "                    if (text_style == '' and marker_stack\n",
    "                        and any(c.isalpha() for c in text)\n",
    "                        and not text.isalpha()):\n",
    "                        # In one case, there is no closing marker tag, so force closing the marker\n",
    "                        # Urmi_C A42 9: 'RzdànyəlaR' (p.154, r.28) 'zdàny' roman, 'əla' cursive\n",
    "                        # Urmi_C A43 17: 'ʾe-Rbuk̭ḗṱ' (p. 174, r.14), no closing 'R'\n",
    "                        # Urmi_C B2 16: 'Pʾafšɑ̄rī̀P' (p.250 r.17), inital 'ʾ' cursive\n",
    "                        marker = marker_stack.pop()\n",
    "                        logging.warning(f'Unfinished marker: {repr(marker)}, closed forcibly..')\n",
    "                        logging.debug(f'{dialect}, {metadata[\"text_id\"]}:{metadata[\"verse_no\"]}')\n",
    "                        logging.debug(f'Text: {repr(text)}')\n",
    "                    \n",
    "                    # If we got this far, we have a text string,\n",
    "                    # with either text_style '' or 'foreign'.\n",
    "                    # We will iterate over them character by character.\n",
    "                    for c in combine_chars(text):\n",
    "                        \n",
    "                        if c[0].isalpha() or c == '+':\n",
    "                            \n",
    "                            # Increment text units on start of new word\n",
    "                            if morpheme_end:\n",
    "                                this_morpheme += 1\n",
    "                                morpheme_end = False\n",
    "                            if word_end:\n",
    "                                this_word += 1\n",
    "                                word_end = False\n",
    "                            if subsentence_end:\n",
    "                                this_subsentence += 1\n",
    "                                subsentence_end = False\n",
    "                            if sentence_end:\n",
    "                                this_sentence += 1\n",
    "                                sentence_end = False\n",
    "                            if prosa_end:\n",
    "                                this_prosa += 1\n",
    "                                prosa_end = False\n",
    "                            \n",
    "                            slot += 1\n",
    "                            raw_features['utf8'][slot] = c\n",
    "                            # initialize 'trailer' feature as empty string,\n",
    "                            # so we can add characters with '+' operator later\n",
    "                            raw_features['trailer'][slot] = ''\n",
    "                            \n",
    "                            raw_oslots['text'][this_text].add(slot)\n",
    "                            raw_oslots['paragraph'][this_paragraph].add(slot)\n",
    "                            raw_oslots['line'][this_line].add(slot)\n",
    "                            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "                            raw_oslots['subsentence'][this_subsentence].add(slot)\n",
    "                            raw_oslots['prosa'][this_prosa].add(slot)\n",
    "                            if not word_end:\n",
    "                                raw_oslots['word'][this_word].add(slot)\n",
    "                            if not morpheme_end:\n",
    "                                raw_oslots['morpheme'][this_morpheme].add(slot)\n",
    "                            if not foreign_end:\n",
    "                                raw_oslots['foreign'][this_foreign].add(slot)\n",
    "                        \n",
    "                        else:  # if c is anything but a letter or '+':\n",
    "                            if slot == 0:\n",
    "                                continue  # discard anything before first word character\n",
    "                            if not morpheme_end:\n",
    "                                morpheme_end = True\n",
    "                            if c == '|':\n",
    "                                prosa_end = True\n",
    "                                c = '\\u02c8'\n",
    "                            if c not in ('-', '=') and not word_end:\n",
    "                                word_end = True\n",
    "                            if c == ',' and not subsentence_end:\n",
    "                                subsentence_end = True\n",
    "                            if c in ('.', '!', '?') and not sentence_end:\n",
    "                                subsentence_end = True\n",
    "                                sentence_end = True\n",
    "                            \n",
    "                            raw_features['trailer'][slot] += c\n",
    "                \n",
    "            else:\n",
    "                logging.debug(f'Unhandled paragraph type: {repr(p.type)}.')\n",
    "                logging.debug(f'Text: {repr(str(p))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindex Objects Above Slot Levels\n",
    "\n",
    "We have given all objects a preliminary node number. Now those node numbers will be renumbered starting from the max slot number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "otype2feature = {\n",
    "    'text': {'text_id', 'title', 'dialect', 'filename', 'informant', 'place'},\n",
    "    'paragraph': {},\n",
    "    'line': {'line'},\n",
    "    'sentence': {},\n",
    "    'subsentence': {},\n",
    "    'word': {'trailer'},\n",
    "    'morpheme': {},\n",
    "    'foreign': {'language'},\n",
    "    'prosa': {},\n",
    "}\n",
    "\n",
    "onode = max(raw_features['utf8']) # max slot, incremented +1 in loop\n",
    "node_features = collections.defaultdict(lambda:collections.defaultdict())\n",
    "edge_features = collections.defaultdict(lambda:collections.defaultdict(set)) # oslots will go here\n",
    "\n",
    "# first add slot features\n",
    "# object features must then be added with otype2feature\n",
    "node_features['utf8'] = raw_features['utf8']\n",
    "node_features['trailer'] = raw_features['trailer']\n",
    "\n",
    "# add slot object types\n",
    "for slot in node_features['utf8']:\n",
    "    node_features['otype'][slot] = 'char'    \n",
    "    \n",
    "# for objects above slot level, \n",
    "# assign new node number and link to feature\n",
    "for otype in raw_oslots.keys():\n",
    "    for oID, slots in raw_oslots[otype].items():\n",
    "        \n",
    "        # make new object node number\n",
    "        onode += 1\n",
    "        node_features['otype'][onode] = otype\n",
    "        \n",
    "        # remap node features to node number\n",
    "        for feat in otype2feature[otype]:\n",
    "            node_features[feat][onode] = raw_features[feat][oID]\n",
    "        edge_features['oslots'][onode] = raw_oslots[otype][oID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following features are logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['utf8', 'trailer', 'otype', 'text_id', 'informant', 'filename', 'place', 'dialect', 'title', 'line', 'language'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following edges are logged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oslots'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purge Old TF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in pathlib.Path.cwd().glob('tf/?*.tf'):\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save New TF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "0 features found and 0 ignored\n",
      "  0.01s Warp feature \"otype\" not found in\n",
      "tf//\n",
      "  0.01s Warp feature \"oslots\" not found in\n",
      "tf//\n",
      "  0.06s Warp feature \"otext\" not found. Working without Text-API\n",
      "\n",
      "  0.00s Exporting 11 node and 1 edge and 4 config features to tf/:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.10s VALIDATING oslots feature\n",
      "  0.10s maxSlot=     551014\n",
      "  0.10s maxNode=     846438\n",
      "  0.16s OK: oslots is valid\n",
      "   |     0.00s T dialect              to tf\n",
      "   |     0.00s T filename             to tf\n",
      "   |     0.00s T informant            to tf\n",
      "   |     0.00s T language             to tf\n",
      "   |     0.01s T line                 to tf\n",
      "   |     0.23s T otype                to tf\n",
      "   |     0.00s T place                to tf\n",
      "   |     0.00s T text_id              to tf\n",
      "   |     0.00s T title                to tf\n",
      "   |     0.92s T trailer              to tf\n",
      "   |     0.79s T utf8                 to tf\n",
      "   |     1.23s T oslots               to tf\n",
      "   |     0.00s M otext                to tf\n",
      "   |     0.00s M paragraph            to tf\n",
      "   |     0.00s M text                 to tf\n",
      "   |     0.00s M word                 to tf\n",
      "  3.37s Exported 11 node features and 1 edge features and 4 config features to tf/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otext = {\n",
    "    'sectionTypes': 'text,line',\n",
    "    'sectionFeatures': 'text_id,line',\n",
    "    'fmt:text-orig-full': '{utf8}{trailer}',\n",
    "    }\n",
    "\n",
    "mastermeta = {'author': 'Geoffrey Khan, Cody Kingham, and Hannes Vlaardingerbroek'}\n",
    "\n",
    "meta = {'':mastermeta,\n",
    "        'oslots':{'edgeValues':False, 'valueType':'int'},\n",
    "        'otype':{'valueType':'str'},\n",
    "        'text':{'valueType':'str'},\n",
    "        'paragraph':{'valueType':'str'},\n",
    "        'line':{'valueType':'str'},\n",
    "        'word':{'valueType':'str'},\n",
    "        'utf8':{'valueType':'str'},\n",
    "        'text_id':{'valueType':'str'},\n",
    "        'title':{'valueType':'str'},\n",
    "        'dialect':{'valueType':'str'},\n",
    "        'filename':{'valueType':'str'},\n",
    "        'language':{'valueType':'str'},\n",
    "        'trailer':{'valueType':'str'},\n",
    "        'informant':{'valueType':'str'},\n",
    "        'place':{'valueType':'str'},\n",
    "        'otext':otext\n",
    "       }\n",
    "\n",
    "TFs = Fabric(locations=['tf/'])\n",
    "\n",
    "TFs.save(nodeFeatures=node_features, edgeFeatures=edge_features, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New TF Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "16 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.34s T otype                from tf\n",
      "   |     4.91s T oslots               from tf\n",
      "   |     0.00s No section config in otext, the section part of the T-API cannot be used\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n",
      "   |     1.27s T utf8                 from tf\n",
      "   |     0.88s T trailer              from tf\n",
      "   |      |     0.24s C __levels__           from otype, oslots, otext\n",
      "   |      |     7.06s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.39s C __rank__             from otype, __order__\n",
      "   |      |     8.89s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |     2.07s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     2.84s C __boundary__         from otype, oslots, __rank__\n",
      "   |     0.00s T text_id              from tf\n",
      "   |     0.01s T line                 from tf\n",
      "   |     0.00s T title                from tf\n",
      "   |     0.00s T dialect              from tf\n",
      "   |     0.00s T language             from tf\n",
      "    29s All features loaded/computed - for details use loadLog()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = TF.load('''\n",
    "\n",
    "text_id paragraph line word utf8 \n",
    "otype title dialect language trailer\n",
    "\n",
    "''')\n",
    "\n",
    "N.makeAvailableIn(globals())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhancements and Extensions\n",
    "\n",
    "Some features are easier to make once the TF resource is built. The following features will be constructed:\n",
    "\n",
    "* `utf8` will be extended to word objects to aid in word searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extend_features = collections.defaultdict(lambda: collections.defaultdict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend `utf8` Feature to Word Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first re-create the feature on characters\n",
    "for char in F.otype.s('char'):\n",
    "    extend_features['utf8'][char] = F.utf8.v(char)\n",
    "    \n",
    "# add words; compose without trailer\n",
    "for word in F.otype.s('word'):\n",
    "    \n",
    "    txt = ''\n",
    "    \n",
    "    for char in L.d(word, 'char'):\n",
    "        txt += F.utf8.v(char)\n",
    "    \n",
    "    extend_features['utf8'][word] = txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Extended Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "16 features found and 0 ignored\n",
      "  0.00s Exporting 1 node and 0 edge and 0 config features to tf/:\n",
      "   |     0.95s T utf8                 to tf\n",
      "  0.96s Exported 1 node features and 0 edge features and 0 config features to tf/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = {'':mastermeta,\n",
    "        'utf8': {'valueType':'str'},\n",
    "       }\n",
    "\n",
    "TFs = Fabric(locations=['tf/'])\n",
    "\n",
    "TFs.save(nodeFeatures=extend_features, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Load Enhanced TF Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "16 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.00s No section config in otext, the section part of the T-API cannot be used\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n",
      "   |     1.53s T utf8                 from tf\n",
      "  2.97s All features loaded/computed - for details use loadLog()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = TF.load('''\n",
    "\n",
    "text_id paragraph line word utf8 \n",
    "otype title dialect language trailer\n",
    "\n",
    "''')\n",
    "\n",
    "N.makeAvailableIn(globals())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "### Object Types and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                       126\n",
      "paragraph                  465\n",
      "line                      2543\n",
      "sentence                 16784\n",
      "subsentence              24541\n",
      "prosa                    35967\n",
      "word                     93762\n",
      "foreign                   1102\n",
      "morpheme                120134\n",
      "char                    551014\n"
     ]
    }
   ],
   "source": [
    "for otype in F.otype.all:\n",
    "    print('{:20}{:>10}'.format(otype, len(list(F.otype.s(otype)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books and their word counts: \n",
      "\n",
      "551015 The Monk And The Angel\n",
      "\t680 words, 950 morphemes\n",
      "551016 THE MONK WHO WANTED TO KNOW WHEN HE WOULD DIE\n",
      "\t368 words, 487 morphemes\n",
      "551017 THE WISE YOUNG MAN\n",
      "\t1091 words, 1482 morphemes\n",
      "551018 THE FOX AND THE STORK\n",
      "\t70 words, 102 morphemes\n",
      "551019 THE TALE OF RUSTAM (1)\n",
      "\t1008 words, 1316 morphemes\n",
      "551020 THE CROW AND THE CHEESE\n",
      "\t51 words, 71 morphemes\n",
      "551021 THE TALE OF PARIZADA, WARDA AND NARGIS\n",
      "\t2016 words, 2698 morphemes\n",
      "551022 THE FOX AND THE LION\n",
      "\t95 words, 124 morphemes\n",
      "551023 SOUR GRAPES\n",
      "\t62 words, 82 morphemes\n",
      "551024 THE CAT AND THE MICE\n",
      "\t99 words, 138 morphemes\n",
      "551025 THE TALE OF FARXO AND SƏTTIYA\n",
      "\t2490 words, 3303 morphemes\n",
      "551026 THE MAN WHO CRIED WOLF\n",
      "\t199 words, 250 morphemes\n",
      "551027 THE TALE OF RUSTAM (2)\n",
      "\t1645 words, 2254 morphemes\n",
      "551028 THE SCORPION AND THE SNAKE\n",
      "\t216 words, 298 morphemes\n",
      "551029 I AM WORTH THE SAME AS A BLIND WOLF\n",
      "\t528 words, 713 morphemes\n",
      "551030 DƏMDƏMA\n",
      "\t620 words, 849 morphemes\n",
      "551031 THE KING WITH FORTY SONS\n",
      "\t2539 words, 3366 morphemes\n",
      "551032 A TALE OF TWO KINGS\n",
      "\t513 words, 703 morphemes\n",
      "551033 THE LION KING\n",
      "\t114 words, 150 morphemes\n",
      "551034 MAN IS TREACHEROUS\n",
      "\t227 words, 313 morphemes\n",
      "551035 THE TALE OF NASIMO\n",
      "\t389 words, 500 morphemes\n",
      "551036 ŠOŠƏT XERE\n",
      "\t349 words, 439 morphemes\n",
      "551037 THE BROTHER OF GIANTS\n",
      "\t474 words, 659 morphemes\n",
      "551038 THE WISE DAUGHTER OF THE KING\n",
      "\t388 words, 535 morphemes\n",
      "551039 BABY LELIΘA\n",
      "\t845 words, 1148 morphemes\n",
      "551040 The Wise Snake\n",
      "\t779 words, 1020 morphemes\n",
      "551041 The Priest and the Mullah\n",
      "\t317 words, 435 morphemes\n",
      "551042 The Selfish Neighbour\n",
      "\t146 words, 210 morphemes\n",
      "551043 A tale of a prince and a princess\n",
      "\t1763 words, 2437 morphemes\n",
      "551044 The Cooking Pot\n",
      "\t254 words, 327 morphemes\n",
      "551045 A Hundred Gold Coins\n",
      "\t343 words, 483 morphemes\n",
      "551046 A Man Called Čuxo\n",
      "\t789 words, 1041 morphemes\n",
      "551047 THE GIRL AND THE SEVEN BROTHERS\n",
      "\t744 words, 1030 morphemes\n",
      "551048 TALES FROM THE 1001 NIGHTS\n",
      "\t3018 words, 4195 morphemes\n",
      "551049 NANNO AND JƏNDO\n",
      "\t641 words, 865 morphemes\n",
      "551050 THE STORY WITH NO END\n",
      "\t130 words, 195 morphemes\n",
      "551051 MEASURE FOR MEASURE\n",
      "\t132 words, 174 morphemes\n",
      "551052 Gozali and Nozali\n",
      "\t3828 words, 5424 morphemes\n",
      "551053 THE LELIΘA FROM Č̭ĀL\n",
      "\t252 words, 321 morphemes\n",
      "551054 THE BEAR AND THE FOX\n",
      "\t363 words, 506 morphemes\n",
      "551055 THE DAUGHTER OF THE KING\n",
      "\t1264 words, 1716 morphemes\n",
      "551056 THE SALE OF AN OX\n",
      "\t1294 words, 1711 morphemes\n",
      "551057 THE MAN WHO WANTED TO WORK\n",
      "\t1066 words, 1461 morphemes\n",
      "551058 THE TALE OF MĂMO AND ZINE\n",
      "\t2633 words, 3510 morphemes\n",
      "551059 THE CRAFTY HIRELING\n",
      "\t1362 words, 1818 morphemes\n",
      "551060 THE GIANT’S CAVE\n",
      "\t238 words, 336 morphemes\n",
      "551061 THE FOX AND THE MILLER\n",
      "\t841 words, 1101 morphemes\n",
      "551062 THE LION WITH A SWOLLEN LEG\n",
      "\t378 words, 494 morphemes\n",
      "551063 THE SISISAMBƏR PLANT\n",
      "\t289 words, 385 morphemes\n",
      "551064 QAṬINA RESCUES HIS NEPHEW FROM LELIΘA\n",
      "\t358 words, 492 morphemes\n",
      "551065 THE BATTLE WITH YUWANƏS THE ARMENIAN\n",
      "\t607 words, 780 morphemes\n",
      "551066 THE TALE OF MƏRZA PĂMƏT\n",
      "\t1090 words, 1473 morphemes\n",
      "551067 The Bald Man and the King\n",
      "\t2652 words, 3218 morphemes\n",
      "551068 Women are Stronger than Men\n",
      "\t935 words, 1142 morphemes\n",
      "551069 Axiqar\n",
      "\t2656 words, 3300 morphemes\n",
      "551070 Is there a Man with No Worries?\n",
      "\t796 words, 992 morphemes\n",
      "551071 Women do Things Best\n",
      "\t842 words, 1071 morphemes\n",
      "551072 The Dead Rise and Return\n",
      "\t643 words, 790 morphemes\n",
      "551073 A Pound of Flesh\n",
      "\t752 words, 915 morphemes\n",
      "551074 The Loan of a Cooking Pot\n",
      "\t156 words, 190 morphemes\n",
      "551075 Much Ado About Nothing\n",
      "\t334 words, 392 morphemes\n",
      "551076 A Visit from Harun ar-Rashid\n",
      "\t489 words, 591 morphemes\n",
      "551077 The Cat’s Dinner\n",
      "\t115 words, 129 morphemes\n",
      "551078 Ice for Dinner\n",
      "\t91 words, 108 morphemes\n",
      "551079 Am I dead?\n",
      "\t90 words, 118 morphemes\n",
      "551080 A Thousand Dinars\n",
      "\t472 words, 589 morphemes\n",
      "551081 Kindness to a Donkey\n",
      "\t64 words, 79 morphemes\n",
      "551082 The Stupid Carpenter\n",
      "\t111 words, 142 morphemes\n",
      "551083 A Close Shave\n",
      "\t71 words, 91 morphemes\n",
      "551084 A Sweater to Pay Off a Debt\n",
      "\t77 words, 97 morphemes\n",
      "551085 No Bread Today\n",
      "\t188 words, 239 morphemes\n",
      "551086 An Orphan Duckling\n",
      "\t63 words, 72 morphemes\n",
      "551087 Mistaken Identity\n",
      "\t110 words, 139 morphemes\n",
      "551088 Trickster\n",
      "\t157 words, 193 morphemes\n",
      "551089 Problems Lighting a Fire\n",
      "\t132 words, 156 morphemes\n",
      "551090 The Angel of Death\n",
      "\t82 words, 103 morphemes\n",
      "551091 Stomach Trouble\n",
      "\t41 words, 46 morphemes\n",
      "551092 A Lost Donkey\n",
      "\t65 words, 84 morphemes\n",
      "551093 A Lost Ring\n",
      "\t38 words, 47 morphemes\n",
      "551094 The Purchase of a Donkey\n",
      "\t184 words, 221 morphemes\n",
      "551095 Lost Money\n",
      "\t51 words, 62 morphemes\n",
      "551096 The Wife’s Condition\n",
      "\t210 words, 254 morphemes\n",
      "551097 A Donkey Knows Best\n",
      "\t95 words, 126 morphemes\n",
      "551098 When Shall I Die?\n",
      "\t148 words, 189 morphemes\n",
      "551099 I Have Died\n",
      "\t102 words, 129 morphemes\n",
      "551100 The Fisherman and the Princess\n",
      "\t702 words, 807 morphemes\n",
      "551101 The Wife who Learns How to Work\n",
      "\t690 words, 812 morphemes\n",
      "551102 The Wife who Learns How to Work\n",
      "\t305 words, 386 morphemes\n",
      "551103 A Cure for a Husband’s Madness\n",
      "\t1282 words, 1512 morphemes\n",
      "551104 The Bald Child and the Monsters\n",
      "\t1042 words, 1259 morphemes\n",
      "551105 The Wise Young Daughter\n",
      "\t1005 words, 1228 morphemes\n",
      "551106 The Adventures of Ashur\n",
      "\t2719 words, 3294 morphemes\n",
      "551107 A Dragon in the Well\n",
      "\t528 words, 666 morphemes\n",
      "551108 A Painting of the King Of Iran\n",
      "\t763 words, 947 morphemes\n",
      "551109 The Adventures of Two Brothers\n",
      "\t2131 words, 2541 morphemes\n",
      "551110 The Adventures of a Princess\n",
      "\t1887 words, 2209 morphemes\n",
      "551111 Two Wicked Daughters-in-law\n",
      "\t689 words, 788 morphemes\n",
      "551112 A Dutiful Son\n",
      "\t1065 words, 1204 morphemes\n",
      "551113 The Little Prince and the Snake\n",
      "\t243 words, 284 morphemes\n",
      "551114 The Snake’s Dilemma\n",
      "\t1024 words, 1345 morphemes\n",
      "551115 The Wise Brother\n",
      "\t1619 words, 1991 morphemes\n",
      "551116 The Man who Wanted to Complain to God\n",
      "\t513 words, 614 morphemes\n",
      "551117 The Giant One-Eyed Demon\n",
      "\t390 words, 495 morphemes\n",
      "551118 The Cow and The Poor Girl\n",
      "\t578 words, 671 morphemes\n",
      "551119 A Frog Wants a Husband\n",
      "\t460 words, 554 morphemes\n",
      "551120 The Bird and the Fox\n",
      "\t233 words, 259 morphemes\n",
      "551121 The Old Man and the Fish\n",
      "\t543 words, 630 morphemes\n",
      "551122 Two Birds Fall in Love\n",
      "\t412 words, 485 morphemes\n",
      "551123 Star-Crossed Lovers\n",
      "\t294 words, 353 morphemes\n",
      "551124 The Assyrians of Urmi\n",
      "\t2268 words, 2768 morphemes\n",
      "551125 Village Life\n",
      "\t1064 words, 1372 morphemes\n",
      "551126 Agriculture and Village Life\n",
      "\t2028 words, 2487 morphemes\n",
      "551127 Hunting\n",
      "\t938 words, 1139 morphemes\n",
      "551128 Weddings and Festivals\n",
      "\t727 words, 879 morphemes\n",
      "551129 Events in 1946 on the Urmi Plain\n",
      "\t509 words, 639 morphemes\n",
      "551130 Village Life\n",
      "\t1263 words, 1614 morphemes\n",
      "551131 Weddings\n",
      "\t425 words, 534 morphemes\n",
      "551132 Games\n",
      "\t793 words, 1010 morphemes\n",
      "551133 Village Life\n",
      "\t1796 words, 2226 morphemes\n",
      "551134 St. Zayya’s Cake Dough\n",
      "\t554 words, 720 morphemes\n",
      "551135 Nipuxta\n",
      "\t366 words, 507 morphemes\n",
      "551136 Vineyards\n",
      "\t156 words, 199 morphemes\n",
      "551137 Village Life\n",
      "\t604 words, 812 morphemes\n",
      "551138 Village Life\n",
      "\t584 words, 684 morphemes\n",
      "551139 The Assyrians of Armenia\n",
      "\t712 words, 786 morphemes\n",
      "551140 Village Life\n",
      "\t2851 words, 3540 morphemes\n"
     ]
    }
   ],
   "source": [
    "print('books and their word counts: \\n')\n",
    "for text in F.otype.s('text'):\n",
    "    text_words = L.d(text, 'word')\n",
    "    text_morphemes = L.d(text, 'morpheme')\n",
    "    print(text, F.title.v(text))\n",
    "    print(f'\\t{len(text_words)} words, {len(text_morphemes)} morphemes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = F.otype.s('text')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L.d(text, 'word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Monk And The Angel\n",
      "551606 ʾìθwaˈ xa-ràbbən,ˈ tíwɛwa gu-xa-gəppìθa.ˈ θéle xa-náša swarìya,ˈ rakáwa.ˈ ṣléle rəš-xa-ʾɛ̀na.ˈ tìwle,ˈ xílle mə̀ndi,ˈ štéle mìya.ˈ ʾíθwale xákma zùze.ˈ qímle šqilìle.ˈ muttìleˈ rəš-d-ɛ-ʾɛ̀na.ˈ ʾàwwaˈ munšìle zúze díye.ˈ zìlle.ˈ ʾáwwa zílle b-ʾùrxa.ˈ\n",
      "551607 θéle xá rakáwa xèna,ˈ swarìya.ˈ zílle rəš-ʾɛ̀na.ˈ qəm-xazèlaˈ ʾə̀mma dináre.ˈ šqilíle jal-jàldeˈ muttíle gu-jɛ̀beˈ ʾu-zìlle.ˈ ʾo-qamàyaˈ ʾámər ʾòhˈ zúzi munšìli.ˈ qɛ́mən dɛ̀ṛənˈ ʾázən šáqlən zùziˈ m-rəš-ʾɛ̀na.ˈ\n",
      "551608 ha-t-ʾáθe ʾo-náša qamàyaˈ máṭe l-ʾɛ̀naˈ ʾáθe xa-náša sàwa.ˈ máṭe rəš-ʾɛ̀naˈ mattúla kàrteˈ ʾu-tíwle manyòxe.ˈ ʾo-qamáya ṱ-íle zúze mùnšyaˈ θéle ʾə́lle dìye.ˈ ʾàmərˈ mpáləṭla zùzi!,ˈ ʾə́mma dináre ʾána hon-mùnšəlla láxxa.ˈ lázəm yawə̀tla.ˈ yába lán-xəzya zùze,ˈ lá ʾáxxa-w tàmmaˈ ʾu-kízle b-ay-gòta.ˈ là.ˈ\n",
      "551609 mə́re qaṭlə̀nnux.ˈ mə́re qṭùl!ˈ lìtli.ˈ zúze làn-xəzya.ˈ qìmleˈ qəm-qaṭə̀lle.ˈ qəm-qaṭə̀lle.ˈ ràbbənˈ yăðət-mà-yle?ˈ ràbbənˈ ʾáwwa ṱ-i-sàxəð l-ʾálahaˈ ʾu-ṱ-i-mṣàle-uˈ lé-y-ʾaxəl bə̀sra-wˈ ʾáw y-amríle ràbbən.ˈ hóle tíwa gu-xa-gəppìθaˈ ʾarbì-šənne.ˈ\n",
      "551610 qìmɛleˈ mə́re mádam hàtxɛlaˈ ṱ-ázən ṭắyən báθər ḥaqqùθaˈ ʾu-na-ḥaqqùθa.ˈ ʾɛ̀nila ḥaqqúθaˈ ʾu-ʾɛ̀nila na-ḥaqqúθa.ˈ mára ṣə̀lyɛleˈ ta-t-ʾázəl ʾùrxa.ˈ ʾálaha mšúdrəlle malàxa.ˈ\n",
      "551611 mə́re ṣlí qamθə-d-áwwa nàša,ˈ ràbbən,ˈ t-là-xaləṭ.ˈ ṣə́lyɛle qámθe dìye.ˈ mə̀reˈ ha-gàni,ˈ lɛ̀kət zála?ˈ mə̀reˈ hon-zála ṭắya báθər ḥaqqúθa-w na-ḥaqqùθa.ˈ ʾámər ʾap-ʾàna hówən zála ṭắya báθər ḥaqqúθa ʾu-na-ḥaqqùθa.ˈ\n",
      "551612 mə̀re,ˈ maláxa mə́re ṭla-ràbbən,ˈ mə́re ṱ-áwðət b-xàbriˈ kú-məndi ṱ-amrə̀nnux?ˈ ʾámər hè.ˈ mə́re ma-yxàləf.ˈ mə́re ʾána w-áti xonăwàθəx.ˈ zìlela,ˈ zíle, zíle, zíle gu-ðà-maθa.ˈ ʾaṣə̀rtɛla,ˈ b-áyya dána hàtxa,ˈ mə́xyela l-tắrət bɛ́θət xa-nàša.ˈ wìrela gu-bɛ́θa.ˈ\n",
      "551613 šlàma-llɛxu!ˈ b-šɛ́na b-ṭawàθa,ˈ páqðu tìwe!ˈ tíwela tàmaˈ ʾu-xílela ʾixàlaˈ ʾu-píše dmìxe.ˈ dmìxela,ˈ b-lɛ̀le,ˈ maláxa mŭṛə̀šleˈqa-ràbbənˈ mə́re qù!ˈ qù!ˈ mə́re há də-šúqlən dàmxəx.ˈ lɛ́t-mira ṱ-óðən b-xàbrux?ˈ qu-šqúlla ʾáyya skìnta.ˈ si-prúmle ʾáwwa yála zòraˈ ṱ-íle gu-dudìya.ˈ\n",
      "551614 ʾaw-mə̀reˈ dáx pɛrmə́nne ʾàwwa?ˈ mút ḥaqqúθa na-ḥaqqùθɛla?ˈ mə́re nə́mu lɛ́t-mira ṱ-óðən b-xàbrux?ˈ ʾàyyɛla.ˈ bắyət bằyət.ˈ la-bắyət si-prúmle yàla.ˈ bába-w yə́mmət yàlaˈ xamši-šə́nne ʾíθwala ʾùmraˈ líθwala bnòne.ˈ ʾáwwa yála yíwəlle ʾálaha ʾə́lla díya ʾəštà yárxe.ˈ\n",
      "551615 ʾu-ʾáp-ʾawwa qəm-parmìle.ˈ ʾu-zìlla.ˈ b-lɛ̀le,ˈ zìlla,ˈ síqla xa-máθa xèta.ˈ mə́re ʾáyya mút ḥaqqúθa na-ḥaqqùθɛla,ˈ ya-ʾaxòni?ˈ hátxa măləpə́tli ḥaqqúθa ʾu-na-ḥaqqùθa.ˈ zílela gu-ða-máθa xèta.ˈ màra,ˈ ʾímət sìqlaˈ gu-d-ɛ̀-maθa,ˈ wírra gu-xa-bɛ̀θa.ˈ\n"
     ]
    }
   ],
   "source": [
    "print(F.title.v(text))\n",
    "\n",
    "for sent in F.otype.s('line')[:10]:\n",
    "    print(sent, T.text(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631711 ʾuʾápʾawwa\n",
      "631712 qəmparmìle\n",
      "631713 ʾuzìlla\n",
      "631714 blɛ̀le\n",
      "631715 zìlla\n",
      "631716 síqla\n",
      "631717 xamáθa\n",
      "631718 xèta\n",
      "631719 mə́re\n",
      "631720 ʾáyya\n",
      "631721 mút\n",
      "631722 ḥaqqúθa\n",
      "631723 naḥaqqùθɛla\n",
      "631724 yaʾaxòni\n",
      "631725 hátxa\n",
      "631726 măləpə́tli\n",
      "631727 ḥaqqúθa\n",
      "631728 ʾunaḥaqqùθa\n",
      "631729 zílela\n",
      "631730 guðamáθa\n",
      "631731 xèta\n",
      "631732 màra\n",
      "631733 ʾímət\n",
      "631734 sìqla\n",
      "631735 gudɛ̀maθa\n",
      "631736 wírra\n",
      "631737 guxabɛ̀θa\n"
     ]
    }
   ],
   "source": [
    "for w in L.d(551615, 'word'):\n",
    "    print(w, F.utf8.v(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Query Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = F.utf8.v(631728)\n",
    "\n",
    "find = list(S.search(f'''\n",
    "\n",
    "word utf8={w}\n",
    "\n",
    "'''))\n",
    "\n",
    "len(find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(631728,), (631609,), (631571,)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
