{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Corpus\n",
    "\n",
    "In this notebook, we experiment with producing a TF resource for the Christian Urmi NENA dialect. The text itself comes from Geoffrey Khan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, HTML\n",
    "from tf.fabric import Fabric\n",
    "with open('christian_urmi.txt', 'r') as infile:\n",
    "    urmi = infile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Text-Fabric Resource\n",
    "\n",
    "Text-Fabric is a format and tool for the storage, annotation, and analysis of text corpora. The Text-Fabric data model is explained in depth [in its docs](https://annotation.github.io/text-fabric/Model/Data-Model/).\n",
    "\n",
    "Herein we follow a fairly standard approach to convert a plain-text file into a TF resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Up Node Feature and Oslot Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateKey(dictionary):\n",
    "    '''\n",
    "    Auto increments a key from a dictionary.\n",
    "    '''\n",
    "    return max(dictionary.keys(), default=0)+1\n",
    "\n",
    "\n",
    "def cleanToken(token):\n",
    "    '''\n",
    "    Dealing with encoding variances.\n",
    "    First issue deals with a and its accent.\n",
    "    '''\n",
    "    return token.replace(chr(97)+chr(769), chr(225))\n",
    "\n",
    "raw_node_features = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "raw_oslots = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "slot = 0\n",
    "\n",
    "this_sentence = 1 # for first iteration since only sentence ends are marked\n",
    "\n",
    "for line in urmi.split('\\n'):\n",
    "    \n",
    "    # mark book beginnings, their \"code\" and title\n",
    "    if line.startswith('# '): # book code\n",
    "        this_book = iterateKey(raw_oslots['book'])\n",
    "        raw_node_features['book_code'][this_book] = line.split()[-1].strip()\n",
    "        continue\n",
    "    elif line.startswith('## '): # book title\n",
    "        raw_node_features['book_title'][this_book] = line.split('#')[-1]\n",
    "        continue\n",
    "                \n",
    "    # map slots to objects and features:\n",
    "    for token in line.split():\n",
    "        \n",
    "        if re.match('.*\\(\\d*\\)', token): # line start\n",
    "            this_line = iterateKey(raw_oslots['line'])\n",
    "            raw_node_features['line'][this_line] = token\n",
    "            continue\n",
    "            \n",
    "        # everything up to this point is a valid slot\n",
    "        # iterate slot up by 1\n",
    "        slot += 1\n",
    "            \n",
    "        # record sentence boundaries\n",
    "        if re.match('.*\\.\\|', token): # end of sentence\n",
    "            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "            this_sentence = iterateKey(raw_oslots['sentence']) # get incremented, new sentence ID\n",
    "        else: # beginning/within sentence\n",
    "            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "            \n",
    "        raw_node_features['trans'][slot] = cleanToken(token)\n",
    "        raw_node_features['trailer'][slot] = ' '\n",
    "        raw_oslots['book'][this_book].add(slot)\n",
    "        raw_oslots['line'][this_line].add(slot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex Objects Above Slot Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "otype2feature = {'book':{'book_code', 'book_title'},\n",
    "                 'line':{'line'},\n",
    "                 'sentence':{}}\n",
    "\n",
    "node_features = collections.defaultdict(lambda:collections.defaultdict())\n",
    "node_features['trans'] = raw_node_features['trans'] # add slot features\n",
    "node_features['trailer'] = raw_node_features['trailer']\n",
    "for slot in node_features['trans']:\n",
    "    node_features['otype'][slot] = 'word'\n",
    "    \n",
    "edge_features = collections.defaultdict(lambda:collections.defaultdict(set)) # oslots will go here\n",
    "\n",
    "onode = max(raw_node_features['trans']) # max slot, incremented +1 in loop\n",
    "\n",
    "for otype in raw_oslots.keys():\n",
    "    for oID, slots in raw_oslots[otype].items():\n",
    "        \n",
    "        # make new object node number\n",
    "        onode += 1\n",
    "        node_features['otype'][onode] = otype\n",
    "        \n",
    "        # remap node features to node number\n",
    "        for feat in otype2feature[otype]:\n",
    "            node_features[feat][onode] = raw_node_features[feat][oID]\n",
    "        edge_features['oslots'][onode] = raw_oslots[otype][oID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trans', 'trailer', 'otype', 'book_title', 'book_code', 'line'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oslots'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to TF Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.11\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "9 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "otext = {\n",
    "'sectionTypes': 'book,line',\n",
    "'sectionFeatures':'book_code,line',\n",
    "'fmt:text-orig-full':'{trans}{trailer}'\n",
    "}\n",
    "\n",
    "meta = {'':{'author': 'Geoffrey Khan and Cody Kingham'},\n",
    "        'oslots':{'edgeValues':False, 'valueType':'int'},\n",
    "        'otype':{'valueType':'str'},\n",
    "        'book':{'valueType':'str'},\n",
    "        'line':{'valueType':'str'},\n",
    "        'trans':{'valueType':'str'},\n",
    "        'book_code':{'valueType':'str'},\n",
    "        'book_title':{'valueType':'str'},\n",
    "        'trailer':{'valueType':'str'},\n",
    "        'otext':otext}\n",
    "\n",
    "TFs = Fabric(locations=['tf/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 6 node and 1 edge and 2 config features to tf/:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.01s maxSlot=       2217\n",
      "  0.01s maxNode=       2595\n",
      "  0.01s OK: oslots is valid\n",
      "   |     0.00s T book_code            to tf\n",
      "   |     0.00s T book_title           to tf\n",
      "   |     0.00s T line                 to tf\n",
      "   |     0.00s T otype                to tf\n",
      "   |     0.01s T trailer              to tf\n",
      "   |     0.01s T trans                to tf\n",
      "   |     0.00s T oslots               to tf\n",
      "   |     0.00s M book                 to tf\n",
      "   |     0.00s M otext                to tf\n",
      "  0.04s Exported 6 node features and 1 edge features and 2 config features to tf/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFs.save(nodeFeatures=node_features, edgeFeatures=edge_features, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nena Corpus Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "import collections\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.11\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "9 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s T otype                from tf\n",
      "   |     0.00s T book_code            from tf\n",
      "   |     0.01s T trans                from tf\n",
      "   |      |     0.00s C __levels__           from otype, oslots, otext\n",
      "   |      |     0.02s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.00s C __rank__             from otype, __order__\n",
      "   |      |     0.01s C __levUp__            from otype, oslots, __levels__, __rank__\n",
      "   |      |     0.00s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     0.00s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.00s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book_code, line\n",
      "   |     0.00s T book_title           from tf\n",
      "  0.09s All features loaded/computed - for details use loadLog()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='tf/')\n",
    "\n",
    "N = TF.load('''\n",
    "\n",
    "book_code trans otype book_title\n",
    "\n",
    "''')\n",
    "\n",
    "N.makeAvailableIn(globals())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2217 words in the corpus\n"
     ]
    }
   ],
   "source": [
    "print(len(list(F.otype.s('word'))), 'words in the corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books and their word counts: \n",
      "\n",
      "2218  The Loan of a Cooking Pot (Yulia Davudi, +Hassar +Baba-čanɟa, N)\n",
      "\t180 words\n",
      "2219  Agriculture and Village Life (Natan Khoshaba, Zumallan, N)\n",
      "\t2037 words\n"
     ]
    }
   ],
   "source": [
    "print('books and their word counts: \\n')\n",
    "for book in F.otype.s('book'):\n",
    "    book_words = L.d(book, 'word')\n",
    "    print(book, F.book_title.v(book))\n",
    "    print(f'\\t{len(book_words)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+xárta</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ʾá</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>k̭át</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ʾíta</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xá</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>+rába</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ʾátxa</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cúllǝ</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>+k̭usárta</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>xína</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ʾìta|</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>míyya</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ʾánnǝ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dástə</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>+bár</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cùllǝ|</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bí</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hál</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tré</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ʾíta</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>là</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c-avíva</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ʾǝ́tvalan</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lè</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ʾína</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  count\n",
       "0      +xárta     30\n",
       "1          ʾá     30\n",
       "2        k̭át     22\n",
       "3       ʾíta     18\n",
       "4          xá     16\n",
       "5       +rába     15\n",
       "6       ʾátxa     14\n",
       "7      cúllǝ     11\n",
       "8   +k̭usárta     10\n",
       "9       xína     10\n",
       "10     ʾìta|      9\n",
       "11     míyya      9\n",
       "12      ʾánnǝ      9\n",
       "13      dástə      8\n",
       "14       +bár      8\n",
       "15    cùllǝ|      8\n",
       "16        bí      8\n",
       "17        hál      8\n",
       "18       tré      7\n",
       "19       ʾíta      7\n",
       "20        là      7\n",
       "21   c-avíva      7\n",
       "22  ʾǝ́tvalan      7\n",
       "23        lè      7\n",
       "24      ʾína      6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = collections.Counter()\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    tokens[F.trans.v(w)] += 1\n",
    "    \n",
    "tokens = pd.DataFrame(tokens.most_common(25))\n",
    "tokens.columns = ['token', 'count']\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence examples: \n",
      "\n",
      "2266 xá yuma +málla +Nasràdən| bərrə̀xšələ| bəšk̭álələ +k̭usárta déna mən švàvu.| \n",
      "2267 màrǝlə| hálli xá dana +k̭usàrta| +báyyən bášlən ɟávo bušàla.| \n",
      "2268 +k̭usárta +ɟúrta lə̀tli.| \n",
      "2269 bəšk̭álolə màyolə +k̭usárta| bušála bašùlələ,| labùlolə,| yávolə mə̀drə| k̭à| švàva.| \n",
      "2270 ʾína tré +k̭usaryay sùrə| mattúyəl ɟàvo.| \n",
      "2271 švàva| màrǝlə| ʾáha tré +k̭usaryàtə| k̭àm muyyévət?| mə̀rrə| +k̭usártət dìyyux| də̀lla| tré xínə mə̀nno.| \n",
      "2272 yávəl k̭àtu| ʾávət basìma,| bitàyələ.| \n",
      "2273 ʾé-šabta xìta| +málla +Nasrádən bərrə́xšəl mə̀drə.| \n",
      "2274 màrələ| +maxlèta,| xa +k̭usárta buš +ɟùrta +byáyəvən.| \n",
      "2275 +málla +Nasràdən| +ʾáynu pə́ltəva +ʾal-xa +k̭usartət švàvə.| \n"
     ]
    }
   ],
   "source": [
    "print('sentence examples: \\n')\n",
    "for sent in list(F.otype.s('sentence'))[:10]:\n",
    "    print(sent, T.text(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Search Capacity\n",
    "\n",
    "Find words that follow the most common token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "find = list(S.search(f'''\n",
    "\n",
    "sentence\n",
    "    word trans=+xárta\n",
    "    <: word\n",
    "\n",
    "'''))\n",
    "\n",
    "print(len(find))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+xárta  pā̀n \n",
      "+xárta  ʾǝ́tva \n",
      "+xárta  ci-+yasrìvalun.| \n",
      "+xárta  ɟəddàla| \n",
      "+xárta  +mač̭ràxvalǝ.| \n",
      "+xárta  cùllǝ| \n",
      "+xárta  +marč̭ìvalǝ| \n",
      "+xárta  púmmu \n",
      "+xárta  +marč̭ìvalun.| \n",
      "+xárta  zìla \n",
      "+xárta  ʾé \n",
      "+xárta  pardùvvǝ \n",
      "+xárta  maštàxla| \n",
      "+xárta  pummé \n",
      "+xárta  k̭át \n",
      "+xárta  +xazdàxvala| \n",
      "+xárta  bí \n",
      "+xárta  mǝn-dàha| \n",
      "+xárta  ʾánnǝ \n",
      "+xárta  ɟári \n",
      "+xárta  b-ràcxa,| \n",
      "+xárta  +ʾànvǝ \n",
      "+xárta  +xárta \n",
      "+xárta  nášǝ \n",
      "+xárta  ʾá \n",
      "+xárta  b-ptána \n",
      "+xárta  +ṱárpa \n",
      "+xárta  b-labláxla \n",
      "+xárta  +xàrta,| \n",
      "+xárta  b-šatxìvalun.| \n"
     ]
    }
   ],
   "source": [
    "for res in find:\n",
    "    print(T.text(res[1]), T.text(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Searching\n",
    "\n",
    "It looks like the ending `un` is could be a plural verb ending? Here is a query for those endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 results\n",
      "\n",
      "c-avǝ́dvalun \n",
      "tílun \n",
      "tùttun.| \n",
      "túttun \n",
      "túttun \n",
      "ʾax-šatxáxvalun \n",
      "+rappívalun \n",
      "ci-yavvàvalun.| \n",
      "ci-+yasrìvalun.| \n",
      "ci-+pašṱìvalun,| \n",
      "ɟaršìvalun,| \n",
      "túttun \n",
      "c-odívalun.| \n",
      "ʾǝ́tvalun.| \n",
      "mabrǝzzìvalun,| \n",
      "ṱ-+axlìvalun.| \n",
      "c-odìvalun.| \n",
      "+mardǝxxívalun \n",
      "maštàxvalun.| \n",
      "+pallìvalun.| \n",
      "ʾǝ́tvalun.| \n",
      "mayyáxvalun \n",
      "+palṱìvalun.| \n",
      "b-šatxìvalun.| \n",
      "+pallìvalun.| \n",
      "+rappívalun \n",
      "šaṱxìvalun.| \n",
      "ci-+xalvìvalun.| \n",
      "xǝ́šlun \n",
      "muyyílun \n",
      "+ṱrǝ̀plun,| \n",
      "+ṱripàlun,| \n",
      "mattáxlun \n",
      "lablívalun \n",
      "šk̭ǝ́llun \n",
      "+zrílun.| \n",
      "+jammáxvalun \n",
      "ɟabìvalun,| \n",
      "banìvalun.| \n",
      "šaṱxìvalun,| \n",
      "b-lablàxvalun.| \n",
      "+marč̭ìvalun.| \n",
      "tìlun,| \n",
      "zǝ̀dlun.| \n",
      "ci-banívalun.| \n",
      "ci-malívalun \n",
      "túttun \n",
      "mattáxvalun \n",
      "+daràxlun.| \n",
      "ci-tanáxvalun.| \n",
      "túttun \n",
      "b-+jammáxvalun \n",
      "+dávun \n",
      "+ʾàvun,| \n",
      "ci-mayyàxvalun.| \n",
      "tùttun.| \n",
      "+xalvìvalun.| \n",
      "+ṱamšáxvalun \n",
      "+ʾávun \n",
      "túttun \n"
     ]
    }
   ],
   "source": [
    "suffix = list(S.search('''\n",
    "\n",
    "word trans~un\\.|un$|un,\n",
    "\n",
    "'''))\n",
    "\n",
    "print(len(suffix), 'results\\n')\n",
    "\n",
    "for res in suffix:\n",
    "    print(T.text(res[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
