{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Text-Fabric Resource from Source Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing for Christian Urmi and Barwar texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with the language data in the NENA corpus, it is important to separate the language text from other data, such as titles, authors/informants, and verse numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of the text comes in MS-Word document files. That is probably also the version of the text that is the richest in language data. It contains not only the text itself, but also meaningful formatting, e.g. word markers set in superscript, and foreign (loan) words set in roman type (where the regular text is set in italic type)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert that information from a Word document to something that we can use in Python, we first convert the word documents to HTML, using LibreOffice in headless mode. It is assumed that the Word files are in the subdirectory `texts`, where the converted `.html` files will also be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    $ soffice --headless --convert-to html texts/*.doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces HTML 4.0 documents in the same directory. Earlier attempts with XHTML using wvWare/AbiWord, or LibreOffice using the XHTML conversion filter, produced output that was more difficult to parse or lacked certain characters that were lost in conversion. Although the conversion with LibreOffice takes a very long time compared with AbiWord, the resulting text seems more reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The custom `nena_corpus` package contains the `Text` class, and several functions that assist in the conversion from HTML, of which we only need the functions `html_to_text()` and `parse_metadata()`.\n",
    "\n",
    "The function `html_to_text()` is a generator function yielding `Text` objects, each containing one paragraph of text.\n",
    "\n",
    "The function `parse_metadata()` extracts metadata from heading paragraphs (e.g. `title`, `text_id`, `informant`, `place`).\n",
    "\n",
    "The `Text` class contains an attribute `type` describing the type of paragraph (e.g., `'sectionheading'`, `'p'`, or `'footnote'`), and a list of tuples, containing the text and text style. A text like `'<i>Normal, </i>foreign<i>, and normal</i>'` becomes `[('Normal, ', ''), ('cursive,', 'italic'), (' and normal', '')]` (note the inversion -- because normal text in the source is actually set in italics).\n",
    "\n",
    "`Text` objects are iterable. New items can be appended with the `append(text, text_style)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nena_corpus import Text, html_to_text, parse_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demonstration of the `Text` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dit is een test.\n",
      "<Text 'test' 'Dit is een test.'>\n",
      "[('Dit is ', 'normal'), ('een test', 'test'), ('.', 'normal')]\n",
      "[('Dit is ', 'normal'), ('een test', 'test'), ('.', 'normal')]\n"
     ]
    }
   ],
   "source": [
    "p = Text(p_type='test', default_style='normal')\n",
    "\n",
    "p.append('Dit is ')\n",
    "p.append('een test', 'test')\n",
    "p.append('.')\n",
    "\n",
    "# str(p) returns concatenated string\n",
    "print(p)\n",
    "# repr(p) returns class name, p_type and str(p)\n",
    "print(repr(p))\n",
    "# list(p) returns the list of tuples\n",
    "print(list(p))\n",
    "# a list comprehension also works\n",
    "print([e for e in p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some more useful libraries, and set logging level to DEBUG to make sure we see all logging messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import pathlib\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tf.fabric import Fabric\n",
    "import pandas as pd\n",
    "\n",
    "logging.getLogger().setLevel(logging.DEBUG) # for terminal messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the subdirectory `texts` contains the HTML files generated earlier, we can import all files in the pattern `texts/*.html`. At this point we just want to do language statistics and not look at the actual texts, so it is sufficient to import the paragraphs of all texts in no particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_barwar = pathlib.Path.cwd().glob('texts/bar text *.html') # get source texts\n",
    "files_urmi_c = pathlib.Path.cwd().glob('texts/cu *.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also prepare a dictionary with some characters that need to be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characters to be replaced\n",
    "replace = {\n",
    "    '\\u2011': '\\u002d',  # U+2011 NON-BREAKING HYPHEN -> U+002D HYPHEN-MINUS\n",
    "    '\\u01dd': '\\u0259',  # U+01DD LATIN SMALL LETTER TURNED E -> U+0259 LATIN SMALL LETTER SCHWA\n",
    "    '\\uf1ea': '\\u003d',  # U+F1EA Deprecated SIL character -> U+003D '=' EQUALS SIGN\n",
    "    '\\u2026': '...',  # U+2026 '…' HORIZONTAL ELLIPSIS -> three dots\n",
    "    'J\\u0335': '\\u0248',  # 'J' + U+0335 COMBINING SHORT STROKE OVERLAY -> U+0248 'Ɉ' LATIN CAPITAL LETTER J WITH STROKE\n",
    "    'J\\u0336': '\\u0248',  # 'J' + U+0336 COMBINING LONG STROKE OVERLAY -> U+0248 'Ɉ' LATIN CAPITAL LETTER J WITH STROKE\n",
    "    '\\u002d\\u032d': '\\u032d\\u002d',  # Switch positions of Hyphen and Circumflex accent below\n",
    "    '\\u2011\\u032d': '\\u032d\\u002d',  # Switch positions of Non-breaking hyphen and Circumflex accent below\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go right ahead to loop over the html files and convert them to a TextFabric structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing file bar text a1-A7.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote1'.\n",
      "DEBUG:root:Text: ' 1 The name Čuxo means ‘one who wears the woolen čuxa garment’. '.\n",
      "INFO:root:Processing file bar text A14.html ...\n",
      "INFO:root:Processing file bar text a15-A17.html ...\n",
      "INFO:root:Processing file bar text a18.html ...\n",
      "INFO:root:Processing file bar text a19-A23.html ...\n",
      "INFO:root:Processing file bar text a24.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 7 '.\n",
      "INFO:root:Processing file bar text a25.html ...\n",
      "INFO:root:Processing file bar text a26.html ...\n",
      "INFO:root:Processing file bar text a27.html ...\n",
      "INFO:root:Processing file bar text a28.html ...\n",
      "INFO:root:Processing file bar text a29.html ...\n",
      "INFO:root:Processing file bar text a30.html ...\n",
      "INFO:root:Processing file bar text a31-A33.html ...\n",
      "INFO:root:Processing file bar text a34.html ...\n",
      "INFO:root:Processing file bar text a35.html ...\n",
      "INFO:root:Processing file bar text a36.html ...\n",
      "INFO:root:Processing file bar text A37-A40.html ...\n",
      "INFO:root:Processing file bar text a41.html ...\n",
      "INFO:root:Processing file bar text A42-A44.html ...\n",
      "INFO:root:Processing file bar text A45.html ...\n",
      "INFO:root:Processing file bar text a46-A47.html ...\n",
      "INFO:root:Processing file bar text a48.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 1 '.\n",
      "INFO:root:Processing file bar text A49.html ...\n",
      "INFO:root:Processing file bar text a50-A52.html ...\n",
      "INFO:root:Processing file bar text a8.html ...\n",
      "INFO:root:Processing file bar text A9-A13.html ...\n",
      "INFO:root:Processing file cu vol 4 texts.html ...\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "WARNING:root:Unfinished marker: 'R', closed forcibly..\n",
      "DEBUG:root:Urmi_C, A43:17\n",
      "DEBUG:root:Text: ' várdə=da mattúvvəla k̭am-bràto.| ʾáxči cálu labùlola.| +p̂urmìlux k̭a-díyyi?| lublàlun,| lublàlun,| lublàlun,| lublàlun.|'\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'gp-sectionheading-western'.\n",
      "DEBUG:root:Text: ' '.\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote1'.\n",
      "DEBUG:root:Text: ' 1 In the original recording of the story the speaker used the word camra ‘animal droppings’ here, but subsequently corrected this to calla. '.\n",
      "DEBUG:root:Unhandled paragraph type: 'sdfootnote2'.\n",
      "DEBUG:root:Text: ' 2 Mistake for šənnə +xarayə. '.\n",
      "DEBUG:root:Unhandled paragraph type: 'footer'.\n",
      "DEBUG:root:Text: ' 189 '.\n"
     ]
    }
   ],
   "source": [
    "def combine_chars(text):\n",
    "    \"\"\"Yield letters combined with combining diacritics\"\"\"\n",
    "    \n",
    "    char = []\n",
    "    \n",
    "    for c in text:\n",
    "        if unicodedata.category(c) == 'Mn':  # 'Mn': non-spacing combining mark\n",
    "            char.append(c)\n",
    "            continue\n",
    "        \n",
    "        if char:\n",
    "            yield ''.join(char)\n",
    "        char = [c]\n",
    "        \n",
    "    yield ''.join(char)\n",
    "\n",
    "raw_node_features = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "raw_oslots = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "\n",
    "# initialize counters (will be increased to start from 1)\n",
    "this_text = 0\n",
    "this_paragraph = 0\n",
    "this_line = 0\n",
    "this_sentence = 0\n",
    "this_subsentence = 0\n",
    "this_word = 0\n",
    "this_morpheme = 0\n",
    "this_foreign = 0\n",
    "this_prosa = 0\n",
    "\n",
    "slot = 0\n",
    "\n",
    "for dialect, files in (('Barwar', files_barwar), ('Urmi_C', files_urmi_c)):\n",
    "    \n",
    "    # TODO At this point record book/publication/dialect?\n",
    "    # E.g. SSLL_2016_Urmi_C, HOS_2008_Barwar?\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        logging.info(f'Processing file {file.name} ...')\n",
    "        \n",
    "        for p in html_to_text(file, replace=replace):\n",
    "            # metadata:\n",
    "            # - dialect\n",
    "            # - file.name\n",
    "            \n",
    "            if p.type.startswith('gp-') and str(p).strip():\n",
    "                # store metadata from headings:\n",
    "                # - text_id\n",
    "                # - title\n",
    "                # - informant\n",
    "                # - place\n",
    "                # - version (if applicable -- only Urmi_C A35)\n",
    "                if p.type.startswith('gp-sectionheading'):\n",
    "                    metadata = {}\n",
    "                for k, v in parse_metadata(p):\n",
    "                    metadata[k] = v\n",
    "            #\n",
    "            elif p.type == 'p':\n",
    "                # regular paragraphs\n",
    "                \n",
    "                # first check if we need to update metadata\n",
    "                # TODO for now we do not store informant, place, and version,\n",
    "                # since those are not always features of a text, but of a section\n",
    "                # of the text, and I do not know how to do that.\n",
    "                # QUESTION -- do we need to add a layer 'subsection'?\n",
    "                if (metadata\n",
    "                    and (not raw_node_features['text_id']\n",
    "                         or raw_node_features['text_id'][this_text] != metadata['text_id'])):\n",
    "                    this_text += 1\n",
    "                    raw_node_features['text_id'][this_text] = metadata['text_id']\n",
    "                    raw_node_features['title'][this_text] = metadata['title']\n",
    "                    raw_node_features['dialect'][this_text] = dialect\n",
    "                    raw_node_features['filename'][this_text] = file.name\n",
    "                \n",
    "                # increment paragraph\n",
    "                this_paragraph += 1\n",
    "                \n",
    "                # start paragraph with an empty marker stack\n",
    "                marker_stack = []\n",
    "                \n",
    "                # set end-of-unit markers to True at the beginning of paragraph,\n",
    "                # so the units can be increased on encounter of first word character\n",
    "                sentence_end = True\n",
    "                subsentence_end = True\n",
    "                word_end = True\n",
    "                morpheme_end = True\n",
    "                foreign_end = True\n",
    "                prosa_end = True\n",
    "                \n",
    "                for text, text_style in p:\n",
    "                    \n",
    "                    if text_style == 'verse_no':\n",
    "                        this_line += 1\n",
    "                        raw_node_features['line'][this_line] = text.strip(' ()') # TODO int()?\n",
    "                        metadata['verse_no'] = text.strip(' ()')  # TODO Remove from metadata dict?\n",
    "                        continue\n",
    "                        \n",
    "                    elif text_style == 'fn_anchor':\n",
    "                        # TODO handle footnotes in some way, discard for now\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style == 'comment':\n",
    "                        continue  # TODO handle comments\n",
    "                    \n",
    "                    elif text_style == 'marker':\n",
    "                        if marker_stack and marker_stack[-1] == text:\n",
    "                            marker_stack.pop()\n",
    "                        else:\n",
    "                            marker_stack.append(text)\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style not in ('', 'foreign'):\n",
    "                        logging.debug(f'Unhandled text_style: {repr(text_style)}, {repr(text)}')\n",
    "                        continue\n",
    "                    \n",
    "                    elif text_style == 'foreign'and foreign_end:\n",
    "                        foreign_end = False\n",
    "                        this_foreign += 1\n",
    "                        if marker_stack:\n",
    "                            language = marker_stack[-1]\n",
    "                        else:\n",
    "                            language = ''\n",
    "                        raw_node_features['language'][this_foreign] = language\n",
    "                    \n",
    "                    else: # text_style == '':\n",
    "                        if not foreign_end:\n",
    "                            foreign_end = True\n",
    "                        pass\n",
    "                    \n",
    "                    if (text_style == '' and marker_stack\n",
    "                        and any(c.isalpha() for c in text)\n",
    "                        and not text.isalpha()):\n",
    "                        # In one case, there is no closing marker tag, so force closing the marker\n",
    "                        # Urmi_C A42 9: 'RzdànyəlaR' (p.154, r.28) 'zdàny' roman, 'əla' cursive\n",
    "                        # Urmi_C A43 17: 'ʾe-Rbuk̭ḗṱ' (p. 174, r.14), no closing 'R'\n",
    "                        # Urmi_C B2 16: 'Pʾafšɑ̄rī̀P' (p.250 r.17), inital 'ʾ' cursive\n",
    "                        marker = marker_stack.pop()\n",
    "                        logging.warning(f'Unfinished marker: {repr(marker)}, closed forcibly..')\n",
    "                        logging.debug(f'{dialect}, {metadata[\"text_id\"]}:{metadata[\"verse_no\"]}')\n",
    "                        logging.debug(f'Text: {repr(text)}')\n",
    "                    \n",
    "                    # If we got this far, we have a text string,\n",
    "                    # with either text_style '' or 'foreign'.\n",
    "                    # We will iterate over them character by character.\n",
    "                    for c in combine_chars(text):\n",
    "                        \n",
    "                        if c[0].isalpha() or c == '+':\n",
    "                            \n",
    "                            # Increment text units on start of new word\n",
    "                            if morpheme_end:\n",
    "                                this_morpheme += 1\n",
    "                                morpheme_end = False\n",
    "                            if word_end:\n",
    "                                this_word += 1\n",
    "                                word_end = False\n",
    "                            if subsentence_end:\n",
    "                                this_subsentence += 1\n",
    "                                subsentence_end = False\n",
    "                            if sentence_end:\n",
    "                                this_sentence += 1\n",
    "                                sentence_end = False\n",
    "                            if prosa_end:\n",
    "                                this_prosa += 1\n",
    "                                prosa_end = False\n",
    "                            \n",
    "                            slot += 1\n",
    "                            raw_node_features['char'][slot] = c\n",
    "                            # initialize 'trailer' feature as empty string,\n",
    "                            # so we can add characters with '+' operator later\n",
    "                            raw_node_features['trailer'][slot] = ''\n",
    "                            \n",
    "                            raw_oslots['text'][this_text].add(slot)\n",
    "                            raw_oslots['paragraph'][this_paragraph].add(slot)\n",
    "                            raw_oslots['line'][this_line].add(slot)\n",
    "                            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "                            raw_oslots['subsentence'][this_subsentence].add(slot)\n",
    "                            raw_oslots['prosa'][this_prosa].add(slot)\n",
    "                            if not word_end:\n",
    "                                raw_oslots['word'][this_word].add(slot)\n",
    "                            if not morpheme_end:\n",
    "                                raw_oslots['morpheme'][this_morpheme].add(slot)\n",
    "                            if not foreign_end:\n",
    "                                raw_oslots['foreign'][this_foreign].add(slot)\n",
    "                        \n",
    "                        else:  # if c is anything but a letter or '+':\n",
    "                            if slot == 0:\n",
    "                                continue  # discard anything before first word character\n",
    "                            if not morpheme_end:\n",
    "                                morpheme_end = True\n",
    "                            if c == '|':\n",
    "                                prosa_end = True\n",
    "                                c = '\\u02c8'\n",
    "                            if c not in ('-', '=') and not word_end:\n",
    "                                word_end = True\n",
    "                            if c == ',' and not subsentence_end:\n",
    "                                subsentence_end = True\n",
    "                            if c in ('.', '!', '?') and not sentence_end:\n",
    "                                subsentence_end = True\n",
    "                                sentence_end = True\n",
    "                            \n",
    "                            raw_node_features['trailer'][slot] += c\n",
    "                \n",
    "            else:\n",
    "                logging.debug(f'Unhandled paragraph type: {repr(p.type)}.')\n",
    "                logging.debug(f'Text: {repr(str(p))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex Objects Above Slot Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "otype2feature = {\n",
    "    'text': {'text_id', 'title', 'dialect', 'filename'},\n",
    "    'paragraph': {},\n",
    "    'line': {'line'},\n",
    "    'sentence': {},\n",
    "    'subsentence': {},\n",
    "    'word': {'trailer'},\n",
    "    'morpheme': {},\n",
    "    'foreign': {'language'},\n",
    "    'prosa': {},\n",
    "}\n",
    "\n",
    "node_features = collections.defaultdict(lambda:collections.defaultdict())\n",
    "\n",
    "node_features['char'] = raw_node_features['char'] # add slot features\n",
    "node_features['trailer'] = raw_node_features['trailer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for slot in node_features['char']:\n",
    "    node_features['otype'][slot] = 'char'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_features = collections.defaultdict(lambda:collections.defaultdict(set)) # oslots will go here\n",
    "\n",
    "onode = max(raw_node_features['char']) # max slot, incremented +1 in loop\n",
    "\n",
    "for otype in raw_oslots.keys():\n",
    "    for oID, slots in raw_oslots[otype].items():\n",
    "        \n",
    "        # make new object node number\n",
    "        onode += 1\n",
    "        node_features['otype'][onode] = otype\n",
    "        \n",
    "        # remap node features to node number\n",
    "        for feat in otype2feature[otype]:\n",
    "            node_features[feat][onode] = raw_node_features[feat][oID]\n",
    "        edge_features['oslots'][onode] = raw_oslots[otype][oID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['char', 'trailer', 'otype', 'filename', 'text_id', 'title', 'dialect', 'line', 'language'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oslots'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "otext = {\n",
    "    'sectionTypes': 'text,paragraph,line,sentence',\n",
    "    'sectionFeatures': 'text_id,line',\n",
    "    'fmt:text-orig-full': '{char}{trailer}',\n",
    "    }\n",
    "\n",
    "meta = {'':{'author': 'Geoffrey Khan, Cody Kingham, and Hannes Vlaardingerbroek'},\n",
    "        'oslots':{'edgeValues':False, 'valueType':'int'},\n",
    "        'otype':{'valueType':'str'},\n",
    "        'text':{'valueType':'str'},\n",
    "        'paragraph':{'valueType':'str'},\n",
    "        'line':{'valueType':'str'},\n",
    "        'word':{'valueType':'str'},\n",
    "        'char':{'valueType':'str'},\n",
    "        'text_id':{'valueType':'str'},\n",
    "        'title':{'valueType':'str'},\n",
    "        'dialect':{'valueType':'str'},\n",
    "        'filename':{'valueType':'str'},\n",
    "        'language':{'valueType':'str'},\n",
    "        'trailer':{'valueType':'str'},\n",
    "        'otext':otext\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "14 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TFs = Fabric(locations=['tf/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 9 node and 1 edge and 4 config features to tf/:\n",
      "  0.01s VALIDATING oslots feature\n",
      "  0.18s VALIDATING oslots feature\n",
      "  0.19s maxSlot=     551014\n",
      "  0.19s maxNode=     846437\n",
      "  0.27s OK: oslots is valid\n",
      "   |     1.31s T char                 to tf\n",
      "   |     0.00s T dialect              to tf\n",
      "   |     0.00s T filename             to tf\n",
      "   |     0.01s T language             to tf\n",
      "   |     0.02s T line                 to tf\n",
      "   |     0.45s T otype                to tf\n",
      "   |     0.00s T text_id              to tf\n",
      "   |     0.00s T title                to tf\n",
      "   |     1.52s T trailer              to tf\n",
      "   |     2.01s T oslots               to tf\n",
      "   |     0.01s M otext                to tf\n",
      "   |     0.00s M paragraph            to tf\n",
      "   |     0.00s M text                 to tf\n",
      "   |     0.01s M word                 to tf\n",
      "  5.65s Exported 9 node features and 1 edge features and 4 config features to tf/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFs.save(nodeFeatures=node_features, edgeFeatures=edge_features, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load New TF Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.4\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "14 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='tf/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.47s T otype                from tf\n",
      "   |     7.95s T oslots               from tf\n",
      "   |     0.00s No section config in otext, the section part of the T-API cannot be used\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n",
      "   |     2.33s T char                 from tf\n",
      "   |     1.66s T trailer              from tf\n",
      "   |      |     0.43s C __levels__           from otype, oslots, otext\n",
      "   |      |       14s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.75s C __rank__             from otype, __order__\n",
      "   |      |       20s C __levUp__            from otype, oslots, __rank__\n",
      "   |      |     3.97s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     5.45s C __boundary__         from otype, oslots, __rank__\n",
      "   |     0.01s T text_id              from tf\n",
      "   |     0.03s T line                 from tf\n",
      "   |     0.01s T title                from tf\n",
      "   |     0.02s T dialect              from tf\n",
      "   |     0.03s T language             from tf\n",
      "    57s All features loaded/computed - for details use loadLog()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = TF.load('''\n",
    "\n",
    "text_id paragraph line word char otype title\n",
    "dialect language trailer\n",
    "\n",
    "''')\n",
    "\n",
    "N.makeAvailableIn(globals())\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "### Object Types and Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                       125\n",
      "paragraph                  465\n",
      "line                      2543\n",
      "sentence                 16784\n",
      "subsentence              24541\n",
      "prosa                    35967\n",
      "word                     93762\n",
      "foreign                   1102\n",
      "morpheme                120134\n",
      "char                    551014\n"
     ]
    }
   ],
   "source": [
    "for otype in F.otype.all:\n",
    "    print('{:20}{:>10}'.format(otype, len(list(F.otype.s(otype)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slot Gaps Between Words\n",
    "\n",
    "Note that there are gaps between word slots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʾána \n",
      "\t1 ʾ\n",
      "\t2 á\n",
      "\t3 n\n",
      "\t4 a \n",
      "ʾíwən \n",
      "\t5 ʾ\n",
      "\t6 í\n",
      "\t7 w\n",
      "\t8 ə\n",
      "\t9 n \n",
      "Yúwəl \n",
      "\t10 Y\n",
      "\t11 ú\n",
      "\t12 w\n",
      "\t13 ə\n",
      "\t14 l \n",
      "Yuḥànnaˈ \n",
      "\t15 Y\n",
      "\t16 u\n",
      "\t17 ḥ\n",
      "\t18 à\n",
      "\t19 n\n",
      "\t20 n\n",
      "\t21 aˈ \n",
      "ʾÌsḥaqˈ \n",
      "\t22 ʾ\n",
      "\t23 Ì\n",
      "\t24 s\n",
      "\t25 ḥ\n",
      "\t26 a\n",
      "\t27 qˈ \n",
      "t-máθət \n",
      "\t28 t-\n",
      "\t29 m\n",
      "\t30 á\n",
      "\t31 θ\n",
      "\t32 ə\n",
      "\t33 t \n",
      "Dùre,ˈ \n",
      "\t34 D\n",
      "\t35 ù\n",
      "\t36 r\n",
      "\t37 e,ˈ \n",
      "t-Bɛ̀rwər.ˈ \n",
      "\t38 t-\n",
      "\t39 B\n",
      "\t40 ɛ̀\n",
      "\t41 r\n",
      "\t42 w\n",
      "\t43 ə\n",
      "\t44 r.ˈ \n"
     ]
    }
   ],
   "source": [
    "for word in L.d(list(F.otype.s('sentence'))[0], 'word'):\n",
    "    print(f'{T.text(word)}')\n",
    "    for slot in L.d(word, 'char'):\n",
    "        print(f'\\t{slot} {T.text(slot)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Skipped Slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 n \n",
      "10 Y\n"
     ]
    }
   ],
   "source": [
    "for c in range(9, 11):\n",
    "    print(c, T.text(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0x7c'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex(ord('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books and their word counts: \n",
      "\n",
      "551015 The Wise Snake\n",
      "\t779 words, 1020 morphemes\n",
      "551016 The Priest and the Mullah\n",
      "\t317 words, 435 morphemes\n",
      "551017 The Selfish Neighbour\n",
      "\t146 words, 210 morphemes\n",
      "551018 A tale of a prince and a princess\n",
      "\t1763 words, 2437 morphemes\n",
      "551019 The Cooking Pot\n",
      "\t254 words, 327 morphemes\n",
      "551020 A Hundred Gold Coins\n",
      "\t343 words, 483 morphemes\n",
      "551021 A Man Called Čuxo\n",
      "\t789 words, 1041 morphemes\n",
      "551022 TALES FROM THE 1001 NIGHTS\n",
      "\t3018 words, 4195 morphemes\n",
      "551023 The Monk And The Angel\n",
      "\t680 words, 950 morphemes\n",
      "551024 THE MONK WHO WANTED TO KNOW WHEN HE WOULD DIE\n",
      "\t368 words, 487 morphemes\n",
      "551025 THE WISE YOUNG MAN\n",
      "\t1091 words, 1482 morphemes\n",
      "551026 BABY LELIΘA\n",
      "\t845 words, 1148 morphemes\n",
      "551027 THE LELIΘA FROM Č̭ĀL\n",
      "\t252 words, 321 morphemes\n",
      "551028 THE BEAR AND THE FOX\n",
      "\t363 words, 506 morphemes\n",
      "551029 THE DAUGHTER OF THE KING\n",
      "\t1264 words, 1716 morphemes\n",
      "551030 THE SALE OF AN OX\n",
      "\t1294 words, 1711 morphemes\n",
      "551031 THE MAN WHO WANTED TO WORK\n",
      "\t1066 words, 1461 morphemes\n",
      "551032 THE TALE OF PARIZADA, WARDA AND NARGIS\n",
      "\t2016 words, 2698 morphemes\n",
      "551033 THE TALE OF FARXO AND SƏTTIYA\n",
      "\t2490 words, 3303 morphemes\n",
      "551034 THE TALE OF MĂMO AND ZINE\n",
      "\t2633 words, 3510 morphemes\n",
      "551035 THE TALE OF MƏRZA PĂMƏT\n",
      "\t1090 words, 1473 morphemes\n",
      "551036 THE TALE OF RUSTAM (1)\n",
      "\t1008 words, 1316 morphemes\n",
      "551037 THE TALE OF RUSTAM (2)\n",
      "\t1645 words, 2254 morphemes\n",
      "551038 THE CRAFTY HIRELING\n",
      "\t1362 words, 1818 morphemes\n",
      "551039 THE GIANT’S CAVE\n",
      "\t238 words, 336 morphemes\n",
      "551040 THE FOX AND THE MILLER\n",
      "\t841 words, 1101 morphemes\n",
      "551041 THE LION WITH A SWOLLEN LEG\n",
      "\t378 words, 494 morphemes\n",
      "551042 THE GIRL AND THE SEVEN BROTHERS\n",
      "\t744 words, 1030 morphemes\n",
      "551043 NANNO AND JƏNDO\n",
      "\t641 words, 865 morphemes\n",
      "551044 THE STORY WITH NO END\n",
      "\t130 words, 195 morphemes\n",
      "551045 THE TALE OF NASIMO\n",
      "\t389 words, 500 morphemes\n",
      "551046 ŠOŠƏT XERE\n",
      "\t349 words, 439 morphemes\n",
      "551047 THE BROTHER OF GIANTS\n",
      "\t474 words, 659 morphemes\n",
      "551048 THE WISE DAUGHTER OF THE KING\n",
      "\t388 words, 535 morphemes\n",
      "551049 MEASURE FOR MEASURE\n",
      "\t132 words, 174 morphemes\n",
      "551050 THE FOX AND THE LION\n",
      "\t95 words, 124 morphemes\n",
      "551051 SOUR GRAPES\n",
      "\t62 words, 82 morphemes\n",
      "551052 THE CAT AND THE MICE\n",
      "\t99 words, 138 morphemes\n",
      "551053 THE FOX AND THE STORK\n",
      "\t70 words, 102 morphemes\n",
      "551054 THE LION KING\n",
      "\t114 words, 150 morphemes\n",
      "551055 MAN IS TREACHEROUS\n",
      "\t227 words, 313 morphemes\n",
      "551056 THE MAN WHO CRIED WOLF\n",
      "\t199 words, 250 morphemes\n",
      "551057 THE CROW AND THE CHEESE\n",
      "\t51 words, 71 morphemes\n",
      "551058 THE SISISAMBƏR PLANT\n",
      "\t289 words, 385 morphemes\n",
      "551059 QAṬINA RESCUES HIS NEPHEW FROM LELIΘA\n",
      "\t358 words, 492 morphemes\n",
      "551060 THE BATTLE WITH YUWANƏS THE ARMENIAN\n",
      "\t607 words, 780 morphemes\n",
      "551061 Gozali and Nozali\n",
      "\t3828 words, 5424 morphemes\n",
      "551062 THE SCORPION AND THE SNAKE\n",
      "\t216 words, 298 morphemes\n",
      "551063 I AM WORTH THE SAME AS A BLIND WOLF\n",
      "\t528 words, 713 morphemes\n",
      "551064 DƏMDƏMA\n",
      "\t620 words, 849 morphemes\n",
      "551065 THE KING WITH FORTY SONS\n",
      "\t2539 words, 3366 morphemes\n",
      "551066 A TALE OF TWO KINGS\n",
      "\t513 words, 703 morphemes\n",
      "551067 The Bald Man and the King\n",
      "\t2652 words, 3218 morphemes\n",
      "551068 Women are Stronger than Men\n",
      "\t935 words, 1142 morphemes\n",
      "551069 Axiqar\n",
      "\t2656 words, 3300 morphemes\n",
      "551070 Is there a Man with No Worries?\n",
      "\t796 words, 992 morphemes\n",
      "551071 Women do Things Best\n",
      "\t842 words, 1071 morphemes\n",
      "551072 The Dead Rise and Return\n",
      "\t643 words, 790 morphemes\n",
      "551073 A Pound of Flesh\n",
      "\t752 words, 915 morphemes\n",
      "551074 The Loan of a Cooking Pot\n",
      "\t156 words, 190 morphemes\n",
      "551075 Much Ado About Nothing\n",
      "\t334 words, 392 morphemes\n",
      "551076 A Visit from Harun ar-Rashid\n",
      "\t489 words, 591 morphemes\n",
      "551077 The Cat’s Dinner\n",
      "\t115 words, 129 morphemes\n",
      "551078 Ice for Dinner\n",
      "\t91 words, 108 morphemes\n",
      "551079 Am I dead?\n",
      "\t90 words, 118 morphemes\n",
      "551080 A Thousand Dinars\n",
      "\t472 words, 589 morphemes\n",
      "551081 Kindness to a Donkey\n",
      "\t64 words, 79 morphemes\n",
      "551082 The Stupid Carpenter\n",
      "\t111 words, 142 morphemes\n",
      "551083 A Close Shave\n",
      "\t71 words, 91 morphemes\n",
      "551084 A Sweater to Pay Off a Debt\n",
      "\t77 words, 97 morphemes\n",
      "551085 No Bread Today\n",
      "\t188 words, 239 morphemes\n",
      "551086 An Orphan Duckling\n",
      "\t63 words, 72 morphemes\n",
      "551087 Mistaken Identity\n",
      "\t110 words, 139 morphemes\n",
      "551088 Trickster\n",
      "\t157 words, 193 morphemes\n",
      "551089 Problems Lighting a Fire\n",
      "\t132 words, 156 morphemes\n",
      "551090 The Angel of Death\n",
      "\t82 words, 103 morphemes\n",
      "551091 Stomach Trouble\n",
      "\t41 words, 46 morphemes\n",
      "551092 A Lost Donkey\n",
      "\t65 words, 84 morphemes\n",
      "551093 A Lost Ring\n",
      "\t38 words, 47 morphemes\n",
      "551094 The Purchase of a Donkey\n",
      "\t184 words, 221 morphemes\n",
      "551095 Lost Money\n",
      "\t51 words, 62 morphemes\n",
      "551096 The Wife’s Condition\n",
      "\t210 words, 254 morphemes\n",
      "551097 A Donkey Knows Best\n",
      "\t95 words, 126 morphemes\n",
      "551098 When Shall I Die?\n",
      "\t148 words, 189 morphemes\n",
      "551099 I Have Died\n",
      "\t102 words, 129 morphemes\n",
      "551100 The Fisherman and the Princess\n",
      "\t702 words, 807 morphemes\n",
      "551101 The Wife who Learns How to Work\n",
      "\t995 words, 1198 morphemes\n",
      "551102 A Cure for a Husband’s Madness\n",
      "\t1282 words, 1512 morphemes\n",
      "551103 The Bald Child and the Monsters\n",
      "\t1042 words, 1259 morphemes\n",
      "551104 The Wise Young Daughter\n",
      "\t1005 words, 1228 morphemes\n",
      "551105 The Adventures of Ashur\n",
      "\t2719 words, 3294 morphemes\n",
      "551106 A Dragon in the Well\n",
      "\t528 words, 666 morphemes\n",
      "551107 A Painting of the King Of Iran\n",
      "\t763 words, 947 morphemes\n",
      "551108 The Adventures of Two Brothers\n",
      "\t2131 words, 2541 morphemes\n",
      "551109 The Adventures of a Princess\n",
      "\t1887 words, 2209 morphemes\n",
      "551110 Two Wicked Daughters-in-law\n",
      "\t689 words, 788 morphemes\n",
      "551111 A Dutiful Son\n",
      "\t1065 words, 1204 morphemes\n",
      "551112 The Little Prince and the Snake\n",
      "\t243 words, 284 morphemes\n",
      "551113 The Snake’s Dilemma\n",
      "\t1024 words, 1345 morphemes\n",
      "551114 The Wise Brother\n",
      "\t1619 words, 1991 morphemes\n",
      "551115 The Man who Wanted to Complain to God\n",
      "\t513 words, 614 morphemes\n",
      "551116 The Giant One-Eyed Demon\n",
      "\t390 words, 495 morphemes\n",
      "551117 The Cow and The Poor Girl\n",
      "\t578 words, 671 morphemes\n",
      "551118 A Frog Wants a Husband\n",
      "\t460 words, 554 morphemes\n",
      "551119 The Bird and the Fox\n",
      "\t233 words, 259 morphemes\n",
      "551120 The Old Man and the Fish\n",
      "\t543 words, 630 morphemes\n",
      "551121 Two Birds Fall in Love\n",
      "\t412 words, 485 morphemes\n",
      "551122 Star-Crossed Lovers\n",
      "\t294 words, 353 morphemes\n",
      "551123 The Assyrians of Urmi\n",
      "\t2268 words, 2768 morphemes\n",
      "551124 Village Life\n",
      "\t1064 words, 1372 morphemes\n",
      "551125 Agriculture and Village Life\n",
      "\t2028 words, 2487 morphemes\n",
      "551126 Hunting\n",
      "\t938 words, 1139 morphemes\n",
      "551127 Weddings and Festivals\n",
      "\t727 words, 879 morphemes\n",
      "551128 Events in 1946 on the Urmi Plain\n",
      "\t509 words, 639 morphemes\n",
      "551129 Village Life\n",
      "\t1263 words, 1614 morphemes\n",
      "551130 Weddings\n",
      "\t425 words, 534 morphemes\n",
      "551131 Games\n",
      "\t793 words, 1010 morphemes\n",
      "551132 Village Life\n",
      "\t1796 words, 2226 morphemes\n",
      "551133 St. Zayya’s Cake Dough\n",
      "\t554 words, 720 morphemes\n",
      "551134 Nipuxta\n",
      "\t366 words, 507 morphemes\n",
      "551135 Vineyards\n",
      "\t156 words, 199 morphemes\n",
      "551136 Village Life\n",
      "\t604 words, 812 morphemes\n",
      "551137 Village Life\n",
      "\t584 words, 684 morphemes\n",
      "551138 The Assyrians of Armenia\n",
      "\t712 words, 786 morphemes\n",
      "551139 Village Life\n",
      "\t2851 words, 3540 morphemes\n"
     ]
    }
   ],
   "source": [
    "print('books and their word counts: \\n')\n",
    "for text in F.otype.s('text'):\n",
    "    text_words = L.d(text, 'word')\n",
    "    text_morphemes = L.d(text, 'morpheme')\n",
    "    print(text, F.title.v(text))\n",
    "    print(f'\\t{len(text_words)} words, {len(text_morphemes)} morphemes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = F.otype.s('text')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(L.d(text, 'word'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wise Snake\n",
      "551605 ʾána ʾíwən Yúwəl Yuḥànnaˈ ʾÌsḥaqˈ t-máθət Dùre,ˈ t-Bɛ̀rwər.ˈ ʾáyya hóla θàya,ˈ hóla màra:ˈ\n",
      "551606 ʾíθwa xa-màlka.ˈ ʾáwwa málka xzéle xa-xə̀lma.ˈ ʾu-qédamta mə̀reˈ kúl náša t-yắðe mòdin xə́zya b-xə́lmiˈ bəd-yawə̀nneˈ ʾə̀mma dáwe.ˈ yáʿni kút yắðe mòdile mhumzə́ma ʾáwwa málka,ˈ mòdile xə́zya b-xə́lme díyeˈ b-yawə́lle ʾə̀mma dáwe.ˈ\n",
      "551607 šúryela náše xáša kəs-màlka.ˈ ʾáwwa mára ʾàtxət xə́zya-wˈ ʾáwwa ʾàtxət xə́zya-wˈ ʾáwwa ʾàtxət xə́zya.ˈ\n",
      "551608 xá-naša ʾámər ṭla-báxte dìyeˈ qɛ́mən ʾàzənˈ ʾap-àna mjarbə́nnaˈ ḥàð̣ð̣ díyi.ˈ xázəx qəsmə̀ttila,ˈ bálki qàrmən.ˈ ʾáwwa qímɛle zìlɛle,ˈ zìla,ˈ zìla,ˈ b-ʾúrxa tfíqɛle xá-xuwwe bìye.ˈ\n",
      "551609 ʾo-xúwwe mə̀reˈ hà-našaˈ lɛ̀kət zála?ˈ mə́re b-álaha hon-zála kəs-màlka.ˈ málka hóle xə́zya xa-xə̀lma.ˈ màraˈ kút-yăðe mòdile xə́zya b-xə́lme w-amə̀rreˈ bəd-šáqəl ʾálpa dàwe.ˈ ʾɛ́-ga ʾáp-ana bắyən ʾàzən,ˈ ṱ-amrə́nne xa-xàbra.ˈ bàlkiˈ képa qítle b-gàwzaˈ npìlle-ʾăra gáwza.ˈ bálki qrìmli.ˈ\n",
      "551610 ʾó-xuwwe mə́re ʾəlle-dìyeˈ ʾən-ʾána ʾamrə́nnux módile xə́zya málka b-xə̀lme,ˈ ʾánna ʾálpa dáwe pălə̀tla,ˈ pálga ṭlàliˈ pálga ṭlàlux?ˈ mə́re nàša!ˈ ʾána ʾə́mma bàssiˈ ʾan-xéne kúlla ṭlàlux.ˈ mə́re làʾ!ˈ pálga ṭlàli,ˈ pálga ṭlàlux.ˈ mə́re mqawòlən.ˈ mə́re ʾámrət ṭla-màlkaˈ xzélux b-xə̀lmuxˈ dúnye ráya tèle.ˈ\n",
      "551611 ʾap-aw-zìlɛleˈ xíša mə́ṭya kəs-màlka.ˈ mṭéle gárre dìyeˈ wírre kəs-màlka.ˈ mə́re hàxˈ xázəx máṣət ṱ-amrə́tli módin xə́zya b-xə̀lmi.ˈ mə́re hɛ̀ málka.ˈ mə́re ʾáti xə́zyət b-xə́lmux dúnye ráya tèle.ˈ mə́re ʾày!ˈ dàx ðílux?!ˈ qáyəm yawə́lle ʾalpà-dawe.ˈ ʾalpá-dawe ʾu-zìlɛle.ˈ\n",
      "551612 plíṭɛle b-úrxa mtuxmə̀nne.ˈ mə́re xúwwe mà ṱ-áwəð b-an-álpa dáwe,ˈ ṱ-ázən hawə́nna ʾəlle-dìye?!ˈ ʾána lɛ́-yawənne ʾàp-xa-fəlsaˈ qɛ́mən ʾázən bɛ̀θaˈ yáwən dèni.ˈ ʾu-mtágəbrən bnòneˈ mtágəbrən gàni.ˈ xúwwe ʾázəl dúke dìye.ˈ\n",
      "551613 xìšɛle.ˈ xíšɛle l-bɛ̀θaˈ lɛ́le-hiwa ʾàtxa.ˈ píšɛle xà-yarxaˈ málka xzéle xa-xə́lma xèna.ˈ mə́dre xzéle xə̀lma.ˈ mə́re kút-yăðe módi mìraˈ ʾálpa dáwe xéne t-yawə̀nne.ˈ\n",
      "551614 ʾaw-šmìleˈ ʾay!ˈ m-xúwwe hon-muxə̀rwəllaˈ máṭo ṱ-òya?!ˈ mə́re ṭla-bàxteˈ hàtxən wíða.ˈ mòdit mára?ˈ mə́ra sí-ṭlub paxálta mə́nne dìyeˈ ʾu-múr t-yawə́nnux zùzux.ˈ mə́re ṱ-àzənˈ xàzəxˈ bálki lá-hawe tàma,ˈ bálki là-pliṭle l-úrxi.ˈ\n"
     ]
    }
   ],
   "source": [
    "print(F.title.v(text))\n",
    "\n",
    "for sent in F.otype.s('line')[:10]:\n",
    "    print(sent, T.text(sent))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
