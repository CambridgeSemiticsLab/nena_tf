{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Corpus\n",
    "\n",
    "In this notebook, we experiment with producing a TF resource for the Christian Urmi NENA dialect. The text itself comes from Geoffrey Khan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "from IPython.display import display, HTML\n",
    "from tf.fabric import Fabric\n",
    "with open('christian_urmi.txt', 'r') as infile:\n",
    "    urmi = infile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Text-Fabric Resource\n",
    "\n",
    "Text-Fabric is a format and tool for the storage, annotation, and analysis of text corpora. The Text-Fabric data model is explained in depth [in its docs](https://annotation.github.io/text-fabric/Model/Data-Model/).\n",
    "\n",
    "Herein we follow a fairly standard approach to convert a plain-text file into a TF resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Up Node Feature and Oslot Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateKey(dictionary):\n",
    "    '''\n",
    "    Auto increments a key from a dictionary.\n",
    "    '''\n",
    "    return max(dictionary.keys(), default=0)+1\n",
    "\n",
    "\n",
    "def cleanToken(token):\n",
    "    '''\n",
    "    Dealing with encoding variances.\n",
    "    First issue deals with a and its accent.\n",
    "    '''\n",
    "    return token.replace(chr(97)+chr(769), chr(225))\n",
    "\n",
    "raw_node_features = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "raw_oslots = collections.defaultdict(lambda:collections.defaultdict(set))\n",
    "slot = 0\n",
    "\n",
    "this_sentence = 1 # for first iteration since only sentence ends are marked\n",
    "\n",
    "for line in urmi.split('\\n'):\n",
    "    \n",
    "    # mark book beginnings, their \"code\" and title\n",
    "    if line.startswith('# '): # book code\n",
    "        this_book = iterateKey(raw_oslots['book'])\n",
    "        raw_node_features['book_code'][this_book] = line.split()[-1].strip()\n",
    "        continue\n",
    "    elif line.startswith('## '): # book title\n",
    "        raw_node_features['book_title'][this_book] = line.split('#')[-1]\n",
    "        continue\n",
    "                \n",
    "    # map slots to objects and features:\n",
    "    for token in line.split():\n",
    "        \n",
    "        if re.match('.*\\(\\d*\\)', token): # line start\n",
    "            this_line = iterateKey(raw_oslots['line'])\n",
    "            raw_node_features['line'][this_line] = token\n",
    "            continue\n",
    "            \n",
    "        # everything up to this point is a valid slot\n",
    "        # iterate slot up by 1\n",
    "        slot += 1\n",
    "            \n",
    "        # record sentence boundaries\n",
    "        if re.match('.*\\.\\|', token): # end of sentence\n",
    "            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "            this_sentence = iterateKey(raw_oslots['sentence']) # get incremented, new sentence ID\n",
    "        else: # beginning/within sentence\n",
    "            raw_oslots['sentence'][this_sentence].add(slot)\n",
    "            \n",
    "        raw_node_features['trans'][slot] = cleanToken(token)\n",
    "        raw_node_features['trailer'][slot] = ' '\n",
    "        raw_oslots['book'][this_book].add(slot)\n",
    "        raw_oslots['line'][this_line].add(slot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reindex Objects Above Slot Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "otype2feature = {'book':{'book_code', 'book_title'},\n",
    "                 'line':{'line'},\n",
    "                 'sentence':{}}\n",
    "\n",
    "node_features = collections.defaultdict(lambda:collections.defaultdict())\n",
    "node_features['trans'] = raw_node_features['trans'] # add slot features\n",
    "node_features['trailer'] = raw_node_features['trailer']\n",
    "for slot in node_features['trans']:\n",
    "    node_features['otype'][slot] = 'word'\n",
    "    \n",
    "edge_features = collections.defaultdict(lambda:collections.defaultdict(set)) # oslots will go here\n",
    "\n",
    "onode = max(raw_node_features['trans']) # max slot, incremented +1 in loop\n",
    "\n",
    "for otype in raw_oslots.keys():\n",
    "    for oID, slots in raw_oslots[otype].items():\n",
    "        \n",
    "        # make new object node number\n",
    "        onode += 1\n",
    "        node_features['otype'][onode] = otype\n",
    "        \n",
    "        # remap node features to node number\n",
    "        for feat in otype2feature[otype]:\n",
    "            node_features[feat][onode] = raw_node_features[feat][oID]\n",
    "        edge_features['oslots'][onode] = raw_oslots[otype][oID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['trans', 'trailer', 'otype', 'book_title', 'book_code', 'line'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['oslots'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_features.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to TF Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.11\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "9 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "otext = {\n",
    "'sectionTypes': 'book,line',\n",
    "'sectionFeatures':'book_code,line',\n",
    "'fmt:text-orig-full':'{trans}{trailer}'\n",
    "}\n",
    "\n",
    "meta = {'':{'author': 'Geoffrey Khan and Cody Kingham'},\n",
    "        'oslots':{'edgeValues':False, 'valueType':'int'},\n",
    "        'otype':{'valueType':'str'},\n",
    "        'book':{'valueType':'str'},\n",
    "        'line':{'valueType':'str'},\n",
    "        'trans':{'valueType':'str'},\n",
    "        'book_code':{'valueType':'str'},\n",
    "        'book_title':{'valueType':'str'},\n",
    "        'trailer':{'valueType':'str'},\n",
    "        'otext':otext}\n",
    "\n",
    "TFs = Fabric(locations=['tf/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Exporting 6 node and 1 edge and 2 config features to tf/:\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.01s maxSlot=       2217\n",
      "  0.01s maxNode=       2595\n",
      "  0.01s OK: oslots is valid\n",
      "   |     0.00s T book_code            to tf\n",
      "   |     0.00s T book_title           to tf\n",
      "   |     0.00s T line                 to tf\n",
      "   |     0.00s T otype                to tf\n",
      "   |     0.01s T trailer              to tf\n",
      "   |     0.01s T trans                to tf\n",
      "   |     0.00s T oslots               to tf\n",
      "   |     0.00s M book                 to tf\n",
      "   |     0.00s M otext                to tf\n",
      "  0.04s Exported 6 node features and 1 edge features and 2 config features to tf/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFs.save(nodeFeatures=node_features, edgeFeatures=edge_features, metaData=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nena Corpus Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf.fabric import Fabric\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.4.11\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "9 features found and 0 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s T otype                from tf\n",
      "   |     0.00s T book_code            from tf\n",
      "   |     0.01s T trans                from tf\n",
      "   |      |     0.00s C __levels__           from otype, oslots, otext\n",
      "   |      |     0.02s C __order__            from otype, oslots, __levels__\n",
      "   |      |     0.00s C __rank__             from otype, __order__\n",
      "   |      |     0.01s C __levUp__            from otype, oslots, __levels__, __rank__\n",
      "   |      |     0.00s C __levDown__          from otype, __levUp__, __rank__\n",
      "   |      |     0.00s C __boundary__         from otype, oslots, __rank__\n",
      "   |      |     0.00s C __sections__         from otype, oslots, otext, __levUp__, __levels__, book_code, line\n",
      "   |     0.00s T book_title           from tf\n",
      "  0.09s All features loaded/computed - for details use loadLog()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TF = Fabric(locations='tf/')\n",
    "\n",
    "N = TF.load('''\n",
    "\n",
    "book_code trans otype book_title\n",
    "\n",
    "''')\n",
    "\n",
    "N.makeAvailableIn(globals())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2218  The Loan of a Cooking Pot (Yulia Davudi, +Hassar +Baba-čanɟa, N)\n",
      "\t180 words\n",
      "2219  Agriculture and Village Life (Natan Khoshaba, Zumallan, N)\n",
      "\t2037 words\n"
     ]
    }
   ],
   "source": [
    "for book in F.otype.s('book'):\n",
    "    book_words = L.d(book, 'word')\n",
    "    print(book, F.book_title.v(book))\n",
    "    print(f'\\t{len(book_words)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Token Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('+xárta', 30),\n",
       " ('ʾá', 30),\n",
       " ('k̭át', 22),\n",
       " ('ʾíta', 18),\n",
       " ('xá', 16),\n",
       " ('+rába', 15),\n",
       " ('ʾátxa', 14),\n",
       " ('cúllǝ', 11),\n",
       " ('+k̭usárta', 10),\n",
       " ('xína', 10),\n",
       " ('ʾìta|', 9),\n",
       " ('míyya', 9),\n",
       " ('ʾánnǝ', 9),\n",
       " ('dástə', 8),\n",
       " ('+bár', 8),\n",
       " ('cùllǝ|', 8),\n",
       " ('bí', 8),\n",
       " ('hál', 8),\n",
       " ('tré', 7),\n",
       " ('ʾíta', 7),\n",
       " ('là', 7),\n",
       " ('c-avíva', 7),\n",
       " ('ʾǝ́tvalan', 7),\n",
       " ('lè', 7),\n",
       " ('ʾína', 6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = collections.Counter()\n",
    "\n",
    "for w in F.otype.s('word'):\n",
    "    tokens[F.trans.v(w)] += 1\n",
    "    \n",
    "tokens.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2266 xá yuma +málla +Nasràdən| bərrə̀xšələ| bəšk̭álələ +k̭usárta déna mən švàvu.| \n",
      "2267 màrǝlə| hálli xá dana +k̭usàrta| +báyyən bášlən ɟávo bušàla.| \n",
      "2268 +k̭usárta +ɟúrta lə̀tli.| \n",
      "2269 bəšk̭álolə màyolə +k̭usárta| bušála bašùlələ,| labùlolə,| yávolə mə̀drə| k̭à| švàva.| \n",
      "2270 ʾína tré +k̭usaryay sùrə| mattúyəl ɟàvo.| \n",
      "2271 švàva| màrǝlə| ʾáha tré +k̭usaryàtə| k̭àm muyyévət?| mə̀rrə| +k̭usártət dìyyux| də̀lla| tré xínə mə̀nno.| \n",
      "2272 yávəl k̭àtu| ʾávət basìma,| bitàyələ.| \n",
      "2273 ʾé-šabta xìta| +málla +Nasrádən bərrə́xšəl mə̀drə.| \n",
      "2274 màrələ| +maxlèta,| xa +k̭usárta buš +ɟùrta +byáyəvən.| \n",
      "2275 +málla +Nasràdən| +ʾáynu pə́ltəva +ʾal-xa +k̭usartət švàvə.| \n"
     ]
    }
   ],
   "source": [
    "for sent in list(F.otype.s('sentence'))[:10]:\n",
    "    print(sent, T.text(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Search Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "find = list(S.search(f'''\n",
    "\n",
    "sentence\n",
    "    word trans=+xárta\n",
    "    <: word\n",
    "\n",
    "'''))\n",
    "\n",
    "print(len(find))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+xárta  pā̀n \n",
      "+xárta  ʾǝ́tva \n",
      "+xárta  ci-+yasrìvalun.| \n",
      "+xárta  ɟəddàla| \n",
      "+xárta  +mač̭ràxvalǝ.| \n",
      "+xárta  cùllǝ| \n",
      "+xárta  +marč̭ìvalǝ| \n",
      "+xárta  púmmu \n",
      "+xárta  +marč̭ìvalun.| \n",
      "+xárta  zìla \n",
      "+xárta  ʾé \n",
      "+xárta  pardùvvǝ \n",
      "+xárta  maštàxla| \n",
      "+xárta  pummé \n",
      "+xárta  k̭át \n",
      "+xárta  +xazdàxvala| \n",
      "+xárta  bí \n",
      "+xárta  mǝn-dàha| \n",
      "+xárta  ʾánnǝ \n",
      "+xárta  ɟári \n",
      "+xárta  b-ràcxa,| \n",
      "+xárta  +ʾànvǝ \n",
      "+xárta  +xárta \n",
      "+xárta  nášǝ \n",
      "+xárta  ʾá \n",
      "+xárta  b-ptána \n",
      "+xárta  +ṱárpa \n",
      "+xárta  b-labláxla \n",
      "+xárta  +xàrta,| \n",
      "+xárta  b-šatxìvalun.| \n"
     ]
    }
   ],
   "source": [
    "for res in find:\n",
    "    print(T.text(res[1]), T.text(res[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suffix Searching\n",
    "\n",
    "It looks like the ending `un` is could be a plural verb ending? Here is a query for those endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 results\n",
      "\n",
      "769 c-avǝ́dvalun \n",
      "1921 tílun \n",
      "1544 tùttun.| \n",
      "2187 túttun \n",
      "2190 túttun \n",
      "792 ʾax-šatxáxvalun \n",
      "1947 +rappívalun \n",
      "668 ci-yavvàvalun.| \n",
      "797 ci-+yasrìvalun.| \n",
      "1824 ci-+pašṱìvalun,| \n",
      "1827 ɟaršìvalun,| \n",
      "1317 túttun \n",
      "678 c-odívalun.| \n",
      "680 ʾǝ́tvalun.| \n",
      "815 mabrǝzzìvalun,| \n",
      "1967 ṱ-+axlìvalun.| \n",
      "691 c-odìvalun.| \n",
      "692 +mardǝxxívalun \n",
      "1588 maštàxvalun.| \n",
      "950 +pallìvalun.| \n",
      "1976 ʾǝ́tvalun.| \n",
      "1081 mayyáxvalun \n",
      "698 +palṱìvalun.| \n",
      "700 b-šatxìvalun.| \n",
      "958 +pallìvalun.| \n",
      "449 +rappívalun \n",
      "707 šaṱxìvalun.| \n",
      "1604 ci-+xalvìvalun.| \n",
      "1862 xǝ́šlun \n",
      "1865 muyyílun \n",
      "202 +ṱrǝ̀plun,| \n",
      "204 +ṱripàlun,| \n",
      "1484 mattáxlun \n",
      "1102 lablívalun \n",
      "1868 šk̭ǝ́llun \n",
      "1871 +zrílun.| \n",
      "977 +jammáxvalun \n",
      "850 ɟabìvalun,| \n",
      "1996 banìvalun.| \n",
      "852 šaṱxìvalun,| \n",
      "980 b-lablàxvalun.| \n",
      "854 +marč̭ìvalun.| \n",
      "1876 tìlun,| \n",
      "1879 zǝ̀dlun.| \n",
      "2001 ci-banívalun.| \n",
      "864 ci-malívalun \n",
      "1506 túttun \n",
      "1764 mattáxvalun \n",
      "1125 +daràxlun.| \n",
      "2023 ci-tanáxvalun.| \n",
      "1512 túttun \n",
      "617 b-+jammáxvalun \n",
      "2151 +dávun \n",
      "748 +ʾàvun,| \n",
      "621 ci-mayyàxvalun.| \n",
      "1397 tùttun.| \n",
      "1655 +xalvìvalun.| \n",
      "761 +ṱamšáxvalun \n",
      "382 +ʾávun \n",
      "1535 túttun \n"
     ]
    }
   ],
   "source": [
    "suffix = list(S.search('''\n",
    "\n",
    "word trans~un\\.|un$|un,\n",
    "\n",
    "'''))\n",
    "\n",
    "print(len(suffix), 'results\\n')\n",
    "\n",
    "for res in suffix:\n",
    "    print(res[0], T.text(res[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
