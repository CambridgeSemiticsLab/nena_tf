{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sweet-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-singer",
   "metadata": {},
   "source": [
    "Make plain text data, remembering the nodes that the text comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "growing-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from tf.app import use\n",
    "from tf.convert.recorder import Recorder\n",
    "from tf.core.helpers import specFromRangesLogical, rangesFromSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expected-decline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local github\">TF-app:</b> <span title=\"repo clone offline under ~/github\">~/github/annotation/app-nena/code</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b title=\"local github\">data:</b> <span title=\"repo clone offline under ~/github\">~/github/CambridgeSemiticsLab/nena_tf/tf/alpha</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 8.4.13</a>, <a target=\"_blank\" href=\"https://github.com/annotation/app-nena\" title=\"nena TF-app\">app-nena v3</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/about/searchusage.html\" title=\"Search Templates Introduction and Reference\">Search Reference</a><br><b>Data:</b> <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md.md\" title=\"provenance of Northeastern Neo-Aramaic Text Corpus\">NENA_TF</a>, <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/transcription.md\" title=\"NENA transcription script\">Character table</a>, <a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#\" title=\"NENA_TF feature documentation\">Feature docs</a><br><b>Features:</b><br><details><summary><b>Northeastern Neo-Aramaic Text Corpus</b></summary><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#dialect\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/dialect.tf\">dialect</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#full\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/full.tf\">full</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#full_end\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/full_end.tf\">full_end</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#fuzzy\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/fuzzy.tf\">fuzzy</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#fuzzy_end\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/fuzzy_end.tf\">fuzzy_end</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#gloss\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/gloss.tf\">gloss</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#gn\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/gn.tf\">gn</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lang\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/lang.tf\">lang</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lemma\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/lemma.tf\">lemma</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#line_number\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/line_number.tf\">line_number</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lite\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/lite.tf\">lite</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#lite_end\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/lite_end.tf\">lite_end</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#n_parses\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/n_parses.tf\">n_parses</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#nu\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/nu.tf\">nu</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#nu_class\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/nu_class.tf\">nu_class</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#otype\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#phonation\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/phonation.tf\">phonation</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#phonetic_class\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/phonetic_class.tf\">phonetic_class</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#phonetic_manner\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/phonetic_manner.tf\">phonetic_manner</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#phonetic_place\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/phonetic_place.tf\">phonetic_place</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#place\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/place.tf\">place</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#pos\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/pos.tf\">pos</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#speaker\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/speaker.tf\">speaker</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#speakers\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/speakers.tf\">speakers</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#st\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/st.tf\">st</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#tense\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/tense.tf\">tense</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/text.tf\">text</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_end\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/text_end.tf\">text_end</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_id\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/text_id.tf\">text_id</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_nostress\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/text_nostress.tf\">text_nostress</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#text_nostress_end\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/text_nostress_end.tf\">text_nostress_end</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#title\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/title.tf\">title</a><br><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#variable\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/variable.tf\">variable</a><br><b><i><a target=\"_blank\" href=\"https://github.com/CambridgeSemiticsLab/nena_tf/blob/master/docs/features.md#oslots\" title=\"~/github/CambridgeSemiticsLab/nena_tf/tf/alpha/oslots.tf\">oslots</a></i></b><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".ubd {\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".col {\n",
       "   display: inline-block;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 2px;\n",
       "    margin: 2px;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -2px 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 2px 0px;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".tfsechead {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--tfsechead);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  10px 2px 2px 2px;\n",
       "    margin: 16px 2px 2px 2px;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 4px;\n",
       "    margin-left: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 4px;\n",
       "    margin-right: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -24px;\n",
       "    margin-left: 20px;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 6px;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 2px;\n",
       "    margin-left: 2px;\n",
       "    padding: 2px 2px;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 40px;\n",
       "\tpadding: 20px;\n",
       "\tborder: 2px solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--tfsechead:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         3px;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   2px;\n",
       "  --border-width0:      2px;\n",
       "  --border-width1:      3px;\n",
       "  --border-width2:      4px;\n",
       "  --border-width3:      6px;\n",
       "  --border-width4:      5px;\n",
       "  --border-width-plain: 2px;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 2px;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 4px ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 4px ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 6px;\n",
       "  padding: 4px;\n",
       "  margin: 4px;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       ".speaker {\n",
       "    vertical-align: super;\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #884400;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = use(\"nena:clone\", checkout=\"clone\", hoist=globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "departmental-unknown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('+Hassar +Baba-čanɟa, N', 36),\n",
       " ('Dure', 31),\n",
       " ('ʾƐn-Nune', 20),\n",
       " ('Zumallan, N', 11),\n",
       " ('Canda, Georgia', 7),\n",
       " ('Guylasar, Armenia', 7),\n",
       " ('Arzni, Armenia', 4),\n",
       " ('Babari, S', 3),\n",
       " ('+Hassar +Baba-canɟa, N', 1),\n",
       " ('+Spurġān, N', 1),\n",
       " ('Gulpashan, S', 1),\n",
       " ('Mushava, N', 1),\n",
       " ('Mushawa, N', 1),\n",
       " ('Spurġān, N', 1),\n",
       " ('Ɛn Nune', 1))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.place.freqList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "broke-nirvana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('A Close Shave', 1),\n",
       " ('A Cure for a Husband’s Madness', 1),\n",
       " ('A Donkey Knows Best', 1),\n",
       " ('A Dragon in the Well', 1),\n",
       " ('A Dutiful Son', 1),\n",
       " ('A Frog Wants a Husband', 1),\n",
       " ('A Hundred Gold Coins', 1),\n",
       " ('A Lost Donkey', 1),\n",
       " ('A Lost Ring', 1),\n",
       " ('A Man Called Čuxo', 1),\n",
       " ('A Painting of the King of Iran', 1),\n",
       " ('A Pound of Flesh', 1),\n",
       " ('A Sweater to Pay Off a Debt', 1),\n",
       " ('A Tale of Two Kings', 1),\n",
       " ('A Tale of a Prince and a Princess', 1),\n",
       " ('A Thousand Dinars', 1),\n",
       " ('A Visit From Harun Ar-Rashid', 1),\n",
       " ('Agriculture and Village Life', 1),\n",
       " ('Am I Dead?', 1),\n",
       " ('An Orphan Duckling', 1),\n",
       " ('Axiqar', 1),\n",
       " ('Baby Leliθa', 1),\n",
       " ('Dəmdəma', 1),\n",
       " ('Events in 1946 on the Urmi Plain', 1),\n",
       " ('Games', 1),\n",
       " ('Gozali and Nozali', 1),\n",
       " ('Hunting', 1),\n",
       " ('I Am Worth the Same as a Blind Wolf', 1),\n",
       " ('I Have Died', 1),\n",
       " ('Ice for Dinner', 1),\n",
       " ('Is There a Man With No Worries?', 1),\n",
       " ('Kindness to a Donkey', 1),\n",
       " ('Lost Money', 1),\n",
       " ('Man Is Treacherous', 1),\n",
       " ('Measure for Measure', 1),\n",
       " ('Mistaken Identity', 1),\n",
       " ('Much Ado About Nothing', 1),\n",
       " ('Nanno and Jəndo', 1),\n",
       " ('Nipuxta', 1),\n",
       " ('No Bread Today', 1),\n",
       " ('Problems Lighting a Fire', 1),\n",
       " ('Qaṭina Rescues His Nephew From Leliθa', 1),\n",
       " ('Sour Grapes', 1),\n",
       " ('St. Zayya’s Cake Dough', 1),\n",
       " ('Star-Crossed Lovers', 1),\n",
       " ('Stomach Trouble', 1),\n",
       " ('Tales From the 1001 Nights', 1),\n",
       " ('The Adventures of Ashur', 1),\n",
       " ('The Adventures of Two Brothers', 1),\n",
       " ('The Adventures of a Princess', 1),\n",
       " ('The Angel of Death', 1),\n",
       " ('The Assyrians of Armenia', 1),\n",
       " ('The Assyrians of Urmi', 1),\n",
       " ('The Bald Child and the Monsters', 1),\n",
       " ('The Bald Man and the King', 1),\n",
       " ('The Battle With Yuwanəs the Armenian', 1),\n",
       " ('The Bear and the Fox', 1),\n",
       " ('The Bird and the Fox', 1),\n",
       " ('The Brother of Giants', 1),\n",
       " ('The Cat and the Mice', 1),\n",
       " ('The Cat’s Dinner', 1),\n",
       " ('The Cooking Pot', 1),\n",
       " ('The Cow and the Poor Girl', 1),\n",
       " ('The Crafty Hireling', 1),\n",
       " ('The Crow and the Cheese', 1),\n",
       " ('The Daughter of the King', 1),\n",
       " ('The Dead Rise and Return', 1),\n",
       " ('The Fisherman and the Princess', 1),\n",
       " ('The Fox and the Lion', 1),\n",
       " ('The Fox and the Miller', 1),\n",
       " ('The Fox and the Stork', 1),\n",
       " ('The Giant One-Eyed Demon', 1),\n",
       " ('The Giant’s Cave', 1),\n",
       " ('The Girl and the Seven Brothers', 1),\n",
       " ('The King With Forty Sons', 1),\n",
       " ('The Leliθa From č̭āl', 1),\n",
       " ('The Lion King', 1),\n",
       " ('The Lion With a Swollen Leg', 1),\n",
       " ('The Little Prince and the Snake', 1),\n",
       " ('The Loan of a Cooking Pot', 1),\n",
       " ('The Man Who Cried Wolf', 1),\n",
       " ('The Man Who Wanted to Complain to God', 1),\n",
       " ('The Man Who Wanted to Work', 1),\n",
       " ('The Monk Who Wanted to Know When He Would Die', 1),\n",
       " ('The Monk and the Angel', 1),\n",
       " ('The Old Man and the Fish', 1),\n",
       " ('The Priest and the Mullah', 1),\n",
       " ('The Purchase of a Donkey', 1),\n",
       " ('The Sale of an Ox', 1),\n",
       " ('The Scorpion and the Snake', 1),\n",
       " ('The Selfish Neighbour', 1),\n",
       " ('The Sisisambər Plant', 1),\n",
       " ('The Snake’s Dilemma', 1),\n",
       " ('The Story With No End', 1),\n",
       " ('The Stupid Carpenter', 1),\n",
       " ('The Tale of Farxo and Səttiya', 1),\n",
       " ('The Tale of Mămo and Zine', 1),\n",
       " ('The Tale of Mərza Pămət', 1),\n",
       " ('The Tale of Nasimo', 1),\n",
       " ('The Tale of Parizada, Warda and Nargis', 1),\n",
       " ('The Tale of Rustam (1)', 1),\n",
       " ('The Tale of Rustam (2)', 1),\n",
       " ('The Wife Who Learns How to Work', 1),\n",
       " ('The Wife Who Learns How to Work (2)', 1),\n",
       " ('The Wife’s Condition', 1),\n",
       " ('The Wise Brother', 1),\n",
       " ('The Wise Daughter of the King', 1),\n",
       " ('The Wise Snake', 1),\n",
       " ('The Wise Young Daughter', 1),\n",
       " ('The Wise Young Man', 1),\n",
       " ('Trickster', 1),\n",
       " ('Two Birds Fall in Love', 1),\n",
       " ('Two Wicked Daughters-In-Law', 1),\n",
       " ('Village Life', 1),\n",
       " ('Village Life (2)', 1),\n",
       " ('Village Life (3)', 1),\n",
       " ('Village Life (4)', 1),\n",
       " ('Village Life (5)', 1),\n",
       " ('Village Life (6)', 1),\n",
       " ('Vineyards', 1),\n",
       " ('Weddings', 1),\n",
       " ('Weddings and Festivals', 1),\n",
       " ('When Shall I Die?', 1),\n",
       " ('Women Are Stronger Than Men', 1),\n",
       " ('Women Do Things Best', 1),\n",
       " ('Šošət Xere', 1))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.title.freqList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2 dialect      nodes average 269689 chars\n",
      "   126 text         nodes average   4281 chars\n",
      "   350 paragraph    nodes average   1541 chars\n",
      "  2544 line         nodes average    212 chars\n",
      " 16326 sentence     nodes average     33 chars\n",
      " 24497 subsentence  nodes average     22 chars\n",
      " 36444 inton        nodes average     15 chars\n",
      " 93766 stress       nodes average      6 chars\n",
      "120151 word         nodes average      4 chars\n",
      "539378 letter       nodes average      1 chars\n"
     ]
    }
   ],
   "source": [
    "for (tp, av, start, end) in C.levels.data:\n",
    "    print(f\"{end - start + 1:>6} {tp:<12} nodes average {int(round(av)):>6} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-township",
   "metadata": {},
   "source": [
    "# Generate full text\n",
    "\n",
    "We use the `full` transcription.\n",
    "\n",
    "We remember nodes of the types *letter*, *word*, *sentence*, *line*, and *text*.\n",
    "\n",
    "We store the positions by node type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "hidden-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "GH = os.path.expanduser(\"~/github\")\n",
    "ORG = \"CambridgeSemiticsLab\"\n",
    "REPO = \"nena_tf\"\n",
    "REL = \"nena2search/app\"\n",
    "OUTPUT = f\"{GH}/{ORG}/{REPO}/{REL}\"\n",
    "DEBUG_OUTPUT = f\"{GH}/{ORG}/{REPO}/_local\"\n",
    "TEST_TEXTS_DIR = f\"{DEBUG_OUTPUT}/texts\"\n",
    "\n",
    "for d in (OUTPUT, TEST_TEXTS_DIR):\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raising-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "PH_ABSENT = \"z\"\n",
    "CH_ABSENT = \"¿\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "expensive-eclipse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node type declared as result container ('by'):\n",
      "\tsentence\n",
      "Layers declared as result showers ('show'):\n",
      "\tletter/full, line/number, text/title\n"
     ]
    }
   ],
   "source": [
    "DATA = dict(\n",
    "    letter=dict(\n",
    "        layers=dict(\n",
    "            full=dict(\n",
    "                feature=\"full\",\n",
    "                map=None,\n",
    "                default=CH_ABSENT,\n",
    "                pos=None,\n",
    "                show=True,\n",
    "            ),\n",
    "            cls=dict(\n",
    "                feature=\"phonetic_class\",\n",
    "                map={\n",
    "                    \"vowel\": \"V\",\n",
    "                    \"consonant\": \"C\",\n",
    "                },\n",
    "                default=PH_ABSENT,\n",
    "                pos=None,\n",
    "            ),\n",
    "            voice=dict(\n",
    "                feature=\"phonation\",\n",
    "                map={\n",
    "                    \"plain\": \"P\",\n",
    "                    \"unvoiced_aspirated\": \"H\",\n",
    "                    \"voiced\": \"V\",\n",
    "                    \"unvoiced\": \"F\",\n",
    "                    \"unvoiced_unaspirated\": \"G\",\n",
    "                    \"emphatic\": \"X\",\n",
    "                },\n",
    "                default=PH_ABSENT,\n",
    "                pos=\"cls\",\n",
    "            ),\n",
    "            place=dict(\n",
    "                feature=\"phonetic_place\",\n",
    "                map={\n",
    "                    \"dental-alveolar\": \"D\",\n",
    "                    \"labial\": \"B\",\n",
    "                    \"palatal-alveolar\": \"C\",\n",
    "                    \"palatal\": \"J\",\n",
    "                    \"velar\": \"G\",\n",
    "                    \"uvular\": \"X\",\n",
    "                    \"pharyngeal\": \"Q\",\n",
    "                    \"laryngeal\": \"H\",\n",
    "                },\n",
    "                default=PH_ABSENT,\n",
    "                pos=\"cls\",\n",
    "            ),\n",
    "            manner=dict(\n",
    "                feature=\"phonetic_manner\",\n",
    "                map={\n",
    "                    \"affricative\": \"A\",\n",
    "                    \"nasal\": \"N\",\n",
    "                    \"other\": \"X\",\n",
    "                    \"fricative\": \"F\",\n",
    "                    \"lateral\": \"L\",\n",
    "                    \"sibilant\": \"S\",\n",
    "                },\n",
    "                default=PH_ABSENT,\n",
    "                pos=\"cls\",\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    word=dict(\n",
    "        layers=dict(\n",
    "            lang=dict(\n",
    "                feature=\"lang\",\n",
    "                map={x[0]: i + 1 for (i, x) in enumerate(F.lang.freqList())},\n",
    "                default=0,\n",
    "                pos=None,\n",
    "            ),\n",
    "            speaker=dict(\n",
    "                feature=\"speaker\",\n",
    "                map={x[0]: i + 1 for (i, x) in enumerate(F.speaker.freqList())},\n",
    "                default=0,\n",
    "                pos=None,\n",
    "            ),\n",
    "        ),\n",
    "        afterFeature=\"full_end\",\n",
    "        afterDefault=\"/\",\n",
    "    ),\n",
    "    sentence=dict(\n",
    "        afterDefault=\"\\n\",\n",
    "        by=True,\n",
    "    ),\n",
    "    line=dict(\n",
    "        layers=dict(\n",
    "            number=dict(\n",
    "                feature=\"line_number\",\n",
    "                map=None,\n",
    "                default=-1,\n",
    "                pos=None,\n",
    "                show=True,\n",
    "            ),\n",
    "        ),\n",
    "        afterDefault=\"\\n\",\n",
    "    ),\n",
    "    text=dict(\n",
    "        layers=dict(\n",
    "            title=dict(\n",
    "                feature=\"title\",\n",
    "                map=None,\n",
    "                default=\"¿\",\n",
    "                pos=None,\n",
    "                show=True,\n",
    "            ),\n",
    "            tid=dict(\n",
    "                feature=\"text_id\",\n",
    "                map=None,\n",
    "                default=\"¿\",\n",
    "                pos=None,\n",
    "            ),\n",
    "            place=dict(\n",
    "                feature=\"place\",\n",
    "                map=None,\n",
    "                default=\"¿\",\n",
    "                pos=None,\n",
    "            ),\n",
    "        ),\n",
    "        afterDefault=\"\\n\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "TYPE_SEQ = list(DATA)\n",
    "TYPES_LOWER = {}\n",
    "\n",
    "for (i, tp) in enumerate(TYPE_SEQ):\n",
    "    TYPES_LOWER[tp] = TYPE_SEQ[0: i + 1]\n",
    "    \n",
    "# check show and by attributes\n",
    "\n",
    "theBys = []\n",
    "theShows = []\n",
    "\n",
    "for (nType, typeInfo) in DATA.items():\n",
    "    if typeInfo.get(\"by\", False):\n",
    "        theBys.append(nType)\n",
    "        \n",
    "    for (name, layerInfo) in DATA[nType].get(\"layers\", {}).items():\n",
    "        if layerInfo.get(\"show\", False):\n",
    "            theShows.append((nType, name))\n",
    "            \n",
    "if len(theBys) == 0:\n",
    "    sys.stderr.write(\"No node type is declared as result container ('by')\\n\")\n",
    "elif len(theBys) > 1:\n",
    "    sys.stderr.write(\"Multiple node types declared as result container ('by'):\\n\")\n",
    "    sys.stderr.write(\"\\t\" + (\", \".join(theBys)) + \"\\n\")\n",
    "else:\n",
    "    sys.stdout.write(\"Node type declared as result container ('by'):\\n\")\n",
    "    sys.stdout.write(f\"\\t{theBys[0]}\\n\")\n",
    "    \n",
    "sys.stderr.flush()\n",
    "sys.stdout.flush()\n",
    "\n",
    "if len(theShows) == 0:\n",
    "    sys.stderr.write(\"No layer type is declared as result shower ('show')\\n\")\n",
    "else:\n",
    "    sys.stdout.write(\"Layers declared as result showers ('show'):\\n\")\n",
    "    sys.stdout.write(\"\\t\" + (\", \".join(\"/\".join(s) for s in theShows)) + \"\\n\")\n",
    "    \n",
    "sys.stderr.flush()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-architect",
   "metadata": {},
   "source": [
    "We take care that for every phonetic property, the value is always exactly one character, no more no less.\n",
    "That means that all recorded phonetic texts have the same mapping between character positions and slot numbers.\n",
    "\n",
    "For the full text it is different: there are 18 letters with an empty full text, and some letters use multiple characters for their full text.\n",
    "\n",
    "In the end, we only have to produce two mappings for the character node type: for the full text and for the phonetics.\n",
    "We choose the phonetic `cls` text to carry the phonetic mapping.\n",
    "\n",
    "As to the mapping from letter nodes to words, sentences, lines and texts: we only need to do that once, and we create\n",
    "it as a single *up* relation, stored outside the recorders.\n",
    "\n",
    "The *up* relation goes from nodes from one type to containing nodes of another type.\n",
    "\n",
    "We make use of the fact that texts are built from lines, which are built from sentences, which are built from words,\n",
    "which are built from characters. \n",
    "\n",
    "This simplifies the *up* relation considerably: we may assume that every node *n* has a single *up* parent:\n",
    "\n",
    "* look in the node type that is one level higher than the type of *n*\n",
    "* pick a node *u* in that type that embeds *n*\n",
    "* *u* must be the only node with that proeprty w.r.t. *n*, since the nodes of these types act as building blocks.\n",
    "\n",
    "Another simplifying hypothesis that holds for this data, is that each character position corresponds with at most\n",
    "one node per node type.\n",
    "\n",
    "So if we have a set of nodes that all correspond with the same character position, they must all belong to different types.\n",
    "Hence, when we organize mappings from character positions to nodes, and we do that for each node type separately,\n",
    "then such mappings map each character position to at most one nodes.\n",
    "\n",
    "Characters on positions that are not mapped by a layer to nodes cannnot be compared with character positions in other layers.\n",
    "So they will fall out of the results if more than one layer is being compared.\n",
    "\n",
    "**N.B.**\n",
    "\n",
    "These simplifying hypothesis make it easier to code the layered search interface in Javascript.\n",
    "But they are not needed for the concept to work.\n",
    "\n",
    "Since this is my first implementation of layered search, written under time constraints, I thankfully make use of these\n",
    "simplifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olympic-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(data):\n",
    "    sets = {}\n",
    "    \n",
    "    compressed = []\n",
    "\n",
    "    for n in sorted(data):\n",
    "        sets.setdefault(data[n], []).append(n)\n",
    "        \n",
    "    for (value, nset) in sorted(\n",
    "        sets.items(), key=lambda x: (x[1][0], x[1][-1])\n",
    "    ):\n",
    "        nodeSpec = n if len(nset) == 1 else specFromRangesLogical(rangesFromSet(nset))\n",
    "        compressed.append([nodeSpec, value])\n",
    "        \n",
    "    return compressed\n",
    "\n",
    "def invert(data):\n",
    "    return {v: k for (k,v) in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "outstanding-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record():\n",
    "    A.indent(reset=True)\n",
    "    A.info(\"preparing ... \")\n",
    "\n",
    "    A.info(\"start recording\")\n",
    "\n",
    "    up = {}\n",
    "    by = {}\n",
    "    show = {}\n",
    "    layers = {}\n",
    "    texts = {}\n",
    "    positions = {}\n",
    "    recorders = {}\n",
    "    accumulators = {}\n",
    "    testTexts = []\n",
    "    \n",
    "    preSep = dict(\n",
    "        text=\"text\",\n",
    "        line=\"\\tline\",\n",
    "        sentence=\"\\t\\tsent\",\n",
    "        word=\"\\t\\t\\tword\",\n",
    "        letter= \"\\t\\t\\t\\tletter\",\n",
    "    )\n",
    "    postSep = dict(\n",
    "        text=\"\\n\",\n",
    "        line=\"\\n\",\n",
    "        sentence=\"\\n\",\n",
    "        word=\"\\n\",\n",
    "        letter= \"\\n\",\n",
    "    )\n",
    "\n",
    "    for (nType, typeInfo) in DATA.items():\n",
    "        ti = typeInfo.get(\"layers\", None)\n",
    "        by[nType] = typeInfo.get(\"by\", False)\n",
    "        if ti is None:\n",
    "            continue\n",
    "\n",
    "        show[nType] = {name: ti[name].get(\"show\", False) for name in ti}\n",
    "        layers[nType] = {\n",
    "            name: dict(map=ti[name][\"map\"], pos=ti[name][\"pos\"] or name) for name in ti\n",
    "        }\n",
    "        texts[nType] = {name: None for name in ti}\n",
    "        positions[nType] = {name: None for name in ti if ti[name][\"pos\"] is None}\n",
    "        recorders[nType] = {\n",
    "            name: Recorder(A.api) for name in ti if ti[name][\"pos\"] is None\n",
    "        }\n",
    "        accumulators[nType] = {name: [] for name in ti if ti[name][\"pos\"] is not None}\n",
    "\n",
    "    nChAbsent = 0\n",
    "\n",
    "    def addValue(node):\n",
    "        returnValue = None\n",
    "\n",
    "        nType = F.otype.v(node)\n",
    "        typeInfo = DATA[nType]\n",
    "        theseLayers = typeInfo.get(\"layers\", {})\n",
    "\n",
    "        first = True\n",
    "        \n",
    "        pre = preSep[nType]\n",
    "        post = postSep[nType]\n",
    "        \n",
    "        if nType == \"text\":\n",
    "            testText = []\n",
    "            testTexts.append((node, testText))\n",
    "        else:\n",
    "            testText = testTexts[-1][-1]\n",
    "            \n",
    "        testText.append(f\"{pre} {node} [\")\n",
    "\n",
    "        for name in theseLayers:\n",
    "            info = theseLayers[name]\n",
    "            default = info[\"default\"]\n",
    "            pos = info[\"pos\"]\n",
    "            value = Fs(info[\"feature\"]).v(node)\n",
    "            vMap = info[\"map\"]\n",
    "            if vMap:\n",
    "                value = vMap.get(value, default)\n",
    "            else:\n",
    "                value = value or default\n",
    "            value = str(value)\n",
    "\n",
    "            if pos is None:\n",
    "                recorders[nType][name].add(value)\n",
    "            else:\n",
    "                accumulators[nType][name].append(value)\n",
    "                \n",
    "            testText.append((\"\" if first else \"|\") + value)\n",
    "\n",
    "            if first:\n",
    "                returnValue = value\n",
    "                first = False\n",
    "\n",
    "        testText.append(f\"]{post}\")\n",
    "        \n",
    "        return returnValue\n",
    "\n",
    "    def addAfterValue(node):\n",
    "        nType = F.otype.v(node)\n",
    "        typeInfo = DATA[nType]\n",
    "        afterFeature = typeInfo.get(\"afterFeature\", None)\n",
    "        afterDefault = typeInfo.get(\"afterDefault\", None)\n",
    "        value = \"\"\n",
    "        if afterFeature is not None:\n",
    "            value = Fs(afterFeature).v(node)\n",
    "        if afterDefault is not None:\n",
    "            if not value:\n",
    "                value = afterDefault\n",
    "        if value:\n",
    "            addAll(nType, value)\n",
    "\n",
    "    def addAll(nType, value):\n",
    "        nTypes = TYPES_LOWER[nType]\n",
    "        for nType in nTypes:\n",
    "            if nType in recorders:\n",
    "                for x in recorders[nType].values():\n",
    "                    x.add(value)\n",
    "            if nType in accumulators:\n",
    "                for x in accumulators[nType].values():\n",
    "                    x.append(value)\n",
    "\n",
    "    def deliverAll():\n",
    "        for (nType, typeInfo) in recorders.items():\n",
    "            for (name, x) in typeInfo.items():\n",
    "                texts[nType][name] = x.text()\n",
    "                # here we are going to use that there is at most one node per node type\n",
    "                # that corresponds to a character position\n",
    "                positions[nType][name] = [\n",
    "                    tuple(nodes)[0] if nodes else None for nodes in x.positions()\n",
    "                ]\n",
    "\n",
    "        for (nType, typeInfo) in accumulators.items():\n",
    "            for (name, x) in typeInfo.items():\n",
    "                texts[nType][name] = \"\".join(x)\n",
    "\n",
    "    def startNode(node):\n",
    "        # we have organized recorders by node type\n",
    "        # we only record nodes of matching type in recorders\n",
    "\n",
    "        nType = F.otype.v(node)\n",
    "\n",
    "        if nType in recorders:\n",
    "            for rec in recorders[nType].values():\n",
    "                rec.start(node)\n",
    "\n",
    "    def endNode(node):\n",
    "        # we have organized recorders by node type\n",
    "        # we only record nodes of matching type in recorders\n",
    "        nType = F.otype.v(node)\n",
    "\n",
    "        if nType in recorders:\n",
    "            for rec in recorders[nType].values():\n",
    "                rec.end(node)\n",
    "\n",
    "    # note the `up[n] = m` statements below:\n",
    "    # we only let `up` connect nodes from one level to one level higher\n",
    "\n",
    "    for (i, text) in enumerate(F.otype.s(\"text\")):\n",
    "        startNode(text)\n",
    "        title = addValue(text)\n",
    "        sys.stdout.write(\"\\r\" + f\"{i + 1:>3} {title:<80}\")\n",
    "\n",
    "\n",
    "        for line in L.d(text, otype=\"line\"):\n",
    "            up[line] = text\n",
    "            startNode(line)\n",
    "            addValue(line)\n",
    "\n",
    "            for sent in L.d(line, otype=\"sentence\"):\n",
    "                up[sent] = line\n",
    "                startNode(sent)\n",
    "                addValue(sent)\n",
    "\n",
    "                for word in L.d(sent, otype=\"word\"):\n",
    "                    up[word] = sent\n",
    "                    startNode(word)\n",
    "                    addValue(word)\n",
    "\n",
    "                    for letter in L.d(word, otype=\"letter\"):\n",
    "                        up[letter] = word\n",
    "                        startNode(letter)\n",
    "\n",
    "                        ch = addValue(letter)\n",
    "                        if ch == CH_ABSENT:\n",
    "                            nChAbsent += 1\n",
    "\n",
    "                        endNode(letter)\n",
    "                        addAfterValue(letter)\n",
    "\n",
    "                    endNode(word)\n",
    "                    addAfterValue(word)\n",
    "\n",
    "                endNode(sent)\n",
    "                addAfterValue(sent)\n",
    "\n",
    "            endNode(line)\n",
    "            addAfterValue(line)\n",
    "\n",
    "        endNode(text)\n",
    "        addAfterValue(text)\n",
    "\n",
    "    deliverAll()\n",
    "\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "    A.info(f\"{nChAbsent} letter nodes with empty full text\")\n",
    "\n",
    "    data = dict(\n",
    "        captions=dict(\n",
    "            title=\"NENA phono search\",\n",
    "        ),\n",
    "        ntypes=TYPE_SEQ[::-1],\n",
    "        by=by,\n",
    "        show=show,\n",
    "        layers=layers,\n",
    "        texts=texts,\n",
    "        positions=positions,\n",
    "        up=compress(up),\n",
    "    )\n",
    "\n",
    "    return (data, testTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "employed-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumpData(data, testTexts, debug=False):\n",
    "    A.indent(reset=True)\n",
    "    A.info(\"Dumping data to a single compact json file\")\n",
    "    \n",
    "    fileName = f\"{OUTPUT}/corpus.js\"\n",
    "    \n",
    "    with open(fileName, \"w\") as fh:\n",
    "        fh.write(\"const corpus = \")\n",
    "        json.dump(data, fh, ensure_ascii=False, indent=None, separators=(',', ':'))\n",
    "        \n",
    "    A.info(f\"Data written to file {fileName}\")\n",
    "    \n",
    "    if debug:\n",
    "        A.info(f\"Writing same data as non-compact json file\")\n",
    "        fileName = f\"{DEBUG_OUTPUT}/corpus.js\"\n",
    "        with open(fileName, \"w\") as fh:\n",
    "            fh.write(\"const corpus = \")\n",
    "            json.dump(data, fh, ensure_ascii=False, indent=1)\n",
    "        A.info(f\"Data written to file {fileName}\")\n",
    "            \n",
    "        A.info(f\"Writing same data as separate, human readable files\")\n",
    "        for (kind, subData) in data.items():\n",
    "            if kind == \"ntypes\":\n",
    "                fileName = f\"{DEBUG_OUTPUT}/{kind}.tsv\"\n",
    "                A.info(fileName)\n",
    "                with open(fileName, \"w\") as fh:\n",
    "                    fh.write(\"\\t\".join(subData) + \"\\n\")\n",
    "                continue\n",
    "                \n",
    "            if kind in {\"captions\", \"up\", \"by\", \"show\"}:\n",
    "                fileName = f\"{DEBUG_OUTPUT}/{kind}.json\"\n",
    "                A.info(fileName)\n",
    "                with open(fileName, \"w\") as fh:\n",
    "                    json.dump(subData, fh, ensure_ascii=False, indent=1)\n",
    "                continue\n",
    "                \n",
    "            for (nType, typeData) in subData.items():\n",
    "                for (name, layerData) in typeData.items():\n",
    "                    ext = \"txt\" if kind == \"texts\" else \"tsv\" if kind == \"positions\" else \"json\"\n",
    "                    fileName = f\"{DEBUG_OUTPUT}/{kind}-{nType}-{name}.{ext}\"\n",
    "                    A.info(fileName)\n",
    "                    with open(fileName, \"w\") as fh:\n",
    "                        if ext == \"json\":\n",
    "                            json.dump(layerData, fh, ensure_ascii=False, indent=1)\n",
    "                        elif ext == \"tsv\":\n",
    "                            for entry in layerData:\n",
    "                                fh.write(f\"{entry}\\n\")\n",
    "                        else:\n",
    "                            fh.write(layerData)\n",
    "                            \n",
    "        for (node, testText) in testTexts:\n",
    "            fileName = f\"{TEST_TEXTS_DIR}/{node:>06}.txt\"\n",
    "\n",
    "            with open(fileName, \"w\") as fh:\n",
    "                fh.write(\"\".join(testText))\n",
    "        A.info(f\"Test texts written to directory {TEST_TEXTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "strange-exemption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s preparing ... \n",
      "  0.00s start recording\n",
      "126 Women Do Things Best                                                            \n",
      "    12s 18 letter nodes with empty full text\n"
     ]
    }
   ],
   "source": [
    "(data, testText) = record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "satisfied-australian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s Dumping data to a single compact json file\n",
      "  2.26s Data written to file ~/github/CambridgeSemiticsLab/nena_tf/nena2search/app/corpus.js\n",
      "  2.26s Writing same data as non-compact json file\n",
      "  4.83s Data written to file ~/github/CambridgeSemiticsLab/nena_tf/_local/corpus.js\n",
      "  4.83s Writing same data as separate, human readable files\n",
      "  4.83s ~/github/CambridgeSemiticsLab/nena_tf/_local/captions.json\n",
      "  4.83s ~/github/CambridgeSemiticsLab/nena_tf/_local/ntypes.tsv\n",
      "  4.83s ~/github/CambridgeSemiticsLab/nena_tf/_local/by.json\n",
      "  4.83s ~/github/CambridgeSemiticsLab/nena_tf/_local/show.json\n",
      "  4.83s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-letter-full.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-letter-cls.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-letter-voice.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-letter-place.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-letter-manner.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-word-lang.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-word-speaker.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-line-number.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-text-title.json\n",
      "  4.84s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-text-tid.json\n",
      "  4.85s ~/github/CambridgeSemiticsLab/nena_tf/_local/layers-text-place.json\n",
      "  4.85s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-letter-full.txt\n",
      "  4.85s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-letter-cls.txt\n",
      "  4.85s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-letter-voice.txt\n",
      "  4.85s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-letter-place.txt\n",
      "  4.86s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-letter-manner.txt\n",
      "  4.86s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-word-lang.txt\n",
      "  4.86s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-word-speaker.txt\n",
      "  4.86s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-line-number.txt\n",
      "  4.86s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-text-title.txt\n",
      "  4.87s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-text-tid.txt\n",
      "  4.87s ~/github/CambridgeSemiticsLab/nena_tf/_local/texts-text-place.txt\n",
      "  4.87s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-letter-full.tsv\n",
      "  5.24s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-letter-cls.tsv\n",
      "  5.52s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-word-lang.tsv\n",
      "  5.65s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-word-speaker.tsv\n",
      "  5.78s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-line-number.tsv\n",
      "  5.78s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-text-title.tsv\n",
      "  5.79s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-text-tid.tsv\n",
      "  5.79s ~/github/CambridgeSemiticsLab/nena_tf/_local/positions-text-place.tsv\n",
      "  5.79s ~/github/CambridgeSemiticsLab/nena_tf/_local/up.json\n",
      "  7.13s Test texts written to directory ~/github/CambridgeSemiticsLab/nena_tf/_local/texts\n"
     ]
    }
   ],
   "source": [
    "dumpData(data, testText, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-concord",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
